{"path":"iCloudDrive/bks/Calculus/proj/i-n-herstein-topics-in-algebra-2nd-edition-1975-wiley-international-editions-john-wiley-and-sons-wie-1975.pdf","text":"i. n. herstein University of Chicago TOPICS IN ALGEBRA 2nd edition JOHN WILEY & SONS New York • Chichester • Brisbane • Toronto • Singapore To Marianne Copyright © 1975, 1964 by Xerox Corporation. All r!ghts reserved. Reproduction or translation of any part of this work beyond that permitted by Sections 107 or 108 of the 1976 United States Copyright Act without the permission of the copyright owner is unlawful. Requests for permission or further information should be addressed to the Permissions Department, John Wiley & Sons, Inc. Library of Congress Catalog Card Number: 74-82577 Printed in the United States of America. 20 19 18 17 16 15 Preface to the Second Edition I approached rev1smg Topics in Algebra with a certain amount of trepidation. On the whole, I was satisfied with the first edition and did not want to tamper with it. However, there were certain changes I felt should be made, changes which would not affect the general style or content, but which would make the book a little more complete. I hope that I have achieved this objective in the present version. For the most part, the major changes take place in the chapt¥r on group theory. When the first edition was written it was fairly un- common for a student learning abstract algebra to have had any previous exposure to linear algebra. Nowadays quite the opposite is true; many students, perhaps even a majority, have learned something about 2 x 2 matrices at this stage. Thus I felt free here to draw on 2 x 2 matrices for examples and problems. These parts, which depend on some knowledge of linear algebra, are indicated with a #. In the chapter on groups I have largely expanded one section, that on Sylow's theorem, and added two others, one on direct products and one on the structure of finite abelian groups. In the previous treatment of Sylow's theorem, only the existence of a Sylow subgroup was shown. This was done following the proof of Wielandt. The conjugacy of the Sylow subgroups and their number were developed in a series of exercises, but not in the text proper. Now all the parts of Sylow's theorem are done in the text materi9-l. iii iv Preface to the Second Edition In addition to the proof previously given for the existence, two other proofs of existence are carried out. One could accuse me of overkill at this point, probably rightfully so. The fact of the matter is that Sylow's theorem is important, that each proof illustrates a different aspect of group theory and, above all, that I love Sylow's theorem. The proof of the con- jugacy and number of Sylow subgroups exploits double cosets. A by-product of this development is that a means is given for finding Sylow subgroups in a large set of symmetric groups. For some mysterious reason known only to myself, I had omitted direct products in the first edition. Why is beyond me. The material is easy, straightforward, and important. This lacuna is now filled in the section treating direct products. With this in hand, I go on in the next section to prove the decomposition of a finite abelian group as a direct product of cyclic groups and also prove the uniqueness of the invariants associated with this decomposition. In point of fact, this decomposition was already in the first edition, at the end of the chapter on vector spaces, as a consequence of the structure of finitely generated modules over Euclidean rings. However, the case of a finite group is of great importance by itself; the section on finite abelian groups underlines this importance. Its presence in the chapter on groups, an early chapter, makes it more likely that it will be taught. One other entire section has been added at the end of the chapter on field theory. I felt that the student should see an explicit polynomial over an explicit field whose Galois group was the symmetric group of degree 5, hence one whose roots could not be expressed by radicals. In order to do so, a theorem is first proved which gives a criterion that an irreducible poly- nomial of degree p, p a prime, over the rational field have SP as its Galois group. As an application of this criterion, an irreducible polynomial of degree 5 is given, over the rational field, whose Galois group is the symmetric group of degree 5. There are several other additions. More than 150 new problems are to be found here. They are of varying degrees of difficulty. Many are routine and computational, many are very djfficult. Furthermore, some inter- polatory remarks are made about problems that have given readers a great deal of difficulty. Some paragraphs have been inserted, others rewritten, at places where the writing had previously been obscure or too terse. Above I have described what I have added. What gave me greater difficulty about the revision was, perhaps, that which I have not added. I debated for a long time with myself whether or not to add a chapter on category theory and some elementary functors, whether or not to enlarge the material on modules substantially. After a great deal of thought and soul- searching, I decided not to do so. The book, as stands, has a certain concrete- ness about it with which this new material would not blend. It could be made to blend, but this would require a complete reworking of the material Preface to the Second Edition v of the book and a complete change in its philosophy-something I did not want to do. A mere addition of this new material, as an adjunct with no applications and no discernible goals, would have violated my guiding principle that all matters discussed should lead to some clearly defined objectives, to some highlight, to some exciting theorems. Thus I decided to omit the additional topics. Many people wrote me about the first edition pointing out typographical mistakes or making suggestions on how to improve the book. I should like to take this opportunity to thank them for their help and kindness. Preface to the First Edition The idea to write this book, and more important the desire to do so, is a direct outgrowth of a course I gave in the academic year 1959-1960 at Cornell University. The class taking this course consisted, in large part, of the most gifted sophomores in mathematics at Cornell. It was my desire to experiment by presenting to them material a little beyond that which is usually taught in algebra at the junior-senior level. I have aimed this book to be, both in content and degree of sophisti- cation, about halfway between two great classics, A Survey of M~dern Algebra, by Birkhoff and MacLane, and Modern Algebra, by Van der Waerden. The last few years have seen marked changes in the instruction given in mathematics at the American universities. This change is most notable at the upper undergraduate and beginning graduate levels. Topics that a few years ago were considered proper subject matter for semiadvanced graduate courses in algebra have filtered down to, and are being taught in, the very first course in abstract algebra. Convinced that this filtration will continue and will become intensified in the next few years, I have put into this book, which is designed to be used as the student's first introduction to algebra, material which hitherto has been considered a little advanced for that stage of the game. There is always a great danger when treating abstract ideas to intro- duce them too suddenly and without a sufficient base of examples to render them credible or natural. In order to try to mitigate this, I have tried to motivate the concepts beforehand and to illustrate them in con- crete situations. One of the most telling proofs of the worth of an abstract vii viii Preface to the First Edition concept is what it, and the results about it, tells us in familiar situations. In almost every chapter an attempt is made to bring out the significance of the general results by applying them to particular problems. For instance, in the chapter on rings, the two-square theorem of Fermat is exhibited as a direct consequence of the theory developed for Euclidean rings. The subject matter chosen for discussion has been picked not only because it has become standard to present it at this level or because it is important in the whole general development but also with an eye to this \"concreteness.\" For this reason I chose to omit the Jordan-Holder theorem, which certainly could have easily been included in the results derived about groups. How- ever, to appreciate this result for its own sake requires a great deal of hind- sight and to see it used effectively would require too great a digression. True, one could develop the whole theory of dimension of a vector space as one of its corollaries, but, for the first time around, this seems like a much too fancy and unnatural approach to something so basic and down-to-earth. Likewise, there is no mention of tensor products or related constructions. There is so much time and opportunity to become abstract; why rush it at the beginning? A word about the problems. There are a great number of them. It would be an extraordinary student indeed who could solve them all. Some are present merely to complete proofs in the text material, others to illustrate and to give practice in the results obtained. Many are introduced not so much to be solved as to be tackled. The value of a problem is not so much in coming up with the answer as in the ideas and attempted ideas it forces on the would-be solver. Others are included in anticipation of material to be developed later, the hope and rationale for this being both to lay the groundwork for the subsequent theory and also to make more natural ideas, definitions, and arguments as they are introduced. Several problems appear more than once. Problems that for some reason or other seem difficult to me are often starred (sometimes with two stars). However, even here there will be no agreement among mathematicians; many will feel that some unstarred problems should be starred and vice versa. Naturally, I am indebted to many people for suggestions, comments and criticisms. To mention just a few of these: Charles Curtis, Marshall Hall, Nathan Jacobson, Arthur Mattuck, and Maxwell Rosenlicht. I owe a great deal to Daniel Gorenstein and Irving Kaplansky for the numerous con- versations we have had about the book, its material and its approach. Above all, I thank George Seligman for the many incisive suggestions and remarks that he has made about the presentation both as to its style and to its content. I am also grateful to Francis McNary of the staff of Ginn and Company for his help and cooperation. Finally, I should like to express my thanks to theJohn Simon Guggenheim Memorial Foundation; this book was in part written with their support while the author was in Rome as a Guggenheim Fellow. I Contents 1 Preliminary Notions 1.1 Set Theory 1.2 Mappings 1.3 The Integers 2 Group Theory 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 2.10 2.11 2.12 2.13 2.14 Definition of a Group Some Examples of Groups Some Preliminary Lemmas Subgroups A Counting Principle Normal Subgroups and Quotient Groups Homomorphisms Automorphisms Cayley's Theorem Permutation Groups Another Counting Principle Sylow's Theorem Direct Products Finite Abelian Groups ix 2 10 18 26 27 29 33 37 44 49 54 66 71 75 82 91 103 109 X Contents 3 Ring Theory 120 3.1 Definition and Examples of Rings 120 3.2 Some Special Classes of Rings 125 3.3 Homomorphisms 131 3.4 Ideals and Quotient Rings 133 3.5 More Ideals and Quotient Rings 137 3.6 The Field of Quotients of an Integral Domain 140 3.7 Euclidean Rings 143 3.8 A Particular Euclidean Ring 149 3.9 Polynomial Rings 153 3.10 Polynomials over the Rational Field 159 3.11 Polynomial Rings over Commutative Rings 161 4 Vector Spaces and Modules 170 4.1 Elementary Basic Concepts 171 4.2 Linear Independence and Bases 177 4.3 Dual Spaces 184 4.4 Inner Product Spaces 191 4.5 Modules 201 5 Fields 207 5.1 Extension Fields 207 5.2 The Transcendence of e 216 5.3 Roots of Polynomials 219 5.4 Construction with Straightedge and Compass 228 5.5 More About Roots 232 5.6 The Elements of Galois Theory 237 5.7 Solvability by Radicals 250 5.8 Galois Groups over the Rationals 256 6 Linear Transformations 260 6.1 The Algebra of Linear Transformations 261 6.2 Characteristic Roots 270 6.3 Matrices 273 6.4 Canonical Forms: Triangular Form 285 Contents xi 6.5 Canonical Forms: Nilpotent Transformations 292 6.6 Canonical Forms: A Decomposition of V: Jordan Form 298 6.7 Canonical Forms: Rational Canonical Form 305 6.8 Trace and Transpose 313 6.9 Determinants 322 6.10 Hermitian, Unitary, and Normal Transformations 336 6.11 Real Quadratic Forms 350 7 Selected Topics 355 7.1 Finite Fields 356 7.2 Wedderburn's Theorem on Finite Division Rings 360 7.3 A Theorem of Frobenius 368 7.4 Integral Quaternions and the Four-Square Theorem 371 1 Prelilllinary Notions One of the amazing features of twentieth century mathematics has been its recognition of the power of the abstract approach. This has given rise to a large body of new results and problems and has, in fact, led us to open up whole new areas of mathematics whose very existence had not even been suspected. In the wake of these developments has come not only a new mathematics but a fresh outlook, and along with this, simple new proofs of difficult classical results. The isolation of a problem inl'o its basic essentials has often revealed for us the proper setting, in the whole scheme of things, of results considered to have been special and apart and has shown us interrelations between areas previously thought to have been unconnected. The algebra which has evolved as an outgrowth of all this is not only a subject with an independent life and vigor-it is one of the important current research areas in mathematics-but it also serves as the unifying thread which interlaces almost all of mathematics- geometry, number theory, analysis, topology, and even applied mathematics. This book is intended as an introduction to that part of mathematics that today goes by the name of abstract algebra. The term \"abstract\" is a highly subjective one; what is abstract to one person is very often concrete and down-to-earth to another, and vice versa. In relation to the current research activity in algebra, it could be described as \"not too abstract\"; from the point of view of someone schooled in the 1 2 Preliminary Notions Ch. 1 calculus and who is seeing the present !llaterial for the first time, it may very well be described as \"quite abstract.\" Be that as it may, we shall concern ourselves with the introduction and development of some of the important algebraic systems-groups, rings, vector spaces, fields. An algebraic system can be described as a set of objects together with some operations for combining them. Prior to studying sets restricted in any way whatever-for instance, with operations-it will be necessary to consider sets in general and some notions about them. At the other end of the spectrum, we shall need some informa- tion about the particular set, the set of integers. It is the purpose of this chapter to discuss these and to derive some results about them which we can call upon, as the occasions arise, later in the book. 1 .1 Set Theory We shall not attempt a formal definition of a set nor shall we try to lay the groundwork for an axiomatic theory of sets. Instead we shall take the operational and intuitive approach that a set is some given collection of objects. In most of our applications we shall be dealing with rather specific things, and the nebulous notion of a set, in these, will emerge as something quite recognizable. For those whose tastes run more to the formal and abstract side, we can consider a set as a primitive notion which one does not define. A few remarks about notation and terminology. Given a set S we shall use the notation throughout a E S to read \"a is an element if S.\" In the same vein, a¢ Swill read \"a is not an element of S.\" The set A will be said. to be a subset of the setS if every element in A is an element of S, that is, if a E A implies a E S. We shall write this as A c S (or, sometimes, as S ;::, A), which may be read \"A is contained inS\" (or, S contains A). This notation is not meant to preclude the possibility that A = S. By the way, what is meant by the equality of two sets? For us this will always mean that they contain the same elements, that is, every element which is in one is in the other, and vice versa. In terms of the symbol for the containing relation, the two sets A and B are equal, written A = B, if both A c B and B c A. The standard device for proving the equality of two sets, something we shall be required to do often, is to demonstrate that the two opposite containing relations hold for them. A subset A of S will be called a proper subset of S if A c S but A =I= S (A is not equal to S). The null set is the set having no elements; it is a subset of every set. We shall often describe that a set Sis the null set by saying it is empty. One final, purely notational remark: Given a set S we shall constantly use the notation A = {a E S I P(a)} to read \"A is the set of all elements in S for which the property P holds.\" For instance, if S is the set of integers Sec.1.1 Set Theory and if A is the subset of positive integers, then we can describe A as A = {a E S I a > 0}. Another example of this: If Sis the set consisting of the objects (1), (2), ... , (10), then the subset A consisting of (1), (4), (7), (10) could be described by A = {(i) E S I i = 3n + 1, n = 0, 1, 2, 3}. Given two sets we can combine them to form new sets. There is nothing sacred or particular about this number two; we can carry out the same pro- cedure for any number of sets, finite or infinite, and in fact we shall. We do so for two first because it illustrates the general construction but is not obscured by the additional notational difficulties. DEFINITION The union of the two sets A and B, written as A u B, is the set {x I x E A or x E B}. A word about the use of \"or.\" In ordinary English when we say that something is one or the other we. imply that it is not both. The mathematical \"or\" is quite different, at least when we are speaking about set theory. For when we say that x is in A or x is in B we mean x is in at least one of A or B, and may be in both. Let us consider a few examples of the union of two sets. For any set A, A u A = A; in fact, whenever B is a subset of A, A u B = A. If A is the set {x1, x2 , x3 } (i.e., the set whose elements are x1 , x 2 , x3 ) and if B is the set {y1,y2 , xd, then A u B = {x1, x2 , x3,y1,y2 }. If A is the set of all blonde- haired people and if B is the set of all people who smoke, then A u B consists of all the people who either have blonde hair or smoke or both. Pictorially we can illustrate the union of the two sets A and B by Here, A is the circle on the left, B that on the right, and A u B is the shaded part. DEFINITION The intersection of the two sets A and B, written as A r. B, is the set {x I x E A and x E B}. The intersection of A and B is thus the set of all elements which are both in A and in B. In analogy with the examples used to illustrate the union of two sets, let us see what the intersections are in those very examples. For 3 4 Preliminary Notions Ch. 1 any set A, A n A = A; in fact, if B is any subset of A, then A n B = B. If A is the set {x1 , x2 , x3 } and B the set {y 1,y 2 , xd, then A n B = {xd (we are supposing no y is an x). If A is the set of all blonde-haired people and if B is the set of all people that smoke, then A n B is the set of all blonde-haired people who smoke. Pictorially we can illustrate the inter- section of the two sets A and B by Here A is the circle on the left, B that on the right, while their intersection is the shaded part. Two sets are said to be disjoint if their intersection is empty, that is, is the null set. For instance, if A is the set of positive integers and B the set of negative integers, then A and Bare disjoint. Note however that if Cis the set of nonnegative integers and if D is the set of nonpositive integers, then they are not disjoint, for their intersection consists of the integer 0, and so is not empty. Before we generalize union and intersection from two sets to an arbitrary number of them, we should like to prove a little proposition interrelating union and intersection. This is the first of a whole host of such resqlts that can be proved; some of these can be found in the problems at the end of this section. PROPOSITION For any three sets, A, B, C we have A n (B u C) = (A n B) u (A n C). Proof The proof will consist of showing, to begin with, the relation (A n B) u (A n C) c A n (B u C) and then the converse relation A n (B u C) c (A n B) u (A n C). We first dispose of (A n B) u (A n C) c A n (B u C). Because B c B u C, it is immediate that A n B c A n (B u C). In a similar manner, A n C c A n (B u C). Therefore (A n B) u (A n C) c (A n (B u C)) u (A n (B u C)) = A n (B u C). Now for the other direction. Given an element x E A n (B u C), first of all it must be an element of A. Secondly, as an element in B u C it is either in B or in C. Suppose the former; then as an element both of A and of B, x must be in A n B. The second possibility, namely, x E C, leads us Sec.1.1 Set Theory to x E A n C. Thus in either eventuality x E (A n B) u (A n C), whence A n (B u C) c (A n B) u (A n C). The two opposite containing relations combine to give us the equality asserted in the proposition. We continue the discussion of sets to extend the notion of union and of intersection to arbitrary collections of sets. Given a set Twe say that T serves as an index set for the family§' = {Acx} of sets if for every ex E T there exists a set of Acx in the family §'. The index set T can be any set, finite or infinite. Very often we use the set of non- negative integers as an index set, but, we repeat, T can be any (nonempty) set. By the union of the sets Acx, where ex is in T, we mean the set {xI x E Acx for at least one ex in T}. We shall denote it by UcxeT Acx. By the intersection of the sets Acx, where ex is in T, we mean the set {xI x E Acx for every ex E T}; we shall denote it by ncxeT Acx. The sets Acx are mutually disjoint if for ex =I= {3, A« n Ap is the null set. For instance, if S is the set of real numbers, and if Tis the set of rational numbers, let, for ex E T, A« = {xES I x ~ ex}. It is an easy exercise to see that UaeT Aa = s whereas naeT Acx is the null set. The sets Aa are not mutually disjoint. DEFINITION Given the two sets A, B then the difference set, A - B, is the set { x E A I x ~ B}. Returning to our little pictures, if A is the circle on the left, B that on the right, then A - B is the shaded area. Note that for any set B, the set A satisfies A = (A n B) u (A - B). (Prove!) Note further that B n (A - B) is the null set. A particular case of interest of the difference of two sets is when one of these is a subset of the other. In that case, when B is a subset of A, we call A - B the complement of Bin A. We still want one more construct of two given sets A and B, their Cartesian product A x B. This set A x B is defined as the set of all ordered pairs (a, b) where a E A and bE B and where we declare the pair (a 1 , b1 ) to be equal to (a 2 , b2 ) if and only if a1 = a2 and b1 = b2 • 5 6 Preliminary Notions Ch. 1 A few remarks about the Cartesian product. Given the two sets A and B we could construct the sets A x B and B x A from them. As sets these are distinct, yet we feel that they must be closely related. Given three sets A, B, C we can construct many Cartesian products from them: for instance, the set A x D, where D = B x C; the set E x C, where E =A x B; and also the set of all ordered triples (a, b, c) where a E A, bE B, and c E C. These give us three distinct sets, yet here, also, we feel that these sets must be closely related. Of course, we can continue this process with more and more sets. To see the exact relation between them we shall have to wait until the next section, where we discuss one-to-one correspondences. Given any index set T we could define the Cartesian product of the sets Aa as ex varies over T; since we shall not need so general a product, we do not bother to define it. Finally, we can consider the Cartesian product of a set A with itself, A x A. Note that if the set A is a finite set having n elements, then the set A x A is also a finite set, but has n2 elements. The set of elements (a, a) in A x A is called the diagonal of A x A. A subset R of A x A is said to define an equivalence relation on A if 1. (a, a) E R for all a EA. 2. (a, b) E R implies (b, a) E R. 3. (a, b) E Rand (b, c) E R imply that (a, c) E R. Instead of speaking about subsets of A x A we can speak about a binary relation (one between two elements of A) on A itself, defining b to be related to a if (a, b) E R. The properties 1, 2, 3 of the subset R immediately-translate into the properties 1, 2, 3 of the definition below. DEFINITION The binary relation \"' on A is said to be an equivalence relation on A if for all a, b, c in A I. a \"' a. 2. a \"' b implies b \"' a. 3. a \"' b and b \"' c imply a \"' c. The first of these properties is called riflexivity, the second, symmetry, and the third, transitivity. The concept of an equivalence relation is an extremely important one and plays a central role in all of mathematics. We illustrate it with a few examples. Example 1.1.1 Let S be any set and define a \"' b, for a, b E S, if and only if a = b. This clearly defines an equivalence relation on S. In fact, an equivalence relation is a generalization of equality, measuring equality up to some property. Sec. 1.1 Set Theory Example 1 .1 .2 Let S be the set of all integers. Given a, b E S, define a \"' b if a - b is an even integer. We verify that this defines an equivalence relation of S. I. Since 0 = a a is even, a IV a. 2. If a \"' b, that is, if a - b is even, then b - a = -(a - b) is also even, whence b \"' a. 3. If a \"' b and b IV c, then both a - b and b - c are even, whence a - c = (a - b) + (b - c) is also even, proving that a \"' c. Example 1.1.3 Let S be the set of all integers and let n > 1 be a fixed integer. Define for a, bE S, a \"' b if a - b is a multiple of n. We leave it as an exercise to prove that this defines an equivalence relation on S. Example 1 .1 .4 Let S be the set of all triangles in the plane. Two triangles are defined to be equivalent if they are similar (i.e., have corre- sponding angles equal). This defines an equivalence relation on S. Example 1.1.5 Let S be the set of points in the plane. Two points a and b are defined to be equivalent if they are equidistant from the origin. A simple check verifies that this defines an equivalence relation on S. There are many more equivalence relations; we shall encounter a few as we proceed in the book. DEFINITION If A is a set and if\"' is an equivalence relation on A, then the equivalence class of a E A is the set {x E A I a \"' x}. We write it as cl(a). In the examples just discussed, what are the equivalence classes? In Example 1.1.1, the equivalence class of a consists merely of a itself. In Example 1.1.2 the equivalence class of a consists of all the integers of the form a + 2m, where m = 0, ± 1, ±2, ... ; in this example there are only two distinct equivalence classes, namely, cl(O) and cl(l). In Example 1.1.3, the equivalence class of a consists of all integers of the form a + kn where k = 0, ± I, ± 2, ... ; here there are n distinct equivalence classes, namely cl(O),cl(l), ... ,cl(n- 1). In Example 1.1.5, the equivalence class of a consists of all the points in the plane which lie on the circle which has its center at the origin and passes through a. Although we have made quite a few definitions, introduced some concepts, and have even established a simple little proposition, one could say in all fairness that up to this point we have not proved any result of real substance. We are now about to prove the first genuine result in the book. The proof of this theorem is not very difficult-actually it is quite easy-but nonetheless the result it embodies will be of great use to us. 7 8 Preliminary Notions Ch. 1 THEOREM 1.1.1 The distinct equivalence classes of an equivalence relation on A provide us with a decomposition of A as a union of mutually disjoint subsets. Conversely, given a decomposition of A as a union of mutually disjoint, nonempty subsets, we can difine an equivalence relation on A for which these subsets are the distinct equivalence classes. Proof. Let the equivalence relation on A be denoted by \"\". We first note that since for any a E A, a \"\" a, a must be in cl(a), whence the union of the cl(a)'s is all of A. We now assert that given two equivalence classes they are either equal or disjoint. For, suppose that cl(a) and cl(b) are not disjoint; then there is an element x E cl(a) n cl(b). Since x E cl(a), a \"\" x; since x E cl (b), b \"\" x, whence by the symmetry of the relation, x \"\" b. However, a \"\" x and x \"\" b by the transitivity of the relation forces a \"\" b. Suppose, now that y E cl(b); thus b \"\"y. However, from a \"\" b and b \"\"y, we deduce that a \"\"y, that is, thaty E cl(a). Therefore, every element in cl(b) is in cl(a), which proves that cl(b) c cl(a). The argument is clearly symmetric, whence we conclude that cl(a) c cl(b). The two opposite containing relations imply that cl(a) = cl(b). We have thus shown that the distinct cl(a)'s are mutually disjoint and that their union is A. This proves the first half of the theorem. Now for the other half! Suppose that A = U Aa where the Aa are mutually disjoint, nonempty sets (a is in some index set T). How shall we use them to define an equiva- lence relation? The way is clear; given an element a in A it is in exactly one Aa. We define for a, bE A, a \"\" b if a and b are in the same Aa. We leave it as an exercise to prove that this is an equivalence relation on A and that the distinct equivalence classes are the Aa's. Problems I. (a) If A is a subset of Band B is a subset of C, prove that A is a subset of C. (b) If B c A, prove that A u B = A, and conversely. (c) If B c A, prove that for any set C both B u C c A u C and BnCcAnC. 2. (a) Prove that A n B = B n A and A u B = B u A. (b) Prove that (A n B) n C = A n (B n C). 3. Prove that A u (B n C) = (A u B) n (A u C). 4. For a subset C of S let C' denote the complement of C inS. For any two subsets A, B of S prove the De Morgan rules: (a) (A n B)' = A' u B'. (b) (A u B)' = A' n B'. 5. For a finite set C let o(C) indicate the number of elements in C. If A and B are finite sets prove o(A u B) = o(A) + o(B) - o(A n B). Sec. 1 .1 Set Theory 9 6. If A is a finite set having n elements, prove that A has exactly 2n distinct subsets. 7. A survey shows that 63% of the American people like cheese whereas 76o/0 like apples. What can you say about the percentage of the American people that like both cheese and apples? (The given statistics are not meant to be accurate.) 8. Given two sets A and B their symmetric difference is defined to be (A - B) u (B - A). Prove that the symmetric difference of A and B equals (A u B) - (A n B). 9. Let S be a set and let S* be the set whose elements are the various sub- sets of S. In S* we define an addition and multiplication as follows: If A, BE S* (remember, this means that they are subsets of S): (I) A + B = (A - B) u ( B - A). (2) A·B =An B. Prove the following laws that govern these operations: (a) (A + B) + C = A + (B + C). (b) A· (B + C) = A· B + A· C. (c) A·A =A. (d) A + A = null set. (e) If A + B = A + C then B = C. (The system just described is an example of a Boolean algebra.) IO. For the given set and relation below determine which define equivalence relations. (a) Sis the set of all people in the world today, a ,...., b if a and b have an ancestor in common. (b) Sis the set of all people in the world today, a ,...., b if a lives wL~hin I 00 miles of b. (c) Sis the set of all people in the world today, a ,...., b if a and b have the same father. (d) Sis the set of real numbers, a ,...., b if a = ±b. (e) Sis the set ofintegers, a,...., b ifboth a> band b >a. (f) Sis the set of all straight lines in the plane, a ,...., b if a is parallel to b. 11. (a) Property 2 of an equivalence relation states that if a ,...., b then b ,...., a; property 3 states that if a \"' b and b ,...., c then a \"' c. What is wrong with the following proof that properties 2 and 3 imply property 1 ? Let a ,...., b; then b \"' a, whence, by property 3 (using a = c), a \"' a. (b) Can you suggest an alternative of property 1 which will insure us that properties 2 and 3 do imply property 1 ? 12. In Example 1.1.3 of an equivalence relation given in the text, prove that the relation defined is an equivalence relation and that there are exactly n distinct equivalence classes, namely, cl(O), cl(l ), ... , cl(n - 1 ). 13. Complete the proot of the second half of Theorem 1.1.1. 10 Preliminary Notions Ch. 1 1.2 Mappings We are about to introduce the concept of a mapping of one set into another. Without exaggeration this is probably the single most important and uni- versal notion that runs through all of mathematics. It is hardly a new thing to any of us, for we have been considering mappings from the very earliest days of our mathematical training. When we were asked to plot the relation y = x 2 we were simply being asked to study the particular mapping which takes every real number onto its square. Loosely speaking, a mapping from one set, S, into another, T, is a \"rule\" (whatever that may mean) that associates with each element in Sa unique element tin T. We shall define a mapping somewhat more formally and precisely but the purpose of the definition is to allow us to think and speak in the above terms. We should think of them as rules or devices or mech- anisms that transport us from one set to another. Let us motivate a little the definition that we will make. The point of view we take is to consider the mapping to be defined by its \"graph.\" We illustrate this with the familiar example y = x 2 defined on the real numbers Sand taking its values also in S. For this set S, S x S, the set of all pairs (a, b) can be viewed as the plane, the pair (a, b) corresponding to the point whose coordinates are a and b, respectively. In this plane we single out all those points whose coordinates are of the form (x, x 2 ) and call this set of points the graph of y = x 2 • We even represent this set pictorially as To find the \"value\" of the function or mapping at the point x = a, we look at the point in the graph whose first coordinate is a and read off the second coordinate as the value of the function at x = a. This is, no more or less, the approach we take in the general setting to define a mapping from one set into another. DEFINITION If Sand Tare nonempty sets, then a mapping from S to T is a subset, M, of S x T such that for every s E S there is a unique t E T such that the ordered pair (s, t) is in M. This definition serves to make the concept of a mapping precise for us but we shall almost never use it in this form. Instead we do prefer to think of a Sec. 1.2 Mappings 11 mapping as a rule which associates with any element s in S some element tin T, the rule being, associate (or map) s E S with t E T if and only if (s, t) EM. We shall say that tis the image of sunder the mapping. Now for some notation for these things. Let u be a mapping from S to T; we often denote this by writing u :S ---+ Tor S ~ T. If t is the image of s under u we shall sometimes write this as u :s ---+ t; more often, we shall represent this fact by t = su. Note that we write the mapping u on the right. There is no overall consistency in this usage; many people would write it as t = u(s). Algebraists often write mappings on the right; other mathematicians write them on the left. In fact, we shall not be absolutely consistent in this ourselves; when we shall want to emphasize the functional nature of u we may very well write t = u(s). Examples of Mappings In all the examples the sets are assumed to be nonempty. Example 1 .2.1 Let S be any set; define z :S ---+ S by s = sz for any s E S. This mapping lis called the identity mapping of S. Example 1.2.2 Let S and T be any sets and let t0 be an element of T. Define -r :S ---+ T by -r :s ---+ t0 for every s E S. Example 1 .2.3 Let S be the set of positive rational numbers and let T = J x J where J is the set of integers. Given a rational number s we can write it as s = mfn, where m and n have no common factor. Define -r:S ---+ T by s-r = (m, n). Example 1.2.4 Letjbethesetofintegers andS = {(m, n) Ej x Jl n =I= 0}; let T be the set of rational numbers; define -r:S---+ T by (m, n)-r = mfn for every (m, n) inS. Example 1 .2.5 Let J be the set of integers and S = J x ]. Define -r:S---+ J by (m, n)-r = m + n. Note that in Example 1.2.5 the addition in J itself can be represented in terms rf a mapping of J x J into]. Given an arbitrary set S we call a mapping of S x S into S a binary operation on S. Given such a mapping 't' :S x S ---+ S we could use it to define a \"product\" * in S by declaring a* b = c if (a, b)-r = c. Example 1 .2.6 Let S and T be any sets; define -r :S x T ---+ S by (a, b)-r = a for any (a, b) E S x T. This -r is called the projection of S x T on S. We could similarly define the projection of S x Ton T. 12 Preliminary Notions Ch. 1 Example 1.2.7 Let S be the set consisting of the elements x1, x2 , x3 . Define -r:S---+ Sbyx 1-r = x2 , x2 -r = x3 , x3 -r = x1 . Example 1 .2.8 Let S be the set of integers and let T be the set consisting of the elements E and 0. Define 't' :S ---+ T by declaring n-r = E if n is even and n-r = 0 if n is odd. If S is any set, let {x1 , •.. , xn} be its subset consisting of the elements x1, x2 , ••• , xn of S. In particular, {x} is the subset of S whose only element is x. Given S we can use it to construct a new set S*, the set whose elements are the subsets of S. We call S* the set of subsets of S. Thus for instance, if S = {x1 , x2 } then S* has exactly four elements, namely, a1 = null set, a2 = the subset, S, of S, a3 = {x1 }, a4 = {x2 }. The relation of S to S*, in general, is a very interesting one; some of its properties are examined in the problems. Example 1 .2.9 Let S be a set, T = S*; define -r :S ---+ T by s-r = complement of {s} inS= S- {s}. Example 1 .2.1 0 Let S be a set with an equivalence relation, and let T be the set of equivalence classes in S (note that T is a subset of S*). Define -r:S---+ T by s-r = cl(s). We leave the examples to continue the general discussion. Given a mapping 't' :S ---+ T we define for t E T, the inverse image oft with respect to -r to be the set {s E S I t = s-r }. In Example 1.2.8, the inverse image of E is the subset of S consisting of the even integers. It may happen that for some t in T that its inverse image with respect to -r is empty; that is, t is not the image under -r of any element in S. In Example 1.2.3, the element ( 4, 2) is not the image of any element inS under the 't' used; in Example 1.2.9, S, as an element in S*, is not the image under the 't' used of any element in S. DEFINITION The mapping 't' of S into Tis said to be onto T if given t E T there exists an element s E S such that t = s-r. If we call the subset S-r = { x E T I x = s-r for some s E S} the image of S under -r, then 't' is onto if the image of Sunder 't' is all of T. Note that in Examples 1.2.1, 1.2.4-1.2.8, and 1.2.10 the mappings used are all onto. Another special type of mapping arises often and is important: the one- to-one mapping. DEFINITION The mapping -r of S into Tis said to be a one-to-one mapping if whenever s1 =I= s2 , then s1 -r =I= s2 -r. Sec. 1.2 Mappings 13 In terms of inverse images, the mapping -r is one-to-one if for any t E T the inverse image oft is either empty or is a set consisting of one element. In the examples discussed, the mappings in Examples 1.2.1, 1.2.3, 1.2.7, and 1.2.9 are all one-to-one. When should we say that two mappings from S to Tare equal? A natural definition for this is that they should have the same effect on every element of S; that is, the image of any element in Sunder each of these mappings should be the same. In a little more formal manner: DEFINITION The two mappings a and -r of S into Tare said to be equal if sa = s-r for every s E S. Consider the following situation: We have a mapping a from S to T and another mapping -r from T to U. Can we compound these mappings to produce a mapping from S to U? The most natural and obvious way of doing this is to send a given element s, in S, in two stages into U, first by applying a to sand then applying -r to the resulting element sa in T. This is the basis of the DEFINITION If a:S ~ T and -r:T ~ U then the composition of a and -r (also called their product) is the mapping a o -r:S ~ U defined by means of s(a o-r) = (sa)-r for every s E S. Note that the order of events reads from left to right; a o -r reads: first perform a and then follow it up with -r. Here, too, the left-right business is not a uniform one. Mathematicians who write their mappings on the left would read a o-r to mean first perform -r and then a. Accordingly,~.in reading a given book in mathematics one must make absolutely sure as to what convention is being followed in writing the product of two mappings. We reiterate, for us a o -r will always mean: first apply a and then -r. We illustrate the composition of a and -r with a few examples. Example 1.2.11 Let S = {x1 , x2 , x3 } and let T = S. Let a:S ~ S be defined by and -r:S ~Shy x1a = x2 , x2 a = x3 , x 3 a = x 1 ·; x1-r = x1, x2-r = x3, X3! = Xz. 14 Preliminary Notions Ch. 1 Thus x1 (a o-r) x2 (a o-r) x3 (a o-r) (x1 a)-r = x2 t = x3 , (x?.a)t = x 3-r = x2 , (x3 a)-r = x1-r = x1 . At the same time we can compute -r o a, because in this case it also makes sense. Now x1 (-r o a) x2 (-roa) x3 (-r o a) (x1 -r)a = (x1 a) = x2 , (x2 -r)a = x3a = x1 , (x3-r)a = x2 a = x3 • Note that x2 = x1 (t o a), whereas x3 = x1(a o-r) whence a o 't ¥=-'to a. Example 1.2.12 LetS be the set of integers, T the setS x S, and suppose a:S ~ Tis defined by rna = (rn - 1, 1). Let U = S and suppose that -r: T ~ U( = S) is defined by (rn, n)-r = rn + n. Thus a o -r:S ~ S whereas -r o a: T ~ T; even to speak about the equality of a o-r and -r o a would make no sense since they do not act on the same space. We now compute a o -r as a mapping of S into itself and then -r o a as one on T into itself. Given rn E S, rna = (rn - 1, 1) whence rn(a o-r) = (rna)-r = (rn - 1, 1)-r = (rn - 1) + 1 = rn. Thus a o-r is the identity mapping of S into itsel£ What about -r o a? Given (rn, n) E T, (rn, n)-r = rn + n, whereby (rn, n) (r o a) = ((rn, n)r)a = (rn + n)a = (rn + n - 1, 1). Note that -r o a is not the identity map of T into itself; it is not even an onto mapping of T. Example 1 .2.13 Let S be the set of real numbers, T the set of integers, and U = {E, 0}. Define a:S ~ T by sa = largest integer less than or equal to s, and r: T ~ U defined by n-r = E if n is even, n-r = 0 if n is odd. Note that in this case -r o a cannot be defined. We compute a or for two real numbers s = ! and s = n. Now since ! = 2 + j-, (!)a = 2, whence (})(a or) = (}a)-r = (2)-r = E; (n)a = 3, whence n(a o-r) = (na)r = (3)-r = 0. For mappings of sets, provided the reqmstte products make sense, a general associative law holds. This is the content of LEMMA 1.2.1 (AssociATIVE LAw) If a :S ~ T, -r: T ~ U, and J1: U ~ V, then (a o -r) o J1 = a o ( -r o J1) • Proof. Note first that a o-r makes sense and takes S into U, thus (a o-r) o J1 also makes sense and takes S into V. Similarly a o (-r o /1) is meaningful and takes S into V. Thus we can speak about the equality, or lack of equality, of (a o-r) o J1 and a o (-r o Jl). To prove the asserted equality we merely must show that for any s E S, s((a o-r) o Jl) = s(a o (-r o Jl)). Now by the very definition of the composition Sec. 1.2 Mappings 15 of maps, s((a o-r) o p) = (s(a o -r))p = ((sa)-r)p whereas s(a o (-r o p)) = (sa)(-r o p) = ((sa)-r)p. Thus, the elements s((a o-r) o p) and s(a o (-r o p)) are indeed equal. This proves the lemma. We should like to show that if two mappings a and T are properly condi- tioned the very same conditions carry over to a o-r. LEMMA 1.2.2 Let a:S ~ T and -r: T ~ U; then 1. a o T is onto if each of a and T is onto. 2. a o T is one-to-one if each of a and T is one-to-one. Proof. We prove only part 2, leaving the proof of part I as an exercise. Suppose that s1 , s2 E Sand that s1 =/= s2 • By the one-to-one nature of a, s1 a =1= s2 a. Since T is one-to-one and s1 a and s2 a are distinct elements of T, (s1a)-r =1= (s2 a)-r whence s1 (a o-r) = (s1a)-r =/= (s2 a)-r = s2 (a o-r), proving that a oTis indeed one-to-one, and establishing the lemma. Suppose that a is a one-to-one mapping of S onto T; we call a a one-to-one correspondence between Sand T. Given any t E T, by the \"onto-ness\" of a there exists an element s E S such that t = sa; by the \"one-to-oneness\" of a this s is unique. We define the mapping a- 1 : T ~ S by s = ta- 1 if and only if t = sa. The mapping a- 1 is called the inverse of a. Let us compute a o a- 1 which maps S into itself. Given s E S, let t = sa, whence by definitions = ta- 1 ; thus s(a o a- 1 ) = (sa) a- 1 = ta- 1 = s. We have shown that a o a- 1 is the identity mapping of S onto itself. A similar computation reveals that a- 1 o a is the identity mapping of Tonto itself. 'Conversely, if a :S ~ T is such that there exists a 11: T ~ S with th+ property that a o 11 and 11 o a are the identity mappings on S and T, respec- tively, then we claim that a is a one-to-one correspondence between Sand T. First observe that a is onto for, given t E T, t = t(p o a) = (tp)a (since J1 o a is the identity on T) and so t is the image under a of the element tp in S. Next observe that a is one-to-one, for if s1 a = s2a, using that a o 11 is the identity on S, we have s1 = s1 (a o p) = (s1a)p = (s2 a) 11 = s2 (a o p) = s2 • We have now proved LEMMA 1 .2.3 The mapping a :S ~ T is a one-to-one correspondence between S and T if .. .md only if there exists a mapping 11: T ~ S such that a o 11 and 11 o a are the identity mappings on S and T, respectively. DEFINITION If S is a nonempty set then A(S) IS the set of all one-to-one mappings of S onto itself. ' Aside from its own intrinsic interest A (S) plays a central and universal tfpe of role in considering the mathematical system known as a group 16 Preliminary Notions Ch. 1 (Chapter 2). For this reason we state the next theorem concerning its nature. All the constituent parts of the theorem have already been proved in the various lemmas, so we state the theorem without proof. THEOREM 1.2.1 If u, -r, 11 are elements of A(S), then 1. u o -r is in A ( S) . 2. (uo-r)oJl=Uo(-roJl). 3. There exists an element z (the identity map) in A(S) such that u o z = lou u. 4. There exists an element u- 1 E A(S) such that u o u- 1 = u- 1 o u = z. We close the section with a remark about A(S). Suppose that S has more than two elements; let x1 , x2 , x3 be three distinct elements in S; define the mapping u:S--+ S by x1u = x2 , x2 u = x3 , x3 u = x1 , su = s for any s E S different from x1 , x2 , x3 . Define the mapping -r:S--+ S by x2 -r = x3 , x3 -r = x2 , and s-r = s for any s E S different from x2 , x3 • Clearly both u and -r are in A(S). A simple computation shows that x1 (u o-r) = x3 but that x1 (-r o u) = x2 # x3 • Thus u o-r # -r o u. This is LEMMA 1.2.4 If S has more that two elements we can find two elements u, -r in A(S) such that u o-r # -r o u. Problems 1. In the following, where u :S --+ T, determine whether the u is onto and/or one-to-one and determine the inverse image of any t E T under u. (a) S = set of real numbers, T = set of nonnegative real numbers, su = s2 • (b) S = set of nonnegative real numbers, T = set of nonnegative real numbers, su = s 2 • (c) S = set ofintegers, T = set ofintegers, su = s 2 • (d) S = set of integers, T = set of integers, su = 2s. 2. If S and Tare nonempty sets, prove that there exists a one-to-one correspondence between S x T and T x S. 3. If S, T, U are nonempty sets, prove that there exists a one-to-one correspondence between (a) (S x T) x U and S x ( T x U). (b) Either set in part (a) and the set of ordered triples ( s, t, u) where s E s, t E T, u E u. 4. (a) If there is a one-to-one correspondence between S and T, prove that there exists a one-to-one correspondence between T and S. Sec. 1.2 Mappings 17 (b) If there is a one-to-one correspondence between S and T and between T and U, prove that there is a one-to-one correspondence between S and U. 5. If z is the identity mapping on S, prove that for any a E A(S), (f 0 l = l 0 (J = (f. *6. If Sis any set, prove that it is impossible to find a mapping of S onto S*. 7. If the setS has n elements, prove that A(S) has n! (n factorial) elements. 8. If the set S has a finite number of elements, prove the following: (a) If a maps S onto S, then a is one-to-one. (b) If a is a one-to-one mapping of S onto itself, then u is onto. (c) Prove, by example, that both part (a) and part (b) are false if S does not have a finite number of elements. 9. Prove that the converse to both parts ofLemma 1.2.2 are false; namely, (a) If a o-r is onto, it need not be that both u and -r are onto. (b) If a o -r is one-to-one, it need not be that both u and -r are one-to- one. 10. Prove that there is a one-to-one correspondence between the set of integers and the set of rational numbers. 11. If u :S --+- T and if A is a subset of S, the restriction cif a to A, u A' IS defined by au A = au for any a EA. Prove (a) u A defines a mapping of A into T. (b) u A is one-to-one if a is. (c) u A may very well be one-to-one even if u is not. 12. If u:S --+- S and A is a subset of S such that Au c A, prove that (u 0 a)A =(fA 0 (fA. 13. A set S is said to be infinite if there is a one-to-one correspondence between S and a proper subset of S. Prove (a) The set of integers is infinite. (b) The set of real numbers is infinite. (c) If a setS has a subset A which is infinite, then S must be infinite. (Note: By the result of Problem 8, a set finite in the usual sense is not infinite.) *14. If S is infinite and can be brought into one-to-one correspondence with the set of integers, prove that there is one-to-one correspondence between S and S x S. * 15. Given two sets S and T we declare S < T (S is smaller than T) if there is a mapping ofT onto S but no mapping of S onto T. Prove that if S < T and T < U then S < U. 16. If Sand Tare finite sets having m and n elements, respectively, prove that ifm < n then S < T. II 18 Preliminary Notions Ch. 1 1 .3 The Integers We close this chapter with a brief discussion of the set of integers. We shall make no attempt to construct them axiomatically, assuming instead that we already have the set of integers and that we know many of the elementary facts about them. In this number we include the principle of mathematical induction (which will be used freely throughout the book) and the fact that a nonempty set of positive integers always contains a smallest element. As to notation, the familiar symbols: a > b, a ::::;; b, Ia I, etc., will occur with their usual meaning. To avoid repeating that something is an integer, we make the assumption that all symbols, in this section, written as lowercase Latin letters will be integers. Given a and b, with b '# 0, we can divide a by b to get a nonnegative remainder r which is smaller in size than b; that is, we can find m and r such that a = mb + r where 0 ::::;; r < lbl. This fact is known as the Euclidean algorithm and we assume familiarity with it. We say that b '# 0 divides a if a = mb for some m. We denote that b divides a by b I a, and that b does not divide a by b -f' a. Note that if a I 1 then a = ± 1, that when both a I b and b I a, then a = ±b, and that any b divides 0. If b I a, we call b a divisor of a. Note that if b is a divisor of g and of h, then it is a divisor of mg + nh for arbitrary integers m and n. We leave the verification of these remarks as exercises. DEFINITION The positive integer cis said to be the greatest common divisor of a and b if 1. c is a divisor of a and of b. 2. Any divisor of a and b is a divisor of c. We shall use the notation (a, b) for the greatest common divisor of a and b. Since we insist that the greatest common divisor be positive, (a, b) = (a, -b)= (-a, b)= (-a, -b). Forinstance, (60,24) = (60, -24) = 12. Another comment: The mere fact that we have defined what is to be meant by the greatest common divisor does not guarantee that it exists. This will have to be proved. However, we can say that if it exists then it is unique, for, if we had c1 and c2 satisfying both conditions of the definition above, then c1 I c2 and c2 I c1 , whence we would have c1 = ±c2 ; the insistence on positivity would then force c1 = c2 . Our first business at hand then is to dispose of the existence of (a, b). In doing so, in the next lemma, we actually prove a little more, namely that (a, b) must have a particular form. LEMMA 1 .3.1 If a and b are integers, not both 0, then (a, b) exists; moreover, we canfind integers m0 and n0 such that (a, b) = m0 a + n0 b. Sec. 1.3 The Integers 19 Proof. Let vH be the set of all integers of the form ma + nb, where m and n range freely over the set of integers. Since one of a or b is not 0, there are nonzero integers in Jt. Because x = ma + nb is in Jt, -x = ( -m)a + ( -n)b is also in .A; therefore, .A always has in it some positive integers. But then there is a smallest positive integer, c, in .A; being in .A, c has the form c = m0 a + n0 b. We claim that c = (a, b). Note first that if d I a and d I b, the d I (m0 a + nob), whence d I c. We now must show that c I a and c I b. Given any element x = ma + nb in Jt, then by the Euclidean algorithm, x = tc + r where 0 ~ r < c. Writing this out explicitly, ma + nb = t(m0 a + n0 b) + r, whence r = (m - tm0 )a + (n - tn0 )b and so must be in JU. Since 0 ~ rand r < c, by the choice of c, r = 0. Thus x = tc; we have proved that c I x for any x E Jl/. But a = 1a + Ob E Jlt and b = Oa + lb E Jt, whence c I a and c I b. We have shown that c satisfies the requisite properties to be (a, b) and so we have proved the lemma. DEFINITION The integers a and bare relatively prime if (a, b) l. As an immediate consequence of Lemma 1.3.1, we have the COROLLARY If a and b are relatively prime, we can find integers m and n such that ma + nb = I. We introduce another familiar notion, that of prime number. By this we shall mean an integer which has no nontrivial factorization. For technical reasons, we exclude 1 from the set of prime numbers. The sequence 2, 3, 5, 7, 11, ... are all prime numbers; equally, -2, -3, -5, ... are prime numbers. Since, in factoring, the negative introduces no essential differences, for us prime numbers will always be positive. DEFINITION The integer p > 1 is a prime number if its only divisors are ± 1, ±p. Another way of putting this is to say that an integer p (larger than I) is a prime number if and only if given any other integer n then either (p, n) = 1 or P I n. As we shall soon see, the prime numbers are the building blocks of the integers. But first we need the important observation, LEMMA 1.3.2 If a is relatively prime to b but a I be, then a I c. Proof. Since a and b are relatively prime, by the corollary to Lemma 1.3.1, we can find integers m and n such that ma + nb = I. Thus mac + nbc = c. Now a I mac and, by assumption, a I nbc; consequently, 20 Preliminary Notions Ch. 1 a 1 (mac + nbc). Since mac + nbc = c, we conclude that a I c, which Is precisely the assertion of the lemma. Following immediately from the lemma and the definition of prime number is the important COROLLARY If a prime number divides the product qf certain integers it must divide at least one qf these integers. We leave the proof of the corollary to the reader. We have asserted that the prime numbers serve as the building blocks for the set of integers. The precise statement of this is the unique factorization theorem: THEOREM 1.3.1 Any positive integer a > 1 can befactored in a unique way as a = p 1a. 1p 2a.2 • • • p,a.t, where p 1 > p2 > · · · > Pt are prime numbers and where each ai > 0. Proof. The theorem as stated actually consists of two distinct sub- theorems; the first asserts the possibility of factoring the given integer as a product of prime powers; the second assures us that this decomposition is unique. We shall prove the theorem itself by proving each of these sub- theorems separately. An immediate question presents itself: How shall we go about proving the theorem? A natural method of attack is to use mathematical induction. A short word about this; we shall use the following version of mathematical induction: If the proposition P (m0 ) is true and if the truth of P (r) for all r such that m0 ~ r < k implies the truth of P(k), then P(n) is true for all n ~ m0 • This variant of induction can be shown to be a consequence of the basic property of the integers which asserts that any nonempty set ofpositive integers has a minimal element (see Problem 10). We first prove that every integer a > 1 can be factored as a product of prime powers; our approach is via mathematical induction. Certainly m0 = 2, being a prime number, has a representation as a product of prime powers. Suppose that any integer r, 2 ~ r < k can be factored as a product of prime powers. If k itself is a prime number, then it is a product of prime powers. If k is not a prime number, then k = uv, where 1 < u < k and 1 < v < k. By the induction hypothesis, since both u and v are less than k, each of these can be factored as a product of prime powers. Thus k = uv is also such a product. We have shown that the truth of the proposition for all integers r, 2 :::;; r < k, implies its truth for k. Consequently, by the basic induction principle, the proposition is true for all integers n ;;:::: m0 = 2; that is, every integer n ;;:::: 2 is a product of prime powers. Sec. 1.3 The Integers 21 Now for the uniqueness. Here, too, we shall use mathematical induction, and in the form used above. Suppose that where Pi > P2 > · · · p, qi > q2 > · · · > qs are prime numbers, and where each ai > 0 and each {3i > 0. Our object is to prove 1. r = s. 2. Pi = qi, P2 = q2, · · · , Pr = qr. 3. li.i = fJll li.2 = {32, · · · ' li.r = {3,. For a = 2 this is clearly true. Proceeding by induction we suppose it to be true for all integers u, 2 ~ u < a. Now, since and since ai > 0, pi I a, hence Pt I qiP 1 • • • q/s. However, since Pi is a prime number, by the corollary to Lemma 1.3.2, it follows easily that Pi = qi for some i. Thus qi ;;::: qi = Pi· Similarly, since qi I a we get qi =pi for some J, whence Pi ;;::: Pi = qi. In short, we have shown that Pi = q1 • Therefore a = Pii1. 1P2 11.2 • • • P/r = P/ 1q/ 2 • • • q/s. We claim that this forces a 1 = f3i· (Prove!) But then If b = I, then a2 = · · · = li.r = 0 and {32 = · · · = f3s = 0; that is, r = s = 1, and we are done. If b > 1, then since b < a we can appl-x,.our induction hypothesis to b to get I. The number of distinct prime power factors (in b) on both sides is equal, that is, r - 1 = s - 1, hence r = s. 2. li.2 = fJ2, · · · ' li.r = f3r· 3· p2 = q2, ... ' Pr = q,. Together with the information we already have obtained, namely, Pi = q1 and a 1 = {Ji, this is precisely what we were trying to prove. Thus we see that the assumption of the uniqueness of factorization for the integers less than a implied the uniqueness of factorization for a. In consequence, the induction is completed and the assertion of unique factorization is estab- lished. We change direction a little to study the important notion of congruence modulo a given integer. As we shall see later, the relation that we now introduce is a special case of a much more general one that can be defined in a much broader context. Ill 22 !! Preliminary Notions Ch. 1 DEFINITION Let n > 0 be a fixed integer. We define a = b mod n if n I (a - b). The relation is referred to as congruence modulo n, n is called the modulus of the relation, and we read a = b mod n as \"a is congruent to b modulo n.\" Note, for example, that 73 = 4 mod 23, 21 = -9 mod 10, etc. This congruence relation enjoys the following basic properties: LEMMA 1.3.3 1. The relation congruence modulo n difines an equivalence relation on the set of integers. 2. This equivalence relation has n distinct equivalence classes. 3. lf a = b mod nand c = d mod n, then a + c = b + d mod nand ac = bd mod n. 4. lf ab = ac mod nand a is relatively prime ton, then b = c mod n. Proof. We first verify that the relation congruence modulo n is an equivalence relation. Since n I 0, we indeed have that n I (a - a) whence a = a mod n for every a. Further, if a = b mod n then n I (a - b), and so n I (b- a) = -(a- b); thus b =a mod n. Finally, if a= b mod nand b = c mod n, then n I (a - b) and n I (b - c) whence n I {(a - b) + (b - c)}, that is, n I (a - c). This, of course, implies that a = c mod n. Let the equivalence class, under this relation, of a be denoted by [a] ; we call it the congruence class (mod n) of a. Given any integer a, by the Euclidean algorithm, a = kn + r where 0 :::;:; r < n. But then, a E [r] and so [a] = [r]. Thus there are at most n distinct congruence classes; namely, [OJ, [1], ... , [n- 1]. However, these are distinct, for if [i] = [J] with, say, 0 :::;:; i < j < n, then n I (J - i) where j - i is a positive integer less than n, which is obviously impossible. Consequently, there are exactly the n distinct congruence classes [0], [1], ... , [n - 1]. We have now proved assertions 1 and 2 of the lemma. We now prove part 3. Suppose that a = b mod n and c = d mod n; therefore, n I (a - b) and n I (c - d) whence n I {(a - d) + (c - d)}, and son I {(a+ c)- (b +d)}. But then a+ c = b + d mod n. In addition, nl {(a- b)c + (c- d)b} = ac- bd,whenceac = bdmodn. Finally, notice that if ab = ac mod n and if a is relatively prime to n, then the fact that n I a(b - c), by Lemma 1.3.2, implies that n I (b - c) and sob = c mod n. If a is not relatively prime to n, the result of part 4 may be false; for instance, 2.3 = 4.3 mod 6, yet 2 ¢ 4 mod 6. Lemma 1.3.3 opens certain interesting possibilities for us. Let fn be the Sec. 1.3 The Integers 23 set of the congruence classes mod n; that is, j n = {[OJ, [ 1 J, ... , [ n - 1]}. Given two elements, [iJ and [j] in lm let us define [iJ + [jJ [iJ [jJ [i + jJ; [ijJ. (a) (b) We assert that the lemma assures us that this \"addition\" and \"multipli- cation\" are well defined; that is, if [iJ = [i'J and [jJ = [j'J, then [iJ + [jJ = [i +jJ = [i' + j'J = [i'J + [j'J and that [iJ[jJ = [i'J[j']. (Verify!) These operations in Jn have the following interesting properties (whose proofs we leave as exercises): for any [iJ, [jJ, [kJ in lm 1. [ iJ + [jJ = [j] + [ i] ) . '] [ 'J [ .J ['J commutative laws. 2. [z J = J 'l 3. ( [ iJ + [j]) + [ k J = [ iJ + ( [jJ + [ k J) ) . . 4. ([iJ[jJ)[kJ = [i]([j][kJ) . associative laws. 5. [i] ([jJ + [kJ) = [iJ [jJ + [i][kJ distributive law. 6. [OJ + [iJ = [iJ. 7. [1J[iJ = [i]. One more remark: if n = p is a prime number and if [a J =f:. [OJ is in ]p, then there is an element [bJ in ]p such that [aJ[bJ = [1]. The set fn plays an important role in algebra and number theory. It is called the set of integers mod n; before we proceed much further we will have become well acquainted with it. Problems I. If a I band b I a, show that a = ±b. 2. If b is a divisor of g and of h, show it is a divisor of mg + nh. 3. If a and b are integers, the least common multiple of a and b, written as [a, bJ, is defined as that positive integer d such that (a) a I d and b I d. (b) Whenever a I x and b I x then d I x. Prove that [a, bJ exists and that [a, bJ = abf(a, b), if a > 0, b > 0. 4. If a I x and b I x and (a, b) = 1 prove that (ab) I x. 5. If a = p1a 1 • • • pkak and b = p/ 1 • • • p/k where the Pi are distinct prime numbers and where each cxi ~ 0, pi ~ 0, prove (a) (a, b) = p1 61 • • • p/k where bi = minimum of cxi and Pi for each i. (b) [a, bJ = P/ 1 • • • Pk..,k where Yi = maximum ofcxi and PJor each i. 24 Preliminary Notions Ch. 1 6. Given a, b, on applying the Euclidean algorithm successively we have a = q0 b + r 1 , b = q1r1 + rz, '1 = qzrz + r3, 0 :$; r1 < lbl, 0 :$; r2 < r 1 , 0 :$; r 3 < r2 , Since the integers rk are decreasing and are all nonnegative, there is a first integer n such that rn+l = 0. Prove that rn = (a, b). (We consider, here, r0 = lbl.) 7. Use the method in Problem 6 to calculate (a) (1128,33). (b) (6540, 1206). 8. To check that n is a prime number, prove that it is sufficient to show that it is not divisible by any prime number p, such that p :$; ,J;. 9. Show that n > I is a prime number if and only if for any a either (a, n) = 1 or n I a. 10. Assuming that any nonempty set of positive integers has a minimal element, prove (a) If the proposition Pis such that (I) P (m0 ) is true, (2) the truth of P(m - 1) implies the truth of P(m), then P(n) is true for all n ~ m0 • (b) If the proposition P is such that (1) P(m0 )istrue, (2) P(m) is true whenever P(a) 1s true for all a such that m0 :$; a < m, then P(n) is true for all n ~ m0 . 1I. Prove that the addition and multiplication used in fn are well defined. 12. Prove the properties 1-7 for the addition and multiplication in fn· 13. If (a, n) = 1, prove that one can find [b] E fn such that [a][b] = [1] in fn· * 14. If p is a prime number, prove that for any integer a, aP = a mod p. I5. If (m, n) = 1, given a and b, prove that there exists an x such that x = a mod m and x = b mod n. I6. Prove the corollary to Lemma 1.3.2. I7. Prove that n is a prime number if and only if in fn, [a][b] [0] impliesthat[a] = [b] = [0]. Sec. 1.3 The Integers 25 Supplementary Reading For sets and cardinal numbers: BIRKHOFF, G., and MAcLANE, S., A Brief Survey of Modern Algebra, 2nd ed. New York: The Macmillan Company, 1965. 2 Group Theory In this chapter we shall embark on the study of the algebraic object known as a group which serves as one of the fundamental building blocks for the subject today called abstract algebra. In later chapters we shall have a look at some of the others such as rings, fields, vector spaces, and linear algebras. Aside from the fact that it has become traditional to consider groups at the outset, there are natural, cogent reasons for this choice. To begin with, groups, being one-operational systems, lend themselves to the simplest formal description. Yet despite this simplicity of description the fundamental algebraic con- cepts such as homomorphism, quotient construction, and the like, which play such an important role in all algebraic structures-in fact, in all of mathematics-already enter here in a pure and revealing form. At this point, before we become weighted down with details, let us take a quick look ahead. In abstract algebra we have certain basic systems which, in the history and development of mathematics, have achieved positions of paramount importance. These are usually sets on whose elements we can operate algebraically-by this we mean that we can combine two elements of the set, perhaps in several ways, to obtain a third element of the set-and, in addition, we assume that these algebraic operations are subject to certain rules, which are explicitly spelled out in what we call the axioms or postulates defining the system. In this abstract setting we then attempt to preve theorems about these very general structures, always hoping that when these results are applied to a particular, concrete realization of the abstract 26 Sec. 2.1 Definition of a Group system there will flow out facts and insights into the example at hand which would have been obscured from us by the mass of inessential information available to us in the particular, special case. We should like to stress that these algebraic systems and the axioms which define them must have a certain naturality about them. They must come from the experience of looking at many examples; they should be rich in meaningful results. One does not just sit down, list a few axioms, and then proceed to study the system so described. This, admittedly, is done by some, but most mathematicians would dismiss these attempts as poor mathematics. The systems chosen for study are chosen because particular cases of these structures have appeared time and time again, because some- one finally noted that these special cases were indeed special instances of a general phenomenon, because one notices analogies between two highly disparate mathematical objects and so is led to a search for the root of these analogies. To cite an example, case after case after case of the special object, which we know today as groups, was studied toward the end of the eighteenth, and at the beginning of the nineteenth, century, yet it was not until relatively late in the nineteenth century that the notion of an abstract group was introduced. The only algebraic structures, so far en- countered, that have stood the test of time and have survived to become of importance, have been those based on a broad and tall pillar of special cases. Amongst mathematicians neither the beauty nor the significance of the first example which we have chosen to discuss-groups-is disputed. 2.1 Definition of a Group At this juncture it is advisable to recall a situation discussed in the\"\"\"first chapter. For an arbitrary nonempty setS we defined A(S) to be the set of all one-to-one mappings of the set S onto itself. For any two elements a, T E A(S) we introduced a product, denoted by a o T, and on further investi- gation it turned out that the following facts were true for the elements of A(S) subject to this product: 1. Whenever a, T E A(S), then it follows that a oTis also in A(S). This is described by saying that A(S) is closed under the product (or, sometimes, as closed under multiplication). 2. For any three elements a, T, J1 e A(S), a o (To Jl) = (a o T) o Jl· This relation is called the associative law. 3. There is a very special element z E A(S) which satisfies z o a = a o z = a for all a E A(S). Such an element is called an identity element for A(S). 4. For every a E A(S) there is an element, written as a- 1, also in A(S), such that a o a- 1 = a- 1 o a = z. This is usually described by saying that every element in A(S) has an inverse in A(S). 27 28 Group Theory Ch. 2 One other fact about A(S) stands out, namely, that whenever S has three or more elements we can find two elements a, f3 E A(S) such that a o f3 =1= f3 o a. This possibility, which runs counter to our usual experience and intuition in mathematics so far, introduces a richness into A(S) which would have not been present except for it. With this example as a model, and with a great deal of hindsight, we abstract and make the DEFINITION A nonempty set of elements G is said to form a group if in G there is defined a binary operation, called the product and denoted by ·, such that I. a, b E G implies that a· b E G (closed). 2. a, b, c E G implies that a· (b·c) = (a·b) ·c (associative law). 3. There exists an element e E G such that a· e = e ·a = a for all a E G (the existence of an identity element in G). 4. For every a E G there exists an element a- 1 E G such that a·a- 1 = a- 1 ·a = e (the existence of inverses in G). Considering the source of this definition it is not surprising that for every nonempty setS the set A(S) is a group. Thus we already have presented to us an infinite source of interesting, concrete groups. We shall see later (in a theorem due to Cayley) that these A(S)'s constitute, in some sense, a universal family of groups. If S has three or more elements, recall that we can find elements u, T E A(S) such that u o -r =I= -r o u. This prompts us to single out a highly special, hut very important, class of groups as in the next definition. DEFINITION A group G is said to be abelian (or commutative) if for every a, b E G, a· b = b ·a. A group which is not abelian is called, naturally enough, non-abelian; having seen a family of examples of such groups we know that non-abelian groups do indeed exist. Another natural characteristic of a group G is the number of elements it contains. We call this the order of G and denote it by o(G). This number is, of course, most interesting when it is finite. In that case we say that G is a finite group. To see that finite groups which are not trivial do exist just note that if the setS contains n elements, then the group A(S) has n! elements. (Prove!) This highly important example will be denoted by Sn whenever it appears in this book, and will be called the symmetric group of degree n.~ In the next section we shall more or less dissect S3 , which is a non-abelian group of order 6. Sec. 2.2 Some Examples of Groups 29 2.2 Some Examples of Groups Example 2.2.1 Let G consist of the integers 0, ± 1, ± 2, . . . where we mean by a· b for a, b E G the usual sum of integers, that is, a· b = a + b. Then the reader can quickly verify that G is an infinite abelian group in which 0 plays the role of e and -a that of a- 1 • Example 2.2.2 Let G consist of the real numbers 1, - 1 under the multiplication of real numbers. G is then an abelian group of order 2. Example 2.2.3 Let G = S3 , the group of all 1-1 mappings of the set {x1 , x2 , x3 } onto itself, under the product which we defined in Chapter 1. G is a group of order 6. We digress a little before returning to S3 . For a neater notation, not just in S3 , but in any group G, let us define for any a E G, a0 = e, a1 = a, a2 .= a·a, a 3 = a·a 2, ... , ak = a·ak-1, and a- 2 = (a- 1 ) 2 , a- 3 = (a- 1 ) 3 , etc. The reader may verify that the usual rules of exponents prevail; namely, for any two integers (positive, negative, or zero) m, n, (1) (2) (It is worthwhile noting that, in this notation, if G is the group of Example 2.2.1, a\" means the integer na). With this notation at our disposal let us examine S3 more closely. Con- sider the mapping fjJ defined on the set x1 , x2 , x 3 by and the mapping xl --+ x2 fjJ: x2 --+ x1 x3 --+ x3, xl --+ x2 l/1: x2 --+ x3 x3 --+ xl. Checking, we readily see that c/J2 = e, l/J 3 = e, and that xl --+ x3 c/J·l/J: x2 --+ Xz x3 --+ xl, whereas xl --+ xl 1/J·c/J: x2 --+ x3 x3 --+ x2. 30 Group Theory Ch. 2 It is clear that 4> ·l/J =/= l/J · 4> for they do not take x 1 into the same image. Since l/J3 = e, it follows that l/1- 1 = l/12 • Let us now compute the action of l/1- 1 • 4> on x1, x2, x 3. Since l/J- 1 = l/12 and we have that x1 ---+ x3 l/12: Xz---+ x1 x3 ---+ x2, xt ---+ x3 x2 ---+ x2 x3 ---+ xt. In other words, cf>·l/1 = l/J- 1 ·¢. Consider the elements e, 4>, l/J, l/J2 , cf>·l/1, l/J · 4>; these are all distinct and are in G (since G is closed), which only has six elements. Thus this list enumerates all the elements of G. One might ask, for instance, What is the entry in the list for l/J · ( 4> ·l/1)? Using 4> ·l/J = l/J- 1 · 4>, we see that l/J · ( 4> ·l/J) = l/J · ( l/J- 1 • 4>) = ( l/J ·l/J- 1 ) · 4> = e · ql = cf>. Of more interest is the form of ( 4> ·l/J) · ( ljJ · 4>) = 4> · ( ljJ · ( ljJ · 4>)) = 4>. ( ljJ 2. 4>) = 4> • ( l/1- 1 • 4>) = 4> . ( 4> ·l/J) = 4> 2 ·l/J = e ·l/J = ljJ. (The reader should not be frightened by the long, wearisome chain of equalities here. It is the last time we shall be so boringly conscientious.) Using the same techniques as we have used, the reader can compute to his heart's content others of the 25 products which do not involve e. Some of these will appear in the exercises. Example 2.2.4 Let n be any integer. We construct a group of order n as follows: G will consist of all symbols ai, i = 0, 1, 2, ... , n - 1 where we insist that a0 = an= e, ai·ai = ai+i if i + j ~ n and ai·ai = ai+j-n if i + j > n. The reader may verify that this is a group. It is called a cyclic group of order n. A geometric realization of the group in Example 2.2.4 may be achieved as follows: Let S be the circle, in the plane, of radius 1, and let p n be a rotation through an angle of2njn. Then Pn E A(S) and Pn in A(S) generates a group of order n, namely, {e, Pm Pn 2 , ... , Pnn- 1 }. Example 2.2.5 Let S be the set of integers and, as usual, let A(S) be the set of all one-to-one mappings of S onto itself. Let G be the set of all elements in A(S) which move only a finite number of elements of S; that is, u E G if and only if the number of x in S such that xu =1= x is finite. If u, T E G, let u·-r be the product of u and T as elements of A(S). We claim that G is a group relative to this operation. We verify this now. To begin with, if u, T E G, then u and T each moves only a,finite number of elements of S. In consequence, u · T can possibly move only those elements in S which are moved by at least one of u or T. Hence u · T moves only a Sec. 2.2 Some Examples of Groups 31 finite number of elements in S; this puts a· r in G. The identity element, z, of A(S) moves no element of S; thus l certainly must be in G. Since the associative law holds universally in A(S), it holds for elements of G. Finally, if u E G and xa- 1 =I= x for some x E S, then (xa- 1 )u =I= xu, which is to say, x(u- 1 ·u) =I= xa. This works out to say merely that x =I= xa. In other words, u- 1 moves only those elements of S which are moved by a. Because a only moves a finite number of elements of S, this is also true for a- 1 . Therefore a- 1 must be in G. We have verified that G satisfies the requisite four axioms which define a group, relative to the operation we specified. Thus G is a group. The reader should verify that G is an infinite, non-abelian group. #Example 2.2.6 Let G be the set of all 2 x 2 matrices(: ~) where a, b, e, dare real numbers, such that ad - be =I= 0. For the operation in G we use the multiplication of matrices; that is, e ~). e :) = (::: :: ~} The entries of this 2 x 2 matrix are clearly real. To see that this matrix is in G we merely must show that (aw + by) (ex + dz) - (ax + bz) (ew + dy) =I= 0 (this is the required relation on the entries of a matrix which puts it in G). A short computation reveals that (aw + by) (ex + dz) since both (ax + bz) (cw + dy) = (ad - be) (wz - xy) =I= 0 (: ~) and (~ :) are in G. The associative law of multiplication holds in matrices; therefore it holds in G. The element I=(~ ~) is in G, since 1 · 1 - 0 · 0 = 1 =1= 0; moreover, as the reader knows, or can verify, I acts as an identity element relative to the operation of G. Finally, if(: ~) E G then, since ad - be of 0, the matrix (ad ~c be ad ~\\c) ad- be ad- be 32 Group Theory Ch. 2 makes sense. Moreover, hence the matrix ad- be (ad - be) 2 ( d -b ) ad ~e be ad : be ad - be ad- be =I= 0, ad - be is in G. An easy computation shows that (ad :e be ad :b be)(: ~); ad - be ad - be thus this element of G acts as the inverse of(: ~)- In short, G is a group. It is easy to see that G is an infinite, non-abelian group. #Example 2.2.7 Let G be the set of all 2 x 2 matrices(: ~} where a, b, e, dare real numbers such that ad - be = 1. Define the operation · in G, as we did in Example 2.2.6, via the multiplication of matrices. We leave it to the reader to verify that G is a group. It is, in fact, an infinite, non-abelian group. One should make a comment about the relationship of the group in Example 2.2. 7 to that in Example 2.2.6. Clearly, the group of Example 2.2. 7 is a subset ofthat in Example 2.2.6. However, more is true. Relative to the same operation, as an entity in its own right, it forms a group. One could describe the situation by declaring it to be a subgroup of the group of Example 2.2.6. We shall see much more about the concept of subgroup in a few pages. #Example 2.2.8 Let G be the set of all 2 x 2 matrices ( a b), -b a where a and b are real numbers, not both 0. (We can state this more succinctly by saying that a 2 + b2 =F 0.) Using the same operation as in the preceding two examples, we can easily show that G becomes a group. In fact, G is an infinite, abelian group. Sec. 2.3 Some Preliminary lemmas 33 Does the multiplication in G remind you of anything? Write ( a b) ( 0 1) -b a as al + bj where ] = and compute the product in these terms. -1 0 Perhaps that will ring a bell with you. #Example 2.2.9 Let G be the set of all 2 x 2 matrices (: ~) where a, b, c, d are integers modulo p, p a prime number, such that ad - be =I 0. Define the multiplication in G as we did in Example 2.2.6, understanding the multiplication and addition of the entries to be those modulo p. We leave it to the reader to verify that G is a non-abelianjinite group. In fact, how many elements does G have? Perhaps it might be instructive for the reader to try the early cases p = 2 and p = 3. Here one can write down all the elements of G explicitly. (A word of warning! For p = 3, G already has 48 elements.) To get the case of a general prime, p will require an idea rather than a direct hacking-out of the answer. Try it! 2.3 Some Preliminary Lemmas We have now been exposed to the theory of groups for several pages and as yet not a single, solitary fact has been proved about groups. It is high time to remedy this situation. Although the first few results we demonstrate are, admittedly, not very exciting (in fact, they are rather dull) they will be extremely useful. Learning the alphabet was probably not the most interesting part of our childhood education, yet, once this hurdle was cleared, fascin;ting vistas were opened before us. We begin with LEMMA 2.3.1 If G is a group, then a. The identity element of G is unique. b. Every a E G has a unique inverse in G. c. For every a E G, (a- 1)- 1 =a. d. For all a, b E G, (a · b)- 1 = b- 1 • a- 1 . Proot. Before we proceed with the proof itself it might be advisable to see what it is that we are going to prove. In part (a) we want to show that if two elements e and fin G enjoy the property that for every a E G, a = a · e = e · a = a · f = f · a, then e =f. In part (b) our aim is to show that if x · a = a · x = e and y · a = a · y = e, where all of a, x, y are in G, then X =y. I 1 34 Group Theory Ch. 2 First let us consider part (a). Since e ·a = a for every a E G, then, in particular, e · f = f. But, on the other hand, since b · f = b for every b E G, we must have that e · f = e. Piecing these two bits of information together we obtainf = e · f = e, and so e =f. Rather than proving part (b), we shall prove something stronger which immediately will imply part (b) as a consequence. Suppose that for a in G, a· x = e and a ·y = e; then, obviously, a· x = a ·y. Let us make this our starting point, that is, assume that a· x = a ·y for a, x,y in G. There is an element b E G such that b ·a = e (as far as we know yet there may be several such b's). Thus b · (a· x) = b ·(a ·y); using the associative law this leads to x = e · x = (b ·a) · x = b ·(a· x) = b · (a ·y) = (b ·a) ·y = e ·y = y. We have, in fact, proved that a· x = a ·y in a group G forces x = y. Similarly we can prove that x ·a = y ·a implies that x = y. This says that we can cancel, from the same side, in equations in groups. A note of caution, however, for we cannot conclude that a · x = y · a implies x = y for we have no way of knowing whether a . X = X • a. This is illustrated in s3 with a = ¢, X= t/J, y = t/J-1. Part (c) follows from this by noting that a- 1 • (a- 1) - 1 = e = a- 1 ·a; canceling off the a- 1 on the left leaves us with (a- 1 ) - 1 = a. This is the analog in general groups of the familiar result - ( -5) = 5, say, in the group of real numbers under addition. Part (d) is the most trivial of these, for and so by the very definition of the inverse, (a · b)- 1 = b- 1 · a- 1 . Certain results obtained in the proof just given are important enough to single out and we do so now in LEMMA 2.3.2 Given a, b in the group G, then the equations a · x = b and y ·a = b have unique solutions for x andy in G. In particular, the two cancellation laws, a · u = a · w implies u = w and u·a w · a implies u w hold in G. , The few details needed for the proof of this lemma are left to the reader. Sec. 2.3 Some Preliminary Lemmas 35 problems 1. In the following determine whether the systems described are groups. If they are not, point out which of the group axioms fail to hold. (a) G = set of all integers, a· b = a - b. (b) G = set of all positive integers, a · b = ab, the usual product of integers. (c) G = a0 , a 1 , ... , a6 where ai · a i = ai + i if i + j < 7, ai · a i = ai + i _ 7 if i + j ;:::: 7 (for instance, a 5 • a 4 = a 5 + 4 _ 7 = a2 since 5 + 4 = 9 > 7). (d) G = set of all rational numbers with odd denominators, a· b _ a + b, the usual addition of rational numbers. 2. Prove that if G is an abelian group, then for all a, b E G and all integers n, (a · b) n = an · bn. 3. If G is a group such that (a· b) 2 = a2 • b2 for all a, bEG, show that G must be abelian. *4. If G is a group in which (a· b)i = ai · bi for three consecutive integers i for all a, b E G, show that G is abelian. 5. Show that the conclusion of Problem 4 does not follow if we assume the relation (a· b)i = ai · bi for just two consecutive integers. 6. In S3 give an example of two elements x,y such that (x ·y) 2 =1 x 2 ·y 2 • 7. In S3 show that there are four elements satisfying x 2 = e and three elements satisfying y 3 = e. 8. If G is a finite group, show that there exists a positive integer N such that aN = e for all a E G. 9. (a) If the group G has three elements, show it must be abelian. (b) Do part (a) if G has four elements. (c) Do part (a) if G has five elements. 10. Show that if every element of the group G is its own inverse, then G is abelian. 11. If G is a group of even order, prove it has an element a =P e satisfying a2 =e. 12. Let G be a nonempty set closed under an associative product, which in addition satisfies: (a) There exists an e E G such that a· e = a for all a E G. (b) Give a E G, there exists an elementy(a) E G such that a·y(a) = e. Prove that G must be a group under this product. 36 Group Theory Ch. 2 13. Prove, by an example, that the conclusion of Problem 12 is false if we assume instead : (a') There exists an e E G such that a· e = a for all a E G. (b') Given a E G, there existsy(a) E G such thaty(a) ·a = e. 14. Suppose afinite set G is closed under an associative product and that both cancellation laws hold in G. Prove that G must be a group. 15. (a) Using the result of Problem 14, prove that the nonzero integers modulo p, p a prime number, form a group under multiplication modp. (b) Do part (a) for the nonzero integers relatively prime to n under multiplication mod n. 16. In Problem 14 show by an example that if one just assumed one of the cancellation laws, then the conclusion need not follow. 17. Prove that in Problem 14 infinite examples exist, satisfying the conditions, which are not groups. 18. For any n > 2 construct a non-abelian group of order 2n. (Hint: imitate the relations in s3.) 19. If S is a set closed under an associative operation, prove that no matter how you bracket a 1 a2 • • • am retaining the order of the elements, you get the same element in S (e.g., (a1 · a2 ) • (a3 • a4 ) = a 1 • (a2 • (a3 • a4 )); use induction on n). #20. Let G be the set of all real2 x 2 matrices (: ~} where ad - be # 0 is a rational number. Prove that G forms a group under matrix multiplication. #21. Let G be the set of all real 2 x 2 matrices(~ !) where ad # 0. Prove that G forms a group under matrix multiplication. Is G abelian? #22. Let G be the set of all real 2 x 2 matrices (a 0 ) where a 'I= 0. 0 a- 1 Prove that G is an abelian group under matrix multiplication. #23. Construct in the G of Problem 21 a subgroup of order 4. #24. Let G be the set of all 2 x 2 matrices (: !) where a, b, c, d are integers modulo 2, such that ad - be 'I= 0. Using matrix multi- plication as the operation in G, prove that G is a group of order 6. #25. (a) Let G be the group of all 2 x 2 matrices (a b) where , c d ad - be 'I= 0 and a, b, c, d are integers modulo 3, relative to matrix multiplication. Show that o(G) = 48. Sec. 2.4 Subgroups 37 (b) If we modify the example of G in part (a) by insisting that ad - be = 1, then what is a( G)? #*26. (a) Let G be the group of all 2 x 2 matrices(: ~)where a, b, c, d are integers modulo p, p a prime number, such that ad - be =j:. 0. G forms a group relative to matrix multiplication. What is o(G)? (b) Let H be the subgroup of the G of part (a) defined by H = { (: ~) E G I ad - be = I}- What is o(H)? 2.4 Subgroups Before turning to the study of groups we should like to change our notation slightly. It is cumbersome to keep using the · for the group operation; henceforth we shall drop it and instead of writing a· b for a, bEG we shall simply denote this product as ab. In general we shall not be interested in arbitrary subsets of a group G for they do not reflect the fact that G has an algebraic structure imposed on it. Whatever subsets we do consider will be those endowed with algebraic properties derived from those of G. The most natural such subsets are introduced in the DEFINITION A nonempty subset H of a group G is said to be a subgroup of G if, under the product in G, H itself forms a group. The following remark is clear: if His a subgroup of G and K is a subgroup of H, then K is a subgroup of G. It would be useful to have some criterion for deciding whether a given subset of a group is a subgroup. This is the purpose of the next two lemmas. LEMMA 2.4.1 A nonempty subset H of the group G is a subgroup of G if and onry if 1. a, b E H implies that ab E H. 2. a E H implies that a- 1 E H. Proof. If His a subgroup of G, then it is obvious that ( 1) and (2) must hold. Suppose conversely that H is a subset of G for which ( 1) and (2) hold. In order to establish that His a subgroup, all that is needed is to verify that e E H and that the associative law holds for elements of H. Since the as- sociative law does hold for G, it holds all the more so for H, which is a 38 Group Theory Ch. 2 subset of G. If a E H, by part 2, a- 1 E Hand so by part 1, e = aa- 1 E H. This completes the proof. In the special case of a finite group the situation becomes even nicer for there we can dispense with part 2. LEMMA 2.4.2 If H is a nonempty finite subset of a group G and H is closed under multiplication, then His a subgroup of G. Proof. In light of Lemma 2.4.1 we need but show that whenever a E H, then a- 1 E H. Suppose that a E H; thus a 2 = aa E H, a 3 = a 2a E H, ... , am E H, ... since His closed. Thus the infinite collection of elements a,a 2 , ••• , am, ... must all fit into H, which is a finite subset of G. Thus there must be repetitions in this collection of elements; that is, for some integers r, s with r > s > 0, a' = a 8 • By the cancellation in G, ar-s = e (whence e is in H); since r - s - 1 ~ 0, ar-s- 1 E H and a- 1 = ar-s- 1 since aar-s- 1 = ar-s = e. Thus a- 1 E H, completing the proof of the lemma. The lemma tells us that to check whether a subset of a finite group is a subgroup we just see whether or not it is closed under multiplication. We should, perhaps, now see some groups and some of their subgroups. G is always a subgroup of itself; likewise the set consisting of e is a subgroup of G. Neither is particularly interesting in the role of a subgroup, so we describe them as trivial subgroups. The subgroups between these two extremes we call nontrivial subgroups and it is in these we shall exhibit the most interest. Example 2.4.1 Let G be the group of integers under addition, H the subset consisting of all the multiples of 5. The student should check that H is a subgroup. In this example there is nothing extraordinary about 5; we could similarly define the subgroup Hn as the subset of G consisting of all the multiples of n. Hn is then a subgroup for every n. What can one say about Hn n Hm? It might be wise to try it for H 6 n H 9 . Example 2.4.2 Let S be any set, A(S) the set of one-to-one mappings of S onto itself, made into a group under the composition of mappings. If x0 E S, let H(x0 ) = {</J E A(S) I x0 </J = x0 }. H(x0 ) is a subgroup of A(S). Iffor x1 # x0 E Swe similarly define H(x 1 ), what is H(x0 ) n H(x 1 )? Example 2.4.3 Let G be any group, a E G. Let (a) = {a! I i = 0, ± 1, ±2, ... }. (a) is a subgroup of G (verify!); it is called the cyclic subgroup generated by a. This provides us with a ready means of producing subgroups Sec. 2.4 Subgroups 39 of G. If for some choice of a, G = (a), then G is said to be a cyclic group. Such groups are very special but they play a very important role in the theory of groups, especially in that part which deals with abelian groups. Of course, cyclic groups are abelian, but the converse is false. Example 2.4.4 Let G be a group, W a subset of G. Let ( W) be the set of all elements of G representable as a product of elements of W raised to positive, zero, or negative integer exponents. ( W) is the subgroup of G generated by Wand is the smallest subgroup of G containing W. In fact, (W) is the intersection of all the subgroups of G which contain W (this intersec- tion is not vacuous since G is a subgroup of G which contains W). Example 2.4.5 Let G be the group of nonzero real numbers under multiplication, and let H be the subset of positive rational numbers. Then His a subgroup of G. Example 2.4.6 Let G be the group of all real numbers under addition, and let H be the set of all integers. Then His a subgroup of G. #Example 2.4.7 Let G be the group of all real 2 x 2 matrices(: ~) with ad - be =I= 0 under matrix multiplication. Let Then, as is easily verified, H is a subgroup of G. #Example 2.4.8 Let H be the group of Example 2.4. 7, and let K = {(~ ~)}· Then K is a subgroup of H. Example 2.4.9 Let G be the group of all nonzero complex numbers a + bi (a, b real, not both 0) under multiplication, and let H = {a + bi E G I a 2 + b 2 = 1 } . Verify ~.-hat His a subgroup of G. DEFINITION Let G be a group, H a subgroup of G; for a, bEG we say a is congruent to b mod H, written as a = b mod H if ab- 1 E H. LEMMA 2.4.3 The relation a = b mod His an equivalence relation. 40 Group Theory Ch. 2 Proof. If we look back in Chapter 1, we see that to prove Lemma 2.4.3 we must verify the following three conditions: For all a, b, c E G, 1. a= a mod H. 2. a= b mod Himplies b =a mod H. 3. a = b mod H, b = c mod H implies a = c mod H. Let's go through each of these in turn. 1. To show that a = a mod H we must prove, using the very definition of congruence mod H, that aa- 1 E H. Since His a subgroup of G, e E H, and since aa- 1 = e, aa- 1 E H, which is what we were required to demon- strate. 2. Suppose that a = b mod H, that is, suppose ab- 1 E H; we want to get from this b = a mod H, or, equivalently, ba- 1 E H. Since ab- 1 E H, which is a subgroup of G, (ab- 1 ) - 1 E H; but, by Lemma 2.3.1, (ab- 1 )- 1 = ( b- 1 ) - 1 a- 1 = ba- 1, and so ba- 1 E H and b = a mod H. 3. Finally we require that a = b mod H and b = c mod H forces a = c mod H. The first congruence translates into ab- 1 E H, the second into be- 1 E H; using that His a subgroup of G, (ab- 1 ) (be- 1 ) E H. How- ever, ac- 1 = aec- 1 = a(b- 1b)c- 1 = (ab- 1 )(bc- 1 ); hence ac- 1 E H, from which it follows that a = c mod H. This establishes that congruence mod H is a bona fide equivalence relation as defined in Chapter 1, and all results about equivalence relations have become available to us to be used in examining this particular relation. A word about the notation we used. If G were the group of integers under addition, and H = Hn were the subgroup consisting of all multiples of n, then in G, the relation a = b mod H, that is, ab- 1 E H, under the additive notation, reads \"a - b is a multiple ofn.\" This is the usual number theoretic congruence mod n. In other words, the relation we defined using an arbitrary group and subgroup is the natural generalization of a familiar relation in a familiar group. DEFINITION If His a subgroup of G, a e G, then Ha Ha is called a right coset of H in G. LEMMA 2.4.4 For all a E G, Ha = {x E G I a = x mod H}. {ha I hE H}. Proof. Let [a] = {x E G I a = x mod H}. We first show that Ha c [a]. For, if hE H, then a(ha)- 1 = a(a- 1h- 1 ) = h- 1 E H since His a subgroup of G. By the definition of congruence mod H this implies 'that ha E [a] for every hE H, and so Ha c [a]. Suppose, now, that x E [a]. Thus ax- 1 e H, so (ax- 1 ) -t = xa- 1 is Sec. 2.4 Subgroups 41 also in H. That is, xa- 1 = h for some h E H. Multiplying both sides by a from the right we come up with x = ha, and so x E Ha. Thus [a] c Ha. Having proved the two inclusions [a] c Ha and Ha c [a], we can conclude that [a] = Ha, which is the assertion of the lemma. In the terminology of Chapter 1, [a], and thus Ha, is the equivalence class of a in G. By Theorem 1.1.1 these equivalence classes yield a decomposition of G into disjoint subsets. Thus arry two right cosets qf H in G either are identical or have no element in common. We now claim that between any two right cosets Ha and Hb of H in G there exists a one-to-one correspondence, namely, with any element ha E Ha, where hE H, associate the element hb E Hb. Clearly this mapping is onto Hb. We aver that it is a one-to-one correspondence, for if h1b = h2 b, with hv h2 E H, then by the cancellation law in G, h1 = h2 and so h1 a = h2a. This proves LEMMA 2.4.5 There is a one-to-one correspondence between any two right cosets ofHin G. Lemma 2.4.5 is of most interest when His a finite group, for then it merely states that any two right cosets of H have the same number of elements. How many elements does a right coset of H have? Well, note that H = He is itself a right coset of H, so any right coset of H in G has o(H) elements. Suppose now that G is a finite group, and let k be the number of distinct right cosets of H in G. By Lemmas 2.4.4 and 2.4.5 any two distinct right cosets of H in G have no element in common, and each has o(H) elements. Since any a E G is in the unique right coset Ha, the right cosets fill out..,G. Thus if k represents the number of distinct right cosets of H in G we must have that ko(H) = o(G). We have proved the famous theorem due to Lagrange, namely, THEOREM 2.4.1 If G is a finite group and His a subgroup qf G, then o(H) is a divisor qf o( G). DEFINITION If His a subgroup of G, the index qf H in G is the number of distinct right cosets of H in G. We shall denote it by i6 (H). In case G is a finite group, i6 (H) = o(G)fo(H), as became clear in the proof of Lagrange's theorem. It is quite possible for an infinite group G to have a subgroup H =/= G which is of finite index in G. It might be difficult, at this point, for the student to see the extreme importance of this result. As the subject is penetrated more deeply one will 42 Group Theory Ch. 2 become more and more aware of its basic character. Because the theorem is of such stature it merits a little closer scrutiny, a little more analysis, and so we give, below, a slightly different way of looking at its proof. In truth, the procedure outlined below is no different from the one already given. The introduction of the congruence mod H smooths out the listing of elements used below, and obviates the need for checking that the new elements introduced at each stage did not appear before. So suppose again that G is a finite group and that H is a subgroup of G. Let h1 , h2 , ... , h, be a complete list of the elements of H, r = o(H). If H = G, there is nothing to prove. Suppose, then, that H =I= G; thus there is an a E G, a ¢= H. List all the elements so far in two rows as h1, h2 , ••• , h, h1 a, h2 a, ... , h,a. We claim that all the entries in the second line are different from each other and are different from the entries in the first line. If any two in the second line were equal, then hia = hia with i =I= j, but by the cancellation law this would lead to hi = hi, a contradiction. If an entry in the second line were equal to one in the first line, then hia = hi, resulting in a = hi- 1hi E H since His a subgroup of G; this violates a¢= H. Thus we have, so far, listed 2o(H) elements; if these elements account for all the elements of G, we are done. If not, there is a b E G which did not occur in these two lines. Consider the new list h1, h2 , ••• , h, h1 a, h2 a, ... , h,a, h1 b, h2 b, ... , h,b. As before (we are now waving our hands) we could show that no two entries in the third line are equal to each other, and that no entry in the third line occurs in the first or second line. Thus we have listed 3o(H) elements. Continuing in this way, every new element introduced, in fact, produces o(H) new elements. Since G is a finite group, we must eventually exhaust all the elements of G. But if we ended up using k lines to list all the elements of the group, we would have written down ko(H) distinct elements, and so ko(H) = o(G). It is essential to point out that the converse to Lagrange's theorem is false-a group G need not have a subgroup of order m if m is a divisor of o(G). For instance, a group of order 12 exists which has no subgroup of order 6. The reader might try to find an example of this phenomenon; the place to look is in s4, the symmetric group of degree 4 which has a sub- group of order 12, which will fulfill our requirement. Lagrange's theorem has some very important corollaries. Before we present these we make one definition. ' Sec. 2.4 Subgroups 43 DEFINITION If G is a group and a E G, the order (or period) of a is the least positive integer m such that am = e. If no such integer exists we say that a is of infinite order. We use the notation o(a) for the order of a. Recall our other notation: for two integers u, v, u I v reads \"u is a divisor of v.\" COROLLARY 1 If G is a .finite group and a E G, then o(a) I o(G). Proof. With Lagrange's theorem already in hand, it seems most natural to prov~ the corollary by exhibiting a subgroup of G whose order is o(a). The element a itself furnishes us with this subgroup by considering the cyclic subgroup, (a), of G generated by a; (a) consists of e, a, a 2 , •••• How many elements are there in (a)? We assert that this number is the order of a. Clearly, since ao(a) = e, this subgroup has at most o(a) elements. If it should actually have fewer than this number of elements, then ai = ai for some integers 0 ~ i < j < o(a). Then ai-i = e, yet 0 < j - i < o(a) which would contradict the very meaning of o(a). Thus the cyclic sub- group generated by a has o(a) elements, whence, by Lagrange's theorem, o(a) I o(G). COROLLARY 2 If G is a .finite group and a E G, then ao(G) = e. Proof. By Corollary 1, o(a) I o(G); thus o(G) = mo(a). Therefore, ao(G) = amo(a) = (ao(a))m = em = e. A particular case of Corollary 2 is of great interest in number the.,9ry. The Euler ¢-function, cp(n), is defined for all integers n by the following: c/>(1) = 1; for n > 1, cp(n) = number of positive integers less than nand relatively prime to n. Thus, for instance, ¢(8) = 4 since only 1, 3, 5, 7 are the numbers less than 8 which are relatively prime to 8. In Problem 15 (b) at the end of Section 2.3 the reader was asked to prove that the numbers less than n and relatively prime to n formed a group under multiplication mod n. This group has order cp(n). If we apply Corollary 2 to this group we obtain COROLLARY 3 (EuLER) If n is a positive integer and a is relatively prime to n, tht:n atf>(n) = 1 mod n. In order to apply Corollary 2 one should replace a by its remainder on division by n. If n should be a prime number p, then cp(p) = p - 1. If a is an integer relatively prime to p, then by Corollary 3, ap- 1 = 1 mod p, Whence aP = a mod p. If, on the other hand, a is not relatively prime top, 44 Group Theory Ch. 2 since p is a prime number, we must have that p I a, so that a = 0 mod p; hence 0 = aP = a mod p here also. Thus COROLLARY 4 (FERMAT) If pis a prime number and a is any integer, then aP = a modp. COROLLARY 5 If G is a finite group whose order is a prime number p, then G is a cyclic group. Proof. First we claim that G has no nontrivial subgroups H; for o(H) must divide o(C) = p leaving only two possibilities, namely, o(H) = 1 or o(H) = p. The first of these implies H = (e), whereas the second implies that 1-1 = G. Suppose now that a =/:- e E G, and let H = (a). His a sub- group of G, H =j:. (e) since a =j:. e E H. Thus H = G. This says that G is cyclic and that every element in G is a power of a. This section is of great importance in all that comes later, not only for its results but also because the spirit of the proof5 occurring here are genuinely group-theoretic. The student can expect to encounter other arguments having a similar flavor. It would be wise to assimilate the material and approach thoroughly, now, rather than a few theorems later when it will be too late. 2.5 A Counting Principle As we have defined earlier, if H is a subgroup of G and a E G, then Ha consists of all elements in G of the form ha where hE H. Let us generalize this notion. If II, K are two subgroups of G, let HK = {x E G I x = hk, h E H, k E K}. Let's pause and look at an example; in S3 let H = {e, cp }, K = {e, cpl/J }. Since c/>2 = (c/>l/1) 2 = e, both Hand K are subgroups. What can we say about HK? Just using the definition of HK we can see that HK consists of the elements e, cp, cpl/J, cp2 l/J = l/J. Since HK consists of four elements and 4 is not a divisor of 6, the order of S3 by Lagrange's theorem HK could not be a subgroup of S3 • (Of course, we could verify this directly but it does not hurt to keep recalling Lagrange's theorem.) We might try to find out why HK is not a subgroup. Note that KH = {e, cp, cpljf, cpl/Jcp = ljl- 1 } =j:. HK. This is precisely why HK fails to be a subgroup, as we see in the next lemma. LEMMA 2.5.1 HK is a subgroup of G if and only if HK = Kif. Proof. Suppose, first, that HK = KH; that is, if hE H and k E K, then hk = k1 h1 for some k 1 E K, h1 E H (it need not be that k1 = k or Sec. 2.5 A Counting Principle 45 k = h!). To prove that HK is a subgroup we must verify that it is closed a~d every element in HK has its inverse in HK. Let's show the closure first; so suppose x = hk E HK and y = h' k' E HK. Then xy = hkh' k', but since kh' E KH = HK, kh' = h2k2 with h2 E H, k2 E K. Hence ~ k(k2k2 )k' = (hh2) (k2k') E HK, and HK is closed. Also x- 1 = (hk)- 1 = k-tk- 1 E KH = HK, so x- 1 E HK. Thus HK is a subgroup of G. On the other hand, if HK is a subgroup of G, then for any h E H, k E K, k-tk- 1 E HK and so kh = (h- 1k- 1) - 1 E HK. Thus KH c HK. Now if xis any element of HK, x- 1 = hk E HK and so x = (x- 1)- 1 = (hk)- 1 = k-tk- 1 E KH, so HK c KH. Thus HK = KH. An interesting special case is the situation when G is an abelian group for in that case trivially HK = KH. Thus as a consequence we have the COROLLARY If H, K are subgroups of the abelian group G, then HK is a subgroup of G. If H, K are subgroups of a group G, we have seen that the subset HK need not be a subgroup of G. Yet it is a perfect meaningful question to ask: How many distinct elements are there in the subset HK? If we denote this number by o(HK), we prove THEOREM 2.5.1 If H and K are finite subgroups of G of orders o(H) and o(K), respectively, then o(HK) = o(H)o(K) . o(H n K) Proof. Although there is no need to pay special attention to the particuTar case in which H n K = (e), looking at this case, which is devoid of some of the complexity of the general situation, is quite revealing. Here we should seek to show that o(HK) = o(H)o(K). One should ask oneself: How could this fail to happen? The answer clearly must be that if we list all the elements hk, h E H, k E K there should be some collapsing; that is, some element in the list must appear at least twice. Equivalently, for some h =fi h1 E H, hk = h1k1. But then h1 - 1h = k1k- 1; now since h1 E H, hl -t must also be in H, thus h1 - 1h E H. Similarly, k1k- 1 E K. Since ht- 1h = k1k- 1, h1- 1hEHnK =(e), so h1- 1h = e, whence h = h1, a contradiction. We have proved that no collapsing can occur, and so, here, 0 (HK) is indeed o(H)o(K). With this experience behind us we are ready to attack the general case. As above we must ask: How often does a given element hk appear as a Product in the list of HK? We assert it must appear o(H n K) times! l'o see this we first remark that if h1 E H n K, then (1) I, I I I 'rill 46 Group Theory Ch. 2 where hh1 E H, since h E H, h1 E H n K c H and h1 - 1 k E K since h1 - 1 E H n K c K and k E K. Thus hk is duplicated in the product at least o(H n K) times. However, if hk = h'k', then h- 1h' = k(k')- 1 = u, and u E H n K, and so h' = hu, k' = u- 1k; thus all duplications were accounted for in ( 1). Consequently hk appears in the list of HK exactly o(H n K) times. Thus the number of distinct elements in HK is the total number in the listing of HK, that is, o(H)o(K) divided by the number of times a given element appears, namely, o(H n K). This proves the theorem. Suppose H, K are subgroups of the finite- group G and o(H) > .J o(G), o(K) > .Jo(G). Since HK c G, o(HK) ~ o(G). However, o(G) ~ o(HK) = o(H)o(K) > .j-;(c).j-;;{G) o(H n K) o(H n K) o(G) o(H n K) ' thus o(H n K) > 1. Therefore, H n K =1- (e). We have proved the COROLLARY lf Hand K are subgroups of G and o(H) > .Jo(G), o(K) > .J o( G), then li n K =1- (e). We apply this corollary to a very special group. Suppose G is a finite group of order pq where p and q are prime numbers with p > q. We claim that G can have at most one subgroup of order p. For suppose H, K are subgroups of order p. By the corollary, H n K =1- (e), and being a sub- group of H, which having prime order has no nontrivial subgroups, we must conclude that H n K = H, and so H c H n K c K. Similarly K c H, whence H = K, proving that there is at most one subgroup of order p. Later on we shall see that there is at least one subgroup of order p, which, combined with the above, will tell us there is exactly one subgroup of order p in G. From this we shall be able to determine completely the structure of G. Problems 1. If Hand K are subgroups of G, show that H n K is a subgroup of G. (Can you see that the same proof shows that the intersection of any number of subgroups of G, finite or infinite, is again a subgroup of G?) 2. Let G be a group such that the intersection of all its subgroups which are different from (e) is a subgroup different from (e). Prove that every element in G has finite order. 3. If G has no nontrivial subgroups, show that G must be finite of prime order. I Sec. 2.5 A Counting Principle 47 4. (a) If His a subgroup of G, and a E G let aHa- 1 Show that aHa- 1 is a subgroup of G. {aha- 1 I hE H}. (b) If His finite, what is o(aHa- 1)? 5. For a subgroup H of G define the left coset aH of H in G as the set of all elements of the form ah, h E H. Show that there is a one-to-one correspondence between the set of left cosets of H in G and the set of right cosets of H in G. 6. Write out all the right cosets of H in G where (a) G = (a) is a cyclic group of order 10 and H = (a 2 ) is the subgroup of G generated by a2 . (b) Gas in part (a), H = (as) is the subgroup of G generated by as. (c) G = A(S), S = {x1 , x2 , x3 }, and H = {cr E G I x1cr = xd. 7. Write out all the left cosets of H in G for Hand G as in parts (a), (b), (c) of Problem 6. 8. Is every right coset of H in G a left coset of H in G in the groups of Problem 6? 9. Suppose that H is a subgroup of G such that whenever Ha =f:. Hb then aH =f:. bH. Prove that gHg- 1 c H for ~ll g E G. 10. Let G be the group of integers under addition, Hn the subgroup consisting of all multiples of a fixed integer n in G. Determine the index of Hn in G and write out all the right cosets of Hn in G. 11. In Problem 10, what is Hn n Hm? 12. If G is a group and H, K are two subgroups of finite index in G, prove that H n K is of finite index in G. Can you find an upper bound for the index of H n K in G? ..,. 13. If a E G, define N(a) = {x E G I xa = ax}. Show that N(a) is a subgroup of G. N(a) is usually called the normalizer or centralizer of a in G. 14. If His a subgroup of G, then by the centralizer C(H) of H we mean the set {x E G I xh = hx all h E H}. Prove that C(H) is a subgroup of G. 15. The center Z of a group G is defined by Z = {z E G I zx = xz all x E G}. Prove that Z is a subgroup of G. Can you recognize Z as C( T) for some subgroup T of G? 16. If H is a subgroup of G, let N(H) = {a E G 1 aHa- 1 = H} [see Problem 4(a)]. Prove that (a) N(H) is a subgroup of G. (b) N(H) :::> H. 17. Give an example of a group G and a subgroup H such that N(H) =f:. C(H). Is there any containing relation between N(H) and C(H)? 48 Group Theory Ch. 2 18. If H is a subgroup of G let N = n xHx- 1 • xeG Prove that N is a subgroup of G such that aNa- 1 = N for all a E G. * 19. If H is a subgroup of finite index in G, prove that there is only a finite number of distinct subgroups in G of the form aHa- 1. *20. If H is of finite index in G prove that there is a subgroup N of G, contained in H, and of finite index in G such that aNa- 1 = N for all a E G. Can you give an upper bound for the index of this Nin G? 21. Let the mapping Lab for a, b real numbers, map the reals into the reals by the rule Lab :x ---+ ax + b. Let G = {Lab I a =F 0}. Prove that G is a group under the composition of mappings. Find the formula for LabLcd· 22. In Problem 21, let H = {Lab E G I a is rational}. Show that H is a subgroup of G. List all the right cosets of H in G, and all the left cosets of H in G. From this show that every left coset of H in G is a right coset of H in G. 23. In the group G of Problem 21, let N = {Llb E G}. Prove (a) N is a subgroup of G. (b) If a E G, n E N, then ana- 1 E N. *24. Let G be a finite group whose order is not divisible by 3. Suppose that (ab) 3 = a 3 b3 for all a, b E G. Prove that G must be abelian. *25. Let G be an abelian group and suppose that G has elements of orders m and n, respectively. Prove that G has an element whose order is the least common multiple of m and n. * *26. If an abelian group has subgroups of orders m and n, respectively, then show it has a subgroup whose order is the least common multiple of m and n. (Don't be discouraged if you don't get this problem with what you know about group theory up to this stage. I don't know anybody, including myself, who has done it subject to the restriction of using material developed so far in the text. But it is fun to try. I've had more correspondence about this problem than about any other point in the whole book.) 27. Prove that any subgroup of a cyclic group is itself a cyclic group. 28. How many generators does a cyclic group of order n have? (bEG is a generator if (b) = G.) Let Un denote the integers relatively prime to n under multiplication mod n. In Problem 15(b), Section 2.3, it is indicated that Un is a group. Sec. 2.6 Normal Subgroups and Quotient Groups 49 In the next few problems we look at the nature of Un as a group for some specific values of n. 29. Show that U 8 is not a cyclic group. 30. Show that U 9 is a cyclic group. What are all its generators? 31. Show that U17 is a cyclic group. What are all its generators? 32. Show that U18 is a cyclic group. 33. Show that U20 is not a cyclic group. 34. Show that both U25 and U27 are cyclic groups. 35. Hazard a guess at what all the n such that Un is cyclic are. (You can verify your guess by looking in any reasonable book on number theory.) 36. If a E G and am = e, prove that o(a) I m. 37. If in the group G, a 5 = e, aba- 1 = b2 for some a, bEG, find o(b). *38. Let G be a finite abelian group in which the number of solutions in G of the equation xn = e is at most n for every positive integer n. Prove that G must be a cyclic group. 39. Let G be a group and A, B subgroups of G. If x,y E G define x \"'y ify = axb for some a E A, bE B. Prove (a) The relation so defined is an equivalence relation. (b) The equivalence class of x is AxB = {axb I a E A, b E B}. (AxB is called a double coset of A and Bin G.) 40. If G is a finite group, show that the number of elements in the double coset AxB is o(A)o(B) o(A n xBx- 1 ). 41. If G is a finite group and A is a subgroup of G such that all double cosets AxA have the same number of elements, show that gAg- 1 = A for all g E G. 2.6 Normal Subgroups and Quotient Groups Let G be the group S3 and let H be the subgroup {e, 4> }. Since the index of H in G is 3, there are three right cosets of H in G and three left cosets of ll in G. We list them : Right Cosets H= {e,</>} Ht/J = {t/1, </>t/1} Ht/12 = {tjJ2, </>t/12} Left Cosets H= {e,<f>} t/JH = {t/1, t/14> = </>t/12} t/J2H = {tjJ2, t/12</> = </>t/1} 50 Group Theory Ch. 2 A quick inspection yields the interesting fact that the right coset Htjl is not a left coset. Thus, at least for this subgroup, the notions of left and right coset need not coincide. In G = S3 let us consider the subgroup N = {e, t/1, t/12 }. Since the index of N in G is 2 there are two left cosets and two right cosets of N in G. We list these: Right Cosets N = {e, l/J, l/12} N¢ = {¢, l/1¢, l/12¢} Left Cosets N = {e, l/J, t/12 } ¢N = {¢, </Jl/1, </Jl/12} = {¢, t/12¢, l/14>} A quick inspection here reveals that every left coset of N in G is a right coset in G and conversely. Thus we see that for some subgroups the notion of left coset coincides with that of right coset, whereas for some subgroups these concepts differ. It is a tribute to the genius of Galois that he recognized that those sub- groups for which the left and right cosets coincide are distinguished ones. Very often in mathematics the crucial problem is to recognize and to discover what are the relevant concepts; once this is accomplished the job may be more than half done. We shall define this special class of subgroups in a slightly different way, which we shall then show to be equivalent to the remarks in the above paragraph. DEFINITION A subgroup N of G is said to be a normal subgroup of G if for every g E G and n E N, gng- 1 E N. Equivalently, if by gNg- 1 we mean the set of all gng- 1, n EN, then N is a normal subgroup of G if and only if g Ng- 1 c N for every g E G. LEMMA 2.6.1 N is a normal subgroup of G if and only if gNg- 1 = N for every g E G. Proof. If gNg- 1 = N for every g E G, certainly gNg- 1 c N, so N is normal in G. Suppose that N is normal in G. Thus if g E G, gNg- 1 c Nand g- 1 Ng :::: g- 1 N(g- 1 ) - 1 c N. Now, since g- 1 Ng c N, N = g(g- 1 Ng)g- 1 c gNg- 1 c N, whence N = gNg- 1 • In order to avoid a point of confusion here let us stress tha.t Lemma 2.6.1 does not say that for every n E N and every g E G, gng- 1 = n. No! This can be false. Take, for instance, the group G to be S3 and N to be the sub- I Sec. 2.6 Normal Subgroups and Quotient Groups 51 group {e, 1/J, 1/12 }. Ifwe compute ¢N¢- 1 we obtain {e, ¢1/1¢- 1 , ¢1/1 2¢- 1 }= {e, t/J2, t/J }, yet ¢1/1¢- 1 # 1/J. All we require is that the set of elements gNg- 1 be the same as the set of elements N. We now can return to the question of the equality of left cosets and right cosets. LEMMA 2.6.2 The subgroup N of G is a normal subgroup of G if and only if every left coset of N in G is a right coset of N in G. Proof. If N is a normal subgroup of G, then for every g E G, gNg- 1 = N, whence (gNg- 1 )g = Ng; equivalently gN = Ng, and so the left coset gN is the right coset Ng. Suppose, conversely, that every left coset of N in G is a right coset of N in G. Thus, for g E G, gN, being a left coset, must be a right coset. What right coset can it be? Since g = geE gN, whatever right coset gN turns out to be, it must contain the element g; however, g is in the right coset Ng, and two distinct right cosets have no element in common. (Remember the proof of Lagrange's theorem?) So this right coset is unique. Thus gN = Ng follows. In other words, gNg- 1 = Ngg- 1 = N, and soN is a normal subgroup of G. We have already defined what is meant by HK whenever H, K are subgroups of G. We can easily extend this definition to arbitrary subsets, and we do so by defining, for two subsets, A and B, of G, AB = {x E G I x = ah, a E A, b E B}. As a special case, what can we say when A = B = H, a subgroup of G? HH = { h1 h2 I h1 , h2 E H} c H since H is closed under multiplication. But HH ~ He = H since e E H. Thus HH = H. Suppose that N is a normal subgroup of G, and that a, bE G. Consider (Na) (Nb); since N is normal in G, aN = Na, and so NaNb = N(aN)b = N(Na)b = NNab Nab. What a world of possibilities this little formula opens! But before we get carried away, for emphasis and future reference we record this as LEMMA 2.6.3 A subgroup N of G is a normal subgroup of G if and only if the /Jroduct of two right cosets of N in G is again a right coset of N in G. Proof. If N is normal in G we have just proved the result. The proof of the other half is one of the problems at the end of this section. Suppose that N is a normal subgroup of G. The formula NaNb = Nab, for a, h E G is highly suggestive; the product of right cosets is a right coset. Can we use this product to make the collection of right cosets into a group? Indeed we can! This type of construction, often occurring in mathematics and usually called forming a quotient structure, is of the utmost importance. 52 Group Theory Ch. 2 Let G/ N denote the collection of right cosets of N in G (that is, the elements of Gj N are certain subsets of G) and we use the product of subsets of G to yield for us a product in Gj N. For this product we claim 1. X, Y E GjN implies XY E GjN; for X= Na, Y = Nb for some a, bEG, and XY = NaNb = Nab E GjN. 2. X, Y, Z E Gj N, then X = Na, Y = Nb, Z = Nc with a, b, c E G, and so (XY)Z = (NaNb)Nc = N(ab)Nc = N(ab)c = Na(bc) (since G is associative) = Na(Nbc) = Na(NbNc) = X(YZ). Thus the product in Gj N satisfies the associative law. 3. Consider the element N = Ne E G/ N. If X E Gj N, X = Na, a E G, so XN = NaNe = Nae = Na = X, and similarly NX = X. Con- sequently, Ne is an identity element for Gj N. 4. Suppose X = Na E GjN (where a E G); thus Na- 1 E GjN, and NaNa- 1 = Naa- 1 = Ne. Similarly Na- 1 Na = Ne. Hence Na- 1 is the inverse of Na in GfN. But a system which satisfies 1, 2, 3, 4 is exactly what we called a group. That is, THEOREM 2.6.1 If G is a group, N a normal subgroup of G, then GfN is also a group. It is called the quotient group or factor group of G by N. If, in addition, G is a finite group, what is the order of Gf N? Since Gj N has as its elements the right cosets of N in G, and since there are precisely iG(N) = o(G)fo(N) such cosets, we can say LEMMA 2.6.4 If G is a finite group and N is a normal subgroup of G, then o(GjN) = o(G)fo(N). We close this section with an example. Let G be the group of integers under addition and let N be the set of all multiplies of 3. Since the operation in G is addition we shall write the cosets of N in G as N + a rather than as Na. Consider the three cosets N, N + 1, N + 2. We claim that these are all the cosets of N in G. For, given a E G, a = 3b + c where b E G and c = 0, 1, or 2 (cis the remainder of a on division by 3). Thus N + a = N + 3b + c = ( N + 3b) + c ::::: N + c since 3b EN. Thus every coset is, as we stated, one of N, N + 1, or N + 2, and GfN = {N, N + 1, N + 2}. How do we add elements in GfN? Our formula NaNb = Nab translates into: (N + 1) + (N + 2) ::::: N + 3 = N since 3 E N; (N + 2) + (N + 2) = N + 4 \"= N + 1 and so on. Without being specific one feels that Gj N is closely related to the integers mod 3 under addition. Clearly what we did for 3 we could emulate I Sec. 2.6 Normal Subgroups and Quotient Groups 53 for any integer n, in which case the factor group should suggest a relation to the integers mod n under addition. This type of relation will be clarified in the next section. Problems 1. If H is a subgroup of G such that the product of two right cosets of H in G is again a right coset of H in G, prove that His normal in G. 2. If G is a group and H is a subgroup of index 2 in G, prove that H is a normal subgroup of G. 3. If N is a normal subgroup of G and H is any subgroup of G, prove that NH is a subgroup of G. 4. Show that the intersection of two normal subgroups of G is a normal subgroup of G. 5. If H is a subgroup of G and N is a normal subgroup of G, show that H n N is a normal subgroup of H. 6. Show that every subgroup of an abelian group is normal. *7. Is the converse of Problem 6 true? If yes, prove it, if no, give an example of a non-abelian group all of whose subgroups are normal. 8. Give an example of a group G, subgroup H, and an element a E G such that aHa- 1 c H but aHa- 1 =1= H. 9. Suppose H is the only subgroup of order o(H) in the finite group G. Prove that His a normal subgroup of G. 10. If His a subgroup of G, let N(H) = {g E G I gHg- 1 = H}. Pro¥e (a) N(H) is a subgroup of G. (b) His normal in N(H). (c) If His a normal subgroup of the subgroup KinG, then K c N(H) (that is, N(H) is the largest subgroup of Gin which His normal). (d) His normal in G if and only if N(H) = G. II. If N and M are normal subgroups of G, prove that N M is also a normal subgroup of G. *12. Suppose that N and M are two normal subgroups of G and that N n M = (e). Show that for any n E N, mE M, nm = mn. 13. Jf a cyclic subgroup T of G is normal in G, then show that every subgroup of Tis normal in G. *14. Prove, by an example, that we can find three groups E c F c G, where E is normal in F, F is normal in G, but E is not normal in G. 15. If N is normal in G and aEGis of order o(a), prove that the order, m, of Na in Gf N is a divisor of o(a). 54 Group Theory Ch. 2 16. If N is a normal subgroup in the finite group such that i6 ( N) and o(N) are relatively prime, show that any element x E G satisfying xo(N) = e must be in N. 17. Let G be defined as all formal symbols x~i, i = 0, i,J = 0, 1, 2, ... , n - 1 where we assume x~i = xi'yi' if and only if i = i', J = J' x 2 = yn = e, n > 2 xy = y- 1x. (a) Find the form of the product (x~i)(xk_yl) as xryP. (b) Using this, prove that G is a non-abelian group of order 2n. (c) If n is odd, prove that the center of G is (e), while if n is even the center of G is larger than (e). This group is known as a dihedral group. A geometric realization of this is obtained as follows: let y be a rotation of the Euclidean plane about the origin through an angle of 2njn, and x the reflection about the vertical axis. G is the group of motions of the plane generated by y and x. 18. Let G be a group in which, for some integer n > 1, (abt = anbn for all a, b E G. Show that (a) G(n) = { xn I X E G} is a normal subgroup of G. (b) G(n- 1) = {xn- 1 I X E G} is a normal subgroup of G. 19. Let G be as in Problem 18. Show (a) an- 1bn = bnan- 1 for all a, b E G. (b) (aba- 1b- 1 )n(n- 1 ) = e for all a, bE G. 20. Let G be a group such that (ab)P = aPbP for all a, b E G, where p is a prime number. Let S = {x E G I xPm = e for some m depending on x}. Prove (a) Sis a normal subgroup of G. (b) If C = GJS and if x E Cis such that xP = e then x = e. #21. Let G be the set of all real 2 x 2 matrices (~ !) where ad =I= 0, under matrix multiplication. Let N = ( (~ ~)). Prove that (a) N is a normal subgroup of G. (b) GJNis abelian. 2.7 Homomorphisms The ideas and results in this section are closely interwoven with those of the preceding one. If there is one central idea which is common to all aspects of modern algebra it is the notion of homomorphism. By this one means 1 Sec. 2.7 Homomorphisms 55 mapping from one algebraic system to a like algebraic system which ;reserves structure. We make this precise, for groups, in the next definition. DEFINITION A mapping</> from a group G into a group Cis said to be a Jwmomorphism if for all a, b E G, </> ( ab) = </> (a)</> (b) . . Notice that on the left side of this relation, namely, in the term <f>(ab), the product ab is computed in G using the product of elements of G, whereas on the right side of this relation, namely, in the term <f>(a)<f>(b), the product is that of elements in G. Example 2.7.0 <f>(x) = e all x E G. This is trivially a homomorphism. Likewise <f>(x) = x for every x EGis a homomorphism. Example 2.7.1 Let G be the group of all real numbers under addition (i.e., ab for a, b E G is really the real number a + b) and let C be the group of nonzero real numbers with the product being ordinary multiplication of real numbers. Define </> :G ~ C by </J(a) = 2a. In order to verify that this mapping is a homomorphism we must check to see whether <f>(ab) = t/J(a)<f>(b), remembering that by the product on the left side we mean the operation in G (namely, addition), that is, we must check if 2a+b = 2a2b, which indeed is true. Since 2a is always positive, the image of </> is not all of G, so </> is a homomorphism of G into C, but not onto C. Example 2.7.2 Let G = S3 = {e, </>, ljl, l/12 , </Jl/1, </Jl/12 } and G = {e, </> }. Define the mappingj:G ~ C byf(<f>iljli) = <f>i. Thusf(e) = e,J(</>) = t/J, f (l/1) = e, f (l/1 2 ) = e, f ( </>l/1) = </>, f (</>l/1 2 ) = <f>. The reader should verify that f so defined is a homomorphism. ..,. Example 2. 7.3 Let G be the group of integers under addition and let a = G. For the integer X E G define </> by </J(x) = 2x. That </> is a homo- morphism then follows from <f>(x + y) = 2(x + y) = 2x + 2y = <f>(x) + <f>(y). Example 2. 7.4 Let G be the group of nonzero real numbers under multiplication, C = {1, -1}, where 1.1 = 1, ( -1)( -1) = 1, 1( -1) = ( -1) 1 = -1. Define </> :G ~ C by <f>(x) = 1 if x is positive, </J(x) = -1 if X is negative. The fact that </> is a homomorphism is equivalent to the Statements: positive times positive is positive, positive times negative is negative, negative times negative is positive. Example 2.7.5 Let G be the group of integers under addition, let Cn be the group of integers under addition modulo n. Define </> by <f>(x) = remainder of x on division by n. One can easily verify this is a homo- morphism. 56 Group Theory Ch. 2 Example 2.7.6 Let G be the group of positive real numbers under multiplication and let G be the group of all real numbers under addition. Define 4> :G --+ G by lj>(x) = log10x. Thus lj>(xy) = log10 (xy) = log10 (x) + log10 (y) = lj>(x) lj>(y) since the operation, on the right side, in G is in fact addition. Thus 4> is a homomorphism of G into G. In fact, not only is 4> a homomorphism but, in addition, it is one-to-one and onto. #Example 2. 7. 7 Let G be the group of all real 2 x 2 matrices (: ~) such that ad - be =I= 0, under matrix multiplication. Let G be the group of all nonzero real numbers under multiplication. Define 4> :G --+ G by 4>(: !) = ad - be. We leave it to the reader to check that 4> is a homomorphism of G onto G. The result of the following lemma yields, for us, an infinite class of examples of homomorphisms. When we prove Theorem 2.7.1 it will turn out that in some sense this provides us with the most general example of a homomorphism. LEMMA 2.7.1 Suppose G is a group, N a normal subgroup of G; define the mapping 4> from G to Gf N by lj>(x) = Nx for all x E G. Then 4> is a homo- morphism of G onto G / N. Proof. In actuality, there is nothing to prove, for we already have proved this fact several times. But for the sake of emphasis we repeat it. That 4> is onto is trivial, for every element X E Gf N is of the form X = Ny, y E G, so X = lj>(y). To verify the multiplicative property required in order that 4> be a homomorphism, one just notes that if x,y E G, lj>(xy) = Nxy = NxNy = lj>(x)lj>(y). In Lemma 2.7.1 and in the examples preceding it, a fact which comes through is that a homomorphism need not be one-to-one; but there is a certain uniformity in this process of deviating from one-to-oneness. This will become apparent in a few lines. DEFINITION If 4> is a homomorphism of G into G, the kernel of lj>, Kt/J, is defined by Kt/J = {x E G I lj>(x) = e, e = identity element of G}. Before investigating any properties of Kt/J it is advisable to establish that, as a set, Kt/J is not empty. This is furnished us by the first part of I Sec. 2.7 Homomorphisms 57 LEMMA 2.7.2 lJ </> is a homomorphism of G into G, then 1. l/J(e) = e, the unit element of G. 2. Q>(x- 1) = <f>(x) - 1 for all x E G. Proof. To prove (1) we merely calculate <f>(x)e = </J(x) = <f>(xe) q,(x)</J(e), so by the cancellation property in G we have that <f>(e) = e. To establish (2) one notes that e = <f>(e) = </J(xx- 1) = <f>(x)</J(x- 1), so by the very definition of </J(x)- 1 in G we obtain the result that </J(x- 1) </J(x)-1. The argument used m the proof of Lemma 2. 7.2 should remind any reader who has been exposed to a development of logarithms of the argument used in proving the familiar results that log 1 = 0 and log (1/x) = -log x; this is no coincidence, for the mapping </> :x ~ log xis a homomorphism of the group of positive real numbers under multiplication into the group of real numbers under addition, as we have seen in Example 2. 7.6. Lemma 2. 7.2 shows that e is in the kernel of any homomorphism, so any such kernel is not empty. But we can say even more. LEMMA 2.7.3 1J 4> is a homomorphism of G into G with kernel K, then K is a normal subgroup of G. Proof. First we must check whether K is a subgroup of G. To see this one must show that K is closed under multiplication and has inverses in it for every element belonging to K. If x,y E K, then </J(x) = e, <f>(y) = e, where e is the identity element of G, and so </J(xy) = <f>(x)</J(y) = ee = e, whence xy E K. Also, if x E K, t/J(x) = e, so, by Lemma 2. 7.2, <f>(x- 1 ) = </J(x)- 1 = e- 1 = e; thus x- 1 E K. K is, accordingly, a subgroup of G. To prove the normality of K one must establish that for any g E G, k E K, gkg- 1 e K; in other words, one must prove that <f>(gkg- 1) = e whenever </J(k) = e. But </J(gkg- 1) = <f>(g)<f>(k)</J(g- 1 ) = </J(g)e<f>(g)- 1 = t/>(g)</J(g) - 1 = e. This completes the proof of Lemma 2. 7.3. Let </> now be a homomorphism of the group G onto the group G, and suppose that K is the kernel of <f>. If g E G, we say an element x E G is an inverse image of g under <jJ if </J(x) = g. What are all the inverse images of g? For g = e we have the answer, namely (by its very definition) K. What c..bout elements g =1 e? Well, suppose x E G is one inverse image of g; can we write down others? Clearly yes, for if k E K, and if y = kx, then tP(y) = </J(kx) = </J(k)</J(x) = eg = g. Thus all the elements Kx are in the inverse image of g whenever x is. Can there be others? Let us suppose that </J(z) = g = </J(x). Ignoring the middle term we are left with tP(z) = </J(x), and so </J(z)</J(x) - 1 = e. But </J(x) - 1 = </J(x- 1), whence 58 Group Theory Ch. 2 e = cp(z)cf>(x) - 1 = cf>(z)cf>(x- 1 ) = cf>(zx- 1 ), in consequence of which zx- 1 E K; thus z E Kx. In other words, we have shown that Kx accounts for exactly all the inverse images of g whenever x is a single such inverse image. We record this as LEMMA 2.7.4 If cf> is a homomorphism of G onto C with kernel K, then the set of all inverse images of g E C under cf> in G is given by Kx, where x is any particular inverse image of gin G. A special case immediately presents itself, namely, the situation when K = (e). But here, by Lemma 2.7.4, any g E C has exactly one inverse image. That is, cf> is a one-to-one mapping. The converse is trivially true, namely, if cf> is a one-to-one homomorphism of G into (not even onto) G, its kernel must consist exactly of e. DEFINITION A homomorphism cf> from G into Cis said to be an isomor- phism if cf> is one-to-one. DEFINITION Two groups G, G* are said to be isomorphic if there is an isomorphism of G onto G*. In this case we write G ~ G*. We leave to the reader to verify the following three facts: 1. G ~ G. 2. G ~ G* implies G* ~ G. 3. G ~ G*, G* ~ G** implies G ~ G**. When two groups are isomorphic, then, in some sense, they are equal. They differ in that their elements are labeled differently. The isomorphism gives us the key to the labeling, and with it, knowing a given computation in one group, we can carry out the analogous computation in the other. The isomorphism is like a dictionary which enables one to translate a sentence in one language into a sentence, of the same meaning, in another language. (Unfortunately no such perfect dictionary exists, for in languages words do not have single meanings, and nuances do not come through in a literal translation.) But merely to say that a given sentence in one language can be expressed in another is of little consequence; one needs the dictionary to carry out the translation. Similarly it might be of little consequence to know that two groups are isomorphic; the object of interest might very well be the isomorphism itself. So, whenever we prove two groups to be iso- morphic, we shall endeavor to exhibit the precise mapping which yields this isomorphism. Returning to Lemma 2. 7.4 for a moment, we see in it a means of character- izing in terms of the kernel when a homomorphism is actually an isomor- phism. I Sec. 2.7 Homomorphisms 59 cOROLLARY A homomorphism ¢ of G into G with kernel K.p is an isomorphism of G into G if and only if K.p = (e). This corollary provides us with a standard technique for proving two groups to be isomorphic. First we find a homomorphism of one onto the other, and then prove the kernel of this homomorphism consists only of the identity element. This method will be illustrated for us in the proof of the very important 1HEOREM 2.7.1 Let ¢ be a homomorphism of G onto G with kernel K. Then G/K ~G. Proof. Consider the diagram where a(g) = Kg. We should like to complete this to It seems clear that, in order to construct the mapping t/1 from GJK to G, we should use Gas an intermediary, and also that this construction should be relatively uncomplicated. What is more natural than to complete the diagram using 60 Group Theory Ch. 2 With this preamble we formally define the mapping t/J from GJK to C by: if X E GJK, X = Kg, then t/J(X) = <j>(g). A problem immediately arises: is this mapping well defined? If X E GjK, it can be written as Kg in several ways (for instance, Kg = Kkg, k E K); but if X = Kg = Kg', g, g' E G, then on one hand t/J(X) = <J>(g), and on the other, t/J(X) = <j>(g'). For the mapping t/J to make sense it had better be true that </>(g) = <j>(g'). So, suppose Kg = Kg'; then g = kg', where k E K, hence </>(g) = </>(kg') = <J>(k)<j>(g') = elj>(g') = lj>(g') since k E K, the kernel of <J>. We next determine that t/J is onto. For, if x E C, x = <J>(g), g E G (since </>is onto) sox= <j>(g) = t/J(Kg). If X, Y E GJK, X = Kg, Y = Kj, g,j E G, then XY = KgKf = Kgj, so that t/J(XY) = t/J(Kgf) = <J>(gf) = lj>(g)lj>(f) since</> is a homomorphism of G onto C. But t/J(X) = t/J(Kg) = </>(g), t/J(Y) = t/J(Kf) = </>(f), so we see that t/J(XY) = t/J(X)t/J(Y), and t/J is a homomorphism of GfK onto C. To prove that t/J is an isomorphism of GfK onto Call that remains is to demonstrate that the kernel of t/J is the unit element of GfK. Since the unit element of GfK is K = Ke, we must show that if t/J(Kg) = e, then Kg = Ke = K. This is now easy, for e = t/J(Kg) = <J>(g), so that <J>(g) = e, whence g is in the kernel of <J>, namely K. But then Kg = K since K is a subgroup of G. All the pieces have been put together. We have exhibited a one-to-one homomorphism of GfK onto C. Thus GfK ~ C, and Theorem 2. 7.1 is established. Theorem 2. 7.1 is important, for it tells us precisely what groups can be expected to arise as homomorphic images of a given group. These must be expressible in the form GjK, where K is normal in G. But, by Lemma 2. 7.1, for any normal subgroup N of G, GfN is a homomorphic image of G. Thus there is a one-to-one correspondence between homomorphic images of G and normal subgroups of G. If one were to seek all homomorphic images of G one could do it by never leaving G as follows: find all normal subgroups N of G and construct all groups GJ N. The set of groups so constructed yields all homomorphic images of G (up to isomorphisms). A group is said to be simple if it has no nontrivial homomorphic images, that is, if it has no nontrivial normal subgroups. A famous, long-standing conjecture was that a non-abelian simple group of finite order has an even number of elements. This important result has been proved by the two American mathematicians, Walter Feit and John Thompson. We have stated that the concept of a homomorphism is a very important one. To strengthen this statement we shall now show how the methods and results of this section can be used to prove nontrivial facts about groups. When we construct the group GjN, where N is normal in G. if we should happen to know the structure of GJN we would know that of G \"uptoN.\" True, we blot out a certain amount of information about G, but often Sec. 2.7 Homomorphisms 61 enough is left so that from facts about Gj N we can ascertain certain ones about G. When we photograph a certain scene we transfer a three- dimensional object to a two-dimensional representation of it. Yet, looking :at the picture we can derive a great deal of information about the scene Aphotographed. . . . . . {~\" In the two applications of the Ideas developed so far, which are giVen \".elow, the proofs given are not the best possible. In fact, a little later in ~~s chapter these results will be proved in a more general situation in an ?ta_sier manner. We use the presentation here because it does illustrate '~ffectively many group-theoretic concepts. APPLICATION 1 (CAUCHY's THEOREM FOR ABELIAN GROUPS) Suppose G is a finite abelian group and pI o(G), where p is a prime number. Then there is an 1lement a =I= e E G such that aP = e. Proof. We proceed by induction over o(G). In other words, we assume that the theorem is true for all abelian groups having fewer elements than G. From this we wish to prove that the result holds for G. To start the biduction we note that the theorem is vacuously true for groups having a single element. If G has no subgroups H =I= (e), G, by the result of a problem earlier in the chapter, G must be cyclic of prime order. This prime must be p, and G certainly hasp - 1 elements a =f. e satisfying aP = ao(G) = e. So suppose G has a subgroup N =I= (e), G. If pI o(N), by our induction hypothesis, since o(N) < o(G) and N is abelian, there is an element b E N, b =I= e, satisfying bP = e; since b E N c G we would have exhibited an element of the type required. So we may assume that p% o(N). Since G is abelian, N is a normal subgroup of G, so Gj N is a group. More~ver, o(GfN) = o(G)jo(N), and since p% o(N), p I o(G) < o(G). o(N) Also, since G is abelian, Gj N is abelian. Thus by our induction hypothesis there is an element X E GJN satisfying XP = ev the unit element of GJN, X =/: e1 . By the very form of the elements of Gj N, X = Nb, b E G, so that XP = (Nb)P = NbP. Since e1 = Ne, XP = e1 , X =f. e1 translates into NbP = N, Nb =f. N. Thus bP EN, b ¢ N. Using one of the corollaries to Lagrange's theorem, (bP)o(N) = e. That is, bo(N)p = e. Let. c = bo(N). Certai11ly cP = e. In order to show that c is an element that satisfies the COnclusion of the theorem we must finally show that c =f. e. However, if ~ = e, bo(N) = e, and so (Nb)o(N) = N. Combining this with (Nb)P = N, P ,f o(N), P a prime number, we find that Nb = N, and so b E N, a contra- diction. Thus c =f. e, cP = e, and we have completed the induction. This proves the result. 62 Group Theory Ch. 2 APPLICATION 2 (SYLow's THEOREM FOR ABELIAN GRouPs) If G is an abelian group of order o(G), and if pis a prime number, such that prJ. I o(G), pr;.+ 1 ~ o(G), then G has a subgroup of order pr;.. Proof. If a = 0, the subgroup (e) satisfies the conclusion of the result. So suppose a =I= 0. Then pI o(G). By Application 1, there is an element a =1= e E G satisfying aP = e. Let S = {x E G I xP\" = e some integer n }. Since a E S, a =1= e, it follows that S =I= (e). We now assert that Sis a sub- group of G. Since G is finite we must only verify that S is closed. If x,y E s, xP\" = e, yPm = e, so that (xy)Pn+m = xpn+mypn+m = e (we have used that G is abelian), proving that xy E S. We next claim that o(S) = pP with f3 an integer 0 < f3 ~ a. For, if some prime q I o(S), q =1= p, by the result of Application 1 there is an element c E S, c =1= e, satisfying cq = e. However, cP\" = e for some n since c E S. Since pn, q are relatively prime, we can find integers A, f1 such that A.q + 11Pn = 1, so that c = c 1 = cJ.q+p,p\" = (cq);.(cP\") 11 = e, contradicting c =I= e. By Lagrange's theorem o(S) I o(G), so that f3 ~ a. Suppose that f3 < a; consider the abelian group GJS. Since f3 < a and o(GfS) = o(G)fo(S), pI o(G/S), there is an element Sx, (x E G) in GJS satisfying Sx =I= S, (Sx)P\" = S for some integer n > 0. ButS = (Sx)P\" = SxP\", and so xP\" E S; consequently e = (xP\")o(S) = (xP\")pP = xP\"+P. Therefore, x satisfies the exact requirements needed to put it in S; in other words, xES. Con- sequently Sx = S contradicting Sx =I= S. Thus f3 < a is impossible and we are left with the only alternative, namely, that f3 = a. S is the required subgroup of order pr;.. We strengthen the application slightly. Suppose Tis another subgroup of G of order pr;., T =1= S. Since G is abelian ST = TS, so that ST is a sub- group of G. By Theorem 2.5.1 o(ST) = o(S)o( T) o(S n T) o(S n T) and since S =I= T, o(S n T) < pr;., leaving us with o(ST) = p1, y > a. Since ST is a subgroup of G, o(ST) I o(G); thus p1 I o(G) violating the fact that a is the largest power of p which divides o( G). Thus no such subgroup T exists, and S is the unique subgroup of order prJ.. We have proved the COROLLARY If G is abelian of order o(G) and pr;. I o(G), pr;.+ 1 ~ o(G), there is a unique subgroup of G of order pr;.. If we look at G = S3 , which is non-abelian, o(G) = 2.3, we see that G has 3 distinct subgroups of order 2, namely, {e, ¢ }, {e, f/Jt/1}, {e, f/Jt/1 2 }, so that the corollary asserting the uniqueness does not carry over to non- abelian groups. But Sylow's theorem holds for all finite groups. I Sec. 2.7 Homomorphisms 63 We leave the application and return to the general development. Suppcse 4> is a homom_orphism of G onto G with ke_:nel K, and suppose that fi is a subgroup of G. Let H = {x E G I ¢(x) E H}. We assert that His a sub- group of G and that H ::J K. That H ::J K is trivial, for if x e K, ¢(x) = e is in B, so that K c H follows. Suppose now that x,y e H; hence ¢(x) e fi, q,(y) e fi from which we deduce that ¢(xy) = ¢(x)¢(y) e fi. There- fore, xy e H and H is closed under the product in G. Furthermore, if x e H, <f>(x) e fi and so ¢(x- 1 ) = ¢(x)- 1 e fi from which it follows that x-1 e H. All in all, our assertion has been established. What can we say in addition in case fi is normal in G? Let g e G, he H; then ¢(h) e fi, whence c/J(ghg- 1 ) = ¢(g)¢(h)¢(g) - 1 e fi, since fi is normal in G. Other- wise stated, ghg- 1 e H, from which it follows that His normal in G. One other point should be noted, namely, that the homomorphism ¢ from G onto G, when just considered on elements of H, induces a homomorphism of H onto fi, with kernel exactly K, since K c H; by Theorem 2.7.1 we have that fi ~ HJK. Suppose, conversely, that L is a subgroup of G and K c L. Let L = {x e G I x = ¢(l), l e L}. The reader should verify that Lis a subgroup of G. Can we explicitly describe the subgroup T = {y e G I ¢(y) e L}? Clearly L c T. Is there any element t e T which is not in L? So, suppose t e T; thus ¢(t) e L, so by the very definition of L, ¢(t) = ¢(l) for some l e L. Thus c/J(tr 1 ) = c/J(t)c/J(l) - 1 = e, whence tl- 1 e K c L, thus t is in Ll = L. Equivalently we have proved that T c L, which, combined with L c T, yields that L = T. Thus we have set up a one-to-one correspondence between the set of all subgroups of G and the set of all subgroups of G which contain K. More- over, in this correspondence, a normal subgroup of G corresponds w a normal subgroup of G. We summarize these few paragraphs in LEMMA 2.7.5 Let ¢ be a homomorphism of G onto G with kernel K. For fi a subgroup of G let H be defined by H = {x e G I ¢(x) e fi}. Then His a sub- group of G and H ::J K; if fi is normal in G, then H is normal in G. Moreover, this association sets up a one-to-one mapping from the set of all subgroups of G onto the set of all subgroups of G which contain K. We wish to prove one more general theorem about the relation of two groups which are homomorphic. ,'THEOREM 2.7.2. Let ¢ be a homomorphism of G onto G with kernel K, and let ;:,,IJ be a normal subgroup of G, N = {x e G I ¢(x) eN}. Then GIN~ GIN. :!;\\:Equivalently, GIN ~ ( G I K) I ( Nl K). ~~; 64 Group Theory Ch. 2 Proof. As we already know, there is a homomorphism (} of G onto GfN defined by 8(g) = Ng. We define the mapping 1/J:G ~ GfN by 1/J(g) = Nlf>(g) for all g E G. To begin with, 1/J is onto, for if g E G, g = lf>(g) for some g E G, since 4> is onto, so the typical element Ng in GJN can be represented as Nlf>(g) = 1/J(g). If a, b E G, 1/J(ab) = Nlf>(ab) by the definition of the mapping 1/J. How- ever, since 4> is a homomorphism, lf>(ab) = lf>(a)lf>(b). Thus 1/J(ab) = Nlf>(a)lf>(b) = Nlf>(a)Nlf>(b) = 1/J(a)l/J(b). So far we have shown that 1/1 is a homomorphism of G onto Gf N. What is the kernel, T, of 1/J? Firstly, if n E N, lf>(n) E N, so that 1/J(n) = Nlf>(n) = N, the identity element of Gf N, proving that N c T. On the other hand, if t E T, 1/J (t) = identity element ofGjN = N; but 1/J(t) = Nlf>(t). Comparing these two evaluations of 1/J(t), we arrive at N = Nlf>(t), which forces lf>(t) E N; but this places t in N by definition of N. That is, T c N. The kernel of 1/J has been proved to be equal to N. But then 1/J is a homomorphism of G onto GfN with kernel N. By Theorem 2.7.1 GJN ~ GJN, which is the first part of the theorem. The last statement in the theorem is immediate from the observation (following as a consequence of Theorem 2. 7.1) that C ~ GjK, N ~ NjK, GfN ~ (GfK)j(NjK). Problems 1. In the following, verify if the mappings defined are homomorphisms, and in those cases in which they are homomorphisms, determine the kernel. (a) G is the group of nonzero real numbers under multiplication, C = G, lf>(x) = x 2 aH x E G. (b) G, Cas in (a), l/J(x) = 2x. (c) G is the group of real numbers under addition, C = G, lf>(x) x + 1 all x E G. (d) G, Cas in (c), lf>(x) = 13x for x E G. (e) G is any abelian group, C = G, lf>(x) = x 5 all x E G. 2. Let G be any group, g a fixed element in G. Define 4> :G ~ G by lf>(x) = gxg- 1 . Prove that 4> is an isomorphism of G onto G. 3. Let G be a finite abelian group of order o(G) and suppose the integer n is relatively prime to o(G). Prove that every g E G can be written as g = xn with x E G. (Hint: Consider the mapping 4> :G ~ G defined by lf>(y) = yn, and prove this mapping is an isomorphism of G onto G.) 4. (a) Given any group G and a subset U, let 0 be the !lmallest sub- group of G which contains U. Prove there is such a subgroup 0 in G. ( 0 is called the subgroup generated by U.) I I Sec. 2.7 Homomorphisms 65 (b) If gug- 1 E U for all g E G, u E U, prove that 0 is a normal subgroup of G. 5. Let U = {xyx- 1y- 1 I x,y E G}. In this case 0 is usually written as G' and is called the commutator subgroup if G. (a) Prove that G' is normal in G. (b) Prove that GfG' is abelian. (c) If GJN is abelian, prove that N => G'. (d) Prove that if His a subgroup ofG and H => G', then His normal in G. 6. If N, Mare normal subgroups ofG, prove that NMfM ~ NfN n M. 7. Let V be the set of real numbers, and for a, b real, a # 0 let '!0 b: V ~ V defined by 't'ab(x) = ax + b. Let G = {'t'ab I a, b real, a # 0} and let N = {'t'1b E G}. Prove that N is a normal subgroup of G and that Gf N ~ group of nonzero real numbers under multi- plication. 8. Let G be the dihedral group defined as the set of all formal symbols x~i, i = 0, 1, J = 0, 1, ... , n - 1, where x 2 = e, yn = e, .ry = y- 1x. Prove (a) The subgroup N = {e,y,y 2 , ••. ,yn- 1 } is normal in G. (b) That G/N ~ W, where W = {1, -1} is the group under the multiplication of the real numbers. 9. Prove that the center of a group is always a normal subgroup. 10. Prove that a group of order 9 is abelian. 11. If G is a non-abelian group of order 6, prove that G ~ S3 • 12. If G is abelian and if N is any subgroup of G, prove that Gf N is abelian. 13. Let G be the dihedral group defined in Problem 8. Find the center of G. 14. Let G be as in Problem 13. Find G', the commutator subgroup of G. 15. Let G be the group of nonzero complex numbers under multiplication and let N be the set of complex numbers of absolute value 1 (that is, a + bi E N if a 2 + b2 = 1). Show that Gf N is isomorphic to the group of all positive real numbers under multiplication. #16. Let G be the group of all nonzero complex numbers under multi- plication and let G be the group of all real 2 x 2 matrices of the form ( a b), where not both a and b are 0, under matrix multiplication. -b a Show that G and G are isomorphic by exhibiting an isomorphism of G onto G. 66 Group Theory Ch. 2 * 17. Let G be the group of real numbers under addition and let N be the subgroup of G consisting of all the integers. Prove that Gf N is isomorphic to the group of all complex numbers of absolute value 1 under multiplication. #18. LetGbethegroupofallreal2 x 2matrices(: ~}with ad- be# 0, under matrix multiplication, and let Prove that N :::J G', the commutator subgroup of G. *#19. In Problem 18 show, in fact, that N = G'. #20. Let G be the group of all real 2 x 2 matrices of the form ( ~ ~} where ad =/:. 0, under matrix multiplication. Show that G' is precisely the set of all matrices of the form G ;} 21. Let S1 and S2 be two sets. Suppose that there exists a one-to-one mapping 1/J of S1 into S2 . Show that there exists an isomorphism of A(S1 ) into A(S2 ), where A(S) means the set of all one-to-one mappings of S onto itself. 2.8 Automorphisms In the preceding section the concept of an isomorphism of one group into another was defined and examined. The special case in which the isomor- phism maps a given group into itself should obviously be of some importance. We use the word \"into\" advisedly, for groups G do exist which have iso- morphisms mapping G into, and not onto, itself. The easiest such example is the following: Let G be the group of integers under addition and define </J:G-+ G by ¢:x-+ 2x for every x E G. Since ¢:x + y -+ 2(x + y) = 2x + 2JI, 4> is a homomorphism. Also if the image of x andy under 4> are equal, then 2x = 2J' whence x = y. 4> is thus an isomorphism. Yet 4> is not onto, for the image of any integer under 4> is an even integer, so, for instance, 1 does not appear an image under 4> of any element of G. Of greatest interest to us will be the isomorphisms of a group onto itself. DEFINITION By an automorphism of a group Gwe shall mean an isomorphism of G onto itself. As we mentioned in Chapter 1, whenever we talk about mappings of a set into itself we shall write the mappings on tpe right side, thus if T:S -+ S, x E S, then x T is the image of x under T. I Sec. 2.8 Automorphisms 67 Let I be the mapping of G which sends every element onto itself, that is, ·xi = x for all x E G. Trivially I is an automorphism of G. Let d(G) denote •'the set of all automorphisms of G; being a subset of A(G), the set of one- to-one mappings ofG onto itself, for elements of d(G) we can use the product J!cf A( G), namely, composition of mappings. This product then satisfies the ;~~associative law in A( G), and so, afortiori, in d(G). Also I, the unit element 1':,.~£ A(G), is in d(G), so d(G) is not empty. '•r· An obvious fact that we should try to establish is that d(G) is a subgroup (bf A( G), and so, in its own rights, d( G) should be a group. If T 1 , T 2 are 'ID d(G) we already know that T 1 T 2 E A( G). We want it to be in the ~smaller set d(G). We proceed to verify this. For all x,y E G, therefore (xT1 )(yT1 ), (xT2 ) (yT2 ), (xy) T 1 T 2 ((xy) T 1 ) T 2 = ((xT1 ) (yT1)) T 2 = ((xT1)T2)((yT1)T2 ) = (xT1 T 2 )(yT1 T 2 ) • . That is, T 1 T 2 E d(G). There is only one other fact that needs verifying in order that d(G) be a subgroup of A( G), namely, that if T E d(G), then ,;T- 1 E d(G). If x,y E G, then ((xT- 1)(yT- 1 )) T = ((xT- 1 ) T)((yT- 1 ) T) = (xl)(yl) = xy, thus ( x T- 1 ) ( y T- 1 ) = ( xy) T- 1, ·placing r- 1 in d(G). Summarizing these remarks, we have proved LEMMA 2.8.1 lf G is a group, then d(G), the set of automorphisms oj.,.G, is also a group. Of course, as yet, we have no way of knowing that d(G), in general, has elements other than I. If G is a group having only two elements, the reader should convince himself that d(G) consists only of I. For groups G with more than two elements, d(G) always has more than one element. What we should like is a richer sample of automorphisms than the ones \\Ve have (namely, /). If the group G is abelian and there is some element ,'-\"o E G satisfying x0 =I= x0 - 1 , we can write down an explicit automorphism, ·!'the mapping T defined by xT = x- 1 for all x E G. For any group G, Tis ;,'}'t>nto; Ior any abelian G, (xy) T = (xy)- 1 = y- 1x- 1 = x- 1y- 1 = (xT) (yT). t;,;Also x0 T = x0 - 1 =I= x0 , so T =1= I. However, the class of abelian groups is a little limited, and we should e to have some automorphisms of non-abelian groups. Strangely enough e task of finding automorphisms for such groups is easier than for abelian oups. 68 Group Theory Ch. 2 Let G be a group; forgE G define T_q:G ~ G by xT9 = g- 1xg for all x E G. We claim that T 9 is an automorphism of G. First, T 9 is onto, for given y E G, let x = gyg- 1 • Then xT 9 = g- 1 (x)g = g- 1 (gyg- 1 )g = y, so T 9 is onto. Now consider, for x,y E G, (xy) T 9 = g- 1 (xy)g = g- 1 (xgg- 1y)g = (g- 1xg)(g- 1yg) = (xT9)(yT 9 ). Consequently T 9 is a homomorphism of G onto itself. We further assert that T 9 is one-to-one, for if xT9 = yT9 , then g- 1xg = g- 1yg, so by the cancellation laws in G, x = y. T 9 is called the inner automorphism corresponding to g. If G is non-abelian, there is a pair a, bEG such that ab i= ba; but then bTa = a- 1 ba i= b, so that Ta i= I. Thus for a non-abelian group G there always exist nontrivial automorphisms. Let f(G) = {T9 E d(G) I g E G}. The computation of T 9h, for g, hE G, might be of some interest. So, suppose x E G; by definition, xT9h = (gh) - 1x(gh) = h- 1g- 1xgh = (g- 1xg) Th = (xT9 ) Th = xT9 Th. Looking at the start and finish of this chain of equalities we find that T 9h = T 9 Th. This little remark is both interesting and suggestive. It is of interest because it immediately yields that f(G) is a subgroup of d(G). (Verify!) f(G) is usually called the group of inner automorphisms of G. It is suggestive, for if we consider the mapping 1/J:G ~ d(G) defined by 1/J(g) = T 9 for every g E G, then 1/J(gh) = T 9h = T 9 Th = 1/J(g)tf;(h). That is, 1/J is a homomorphism of G into d(G) whose image is f(G). What is the kernel oftf;? Suppose we call it K, and suppose g0 E K. Then tf;(g0 ) = I, or, equivalently, T 90 = I. But this says that for any x E G, xT90 = x; however, x T 90 = g0 - 1 xg0 , and so x = g0 - 1 xg0 for all x E G. Thus g0x = g0 g0 - 1xg0 = xg0 ; g0 must commute with all elements of G. But the center of G, Z, was defined to be precisely all elements in G which commute with every element of G. (See Problem 15, Section 2.5.) Thus K c Z. However, if z E Z, then xTz = z- 1xz = z- 1 (zx) (since zx = xz) = x, whence Tz = I and so z E K. Therefore, Z c K. Having proved both K c Z and Z c K we have that Z = K. Summarizing, 1/J is a homomorphism of G into d(G) with image f(G) and kernel Z. By Theorem 2.7.1 f(G) ~ GfZ. In order to emphasize this general result we record it as LEMMA 2.8.2 f(G) ~ GfZ, where f(G) is the group of inner automorphisms of G, and Z is the center of G. Suppose that </> is an automorphisms of a group G, and suppose that a E G has order n (that is, an = e but for no lower positive power). Then <J>(a)n = </>(an) = </>(e) = e, hence <J>(a)n = e. If <J>(a)m = e for some 0 < m < n, then </>(am) = <J>(a)m = e, which implies, since <J> is one-to-one, that am = e, a contradiction. Thus \" LEMMA 2.8.3 Let G be a group and </> an automorphism if G. If a E G is if order o(a) > 0, then o( <J>(a)) = o(a). 1 Sec. 2.8 Automorphisms 69 Automorphisms of groups can be used as a means of constructing new groups from the original group. Before explaining this abstractly, we con- sider a particular example. Let G be a cyclic group of order 7, that is, G consists of all ai, where we assume a 7 = e. The mapping ¢ :ai ~ a 2 i, as can be checked trivially, is '1m automorphism of G of order 3, that is, ¢ 3 = I. Let x be a symbol which we formally subject to the following COnditions: x 3 = e, X- 1aix = qy(ai) = '~1.i, and consider all formal symbols xiai, where i = 0, 1, 2 and j = 0, 1, 2, ... , 6. We declare that xiai = xkal if and only if i = k mod 3 andj = l mod 7. We multiply these symbols using the rules x 3 = a 7 = e, x- 1ax = a 2 • For instance, (xa) (xa 2 ) = x(ax)a 2 = x(xa 2 )a 2 = x 2a4 • The reader can verify that one obtains, in this way, a non-abelian group of order 21. Generally, if G is a group, Tan automorphism of order r of G which is not an inner automorphism, pick a symbol x and consider all elements xig, i = 0, ± 1, ± 2, ... , g E G subject to xig = xi'g' if and only if i = i' mod r, g = g' and x- 1 ix = g Ti for all i. This way we obtain a larger group {G, T}; G is normal in {G, T} and {G, T}fG ~ group generated by T = cyclic group of order r. We close the section by determining d(G) for all cyclic groups. Example 2.8.1 Let G be a finite cyclic group of order r, G = (a), a' = e. Suppose Tis an automorphism of G. If aT is known, since ai T = (aT) i, aiT is determined, so gT is determined for all g E G = (a). Thus we need consider only possible images of a under T. Since aTE G, and since every element in G is a power of a, aT = at for some integer 0 < t < r. However, since T is an automorphism, aT must have the same order as a (Lemma 2.8.3), and this condition, we claim, forces t to be relatively prime tor. \"\"For ifd It, d I r, then (aT)'Id = at(r/d) = ar(t/d) = (a')tfd = e; thus aT has order a divisor of rfd, which, combined with the fact that aT has order r, leads us to d = 1. Conversely, for any 0 < s < r and relatively prime to r, the mapping S:ai ~ asi is an automorphism of G. Thus d(G) is in one-to-one Correspondence with the group U, of integers less than r and relatively Prime tor under multiplication modulo r. We claim not only is there such a one-to-one correspondence, but there is one which furthermore is an :isomorphism. Let us label the elements of d(G) as Ti where Ti:a ~ ai, ,~0 < i < r and relatively prime to r; Ti Ti :a ~ ai ~ (ai)i = aii, thus ~:tri Ti = Tij· The mapping i ~ T; exhibits the isomorphism of U, onto ~~(G). Here then, d(G) ~ U,. ';,r> Example 2.8.2 G is an infinite cyclic group. That is, G consists of all ai, == 0, ± 1, ± 2, ... , where we assume that ai = e if and only if i = 0. Uppose that Tis an automorphism of G. As in Example 2.8.1, aT= at. 70 Group Theory Ch. 2 The question now becomes, What values of t are possible? Since T is an automorphism of G, it maps G onto itself, so that a = gT for some g E G. Thus a = aiT = (aT)i for some integer i. Since aT= at, we must have that a = ati, so that ati- 1 = e. Hence ti - I = 0; that is, ti = 1. Clearly, since t and i are integers, this must force t = ± 1, and each of these gives rise to an automorphism, t = I yielding the identity automorphism I, t = -1 giving rise to the automorphism T:g -+ g- 1 for every g in the cyclic group G. Thus here, d(G) ~ cyclic group of order 2. Problems 1. Are the following mappings automorphisms of their respective groups? (a) G group of integers under addition, T:x -+ -x. (b) G group of positive reals under multiplication, T:x -+ x 2 • (c) G cyclic group of order 12, T:x -+ x 3 . (d) G is the group S3 , T:x -+ x- 1 . 2. Let G be a group, H a subgroup of G, T an automorphism of G. Let (H) T = {hT I hE H}. Prove (H) Tis a subgroup of G, 3. Let G be a group, Tan automorphism of G, N a normal subgroup of G. Prove that ( N) Tis a normal subgroup of G. 4. For G = S3 prove that G ~ J(G). 5. For any group G prove that J(G) is a normal subgroup of d(G) (the group d(G)j.F(G) is called the group of outer automorphisms of G). 6. Let G be a group of order 4, G = {e, a, b, ab }, a2 = b2 = e, ab = ba. Determine d (G). 7. (a) A subgroup C of G is said to be a characteristic subgroup of G if (C) T c C for all automorphisms T of G. Prove a characteristic subgroup of G must be a normal subgroup of G. (b) Prove that the converse of (a) is false. 8. For any group G, prove that the commutator subgroup G' is a characteristic subgroup of G. (See Problem 5, Section 2. 7). 9. If G is a group, N a normal subgroup of G, M a characteristic sub- group of N, prove that M is a normal subgroup of G. 10. Let G be a finite group, T an automorphism of G with the property that xT = x for x E G if and only if x = e. Prove that every g E G can be represented as g = x- 1 (xT) for some x E G. 11. Let G be a finite group, T an automorphism of G witp the property that xT = x if and only if x = e. Suppose further that T 2 = I. Prove that G must be abelian. 1 Sec. 2.9 Cayley's Theorem 71 f12. Let G be a finite group and suppose the automorphism T sends more than three-quarters of the elements of G onto their inverses. Prove that x T = x- 1 for all x E G and that G is abelian. In Problem 12, can you find an example of a finite group which is non-abelian and which has an automorphism which maps exactly three-quarters of the elements of G onto their inverses? Prove that every finite group having more than two elements has a nontrivial automorphism. Let G be a group of order 2n. Suppose that half of the elements of G are of order 2, and the other half form a subgroup H of order n. Prove that His of odd order and is an abelian subgroup of G. ~,16. Let ¢(n) be the Euler ¢-function. If a > 1 is an integer, prove that n I ¢(an - 1). !~17. Let G be a group and Z the center of G. If Tis any automorphism of G, prove that (Z) T c Z. 1·18. Let G be a group and Tan automorphism of G. If, for a E G, N(a) = ,,,, {x E G I xa = ax}, prove that N(aT) = (N(a)) T. i~l9. Let G be a group and Tan automorphism of G. If N is a normal subgroup of G such that (N) T c N, show how you could use T to define an automorphism of GJN. '20. Use the discussion following Lemma 2.8.3 to construct (a) a non-abelian group of order 55. (b) a non-abelian group of order 203. ;~1. Let G be the group of order 9 generated by elements a, b, where a 3 = b 3 = e. Find all the automorphisms of G. 2.9 Cayley's Theorem ~en groups first arose in mathematics they usually came from some specific ~urce and in some very concrete form. Very often it was in the form of a Jet of transformations of some particular mathematical object. In fact, @lost finite groups appeared as groups of permutations, that is, as subgroups 'ijf Sn. (Sn = A(S) when Sis a finite set with n elements.) The English ;~thematician Cayley first noted that every group could be realized as a group of A(S) for some S. Our concern, in this section, will be with a esentation of Cayley's theorem and some related results. (CAYLEY) Every group is isomorphic to a subgroup of S) for some appropriateS. Proof. Let G be a group. For the set S we will use the elements of G; t is, put S = G. If g E G, define 7:g:S( = G) ~ S( = G) by X7:g = xg 72 Group Theory Ch. 2 for every x E G. Ify E G, theny = (yg- 1 )g = (yg- 1 )-rg, so that 't\"g maps S onto itself. Ivforeover, 't\"g is one-to-one, for if x, y E S and x-rg = yr 9 , then xg = yg, which, by the cancellation property of groups, implies that x = y. We have proved that for every g E G, 't\"g E A(S). If g, h E G, consider 't\"gh· For any x E S = G, x-rgh = x(gh) = (xg)h :::: (x-rg)-rh = x-rg-rh. Note that we used the associative law in a very essential way here. From x-rgh = X't\"g't\"h we deduce that 't\"gh = 't\"g't\"h. Therefore, if t/J:G--+ A(S) is defined by t/J(g) = -rg, the relation 't\"gh = 't\"g't\"h tells us that tjJ is a homomorphism. What is the kernel K oft/J? If g0 E K, then t/J(g0 ) = -r 90 is the identity map on S, so that for x E G, and, in particular, for e E G, e-rgo = e. But ngo = eg0 = g0 • Thus comparing these two expressions for e-rgo we conclude that g0 = e, whence K = (e). Thus by the corollary to Lemma 2. 7.4 t/J is an isomorphism of G into A(S), proving the theorem. The theorem enables us to exhibit any abstract group as a more concrete object, namely, as a group of mappings. However, it has its shortcomings; for if G is a finite group of order o(G), 'then, using S = G, as in our proof, A(S) has o(G)! elements. Our group G of order o(G) is somewhat lost in the group A(S) which, with its o(G)! elements, is huge in comparison to G. We ask: Can we find a more economical S, one for which A(S) is smaller? This we now attempt to accomplish. Let G be a group, H a subgroup of G. Let S be the set whose elements are the right cosets of H in G. That is, S = {Hg I g E G}. Sneed not be a group itself, in fact, it would be a group only if H were a normal subgroup of G. However, we can make our group G act on Sin the following natural way: forgE G let tg:S--+ S be defined by (Hx)tg = Hxg. Emulating the proof of Theorem 2.9.1 we can easily prove 1. t 9 E A(S) for every g E G. 2. tgh = tgth. Thus the mapping fJ:G --+ A(S) defined by fJ(g) = t9 is a homomorphism of G into A(S). Can one always say that() is an isomorphism? Suppose that K is the kernel of e. If go E K, then fJ(go) = tgo is the identity map on s, so that for every X E S, Xtgo = X. Since every element of S is a right coset of H in G, we must have that Hatgo = Ha for every a E G, and using the de- finition of t90 , namely, Hatgo = Hag0 , we arrive at the identity Hag0 = Ha for every a E G. On the other hand, if b E G is such that. Hxb = Hx for every x E G, retracing our argument we could show that bE K. Thus K = {b E G I Hxb = Hx all x E G}. We claim that from this character- ization of K, K must be the largest normal subgroup of G which is contained in H. We first explain the use of the word largest; by this we mean that if N is a normal subgroup of G which is contained in H, then N must be con- tained inK. We wish to show this is the case. That K is a normal subgroup I Sec. 2.9 Cayley's Theorem 73 of G follows from the fact that it is the kernel of a homomorphism of G. Now we assert that K c H, for if b E K, Hab = Ha for every a E G, so, in particular, Hb = Heb = He = H, whence b E H. Finally, if N is a normal subgroup of G which is contained in H, if n EN, a E G, then .- 1 E N c H, so that Hana- 1 = H; thus Han = Ha for all a E G. therefore, n E K by our characterization of K. ' We have proved THEOREM 2.9.2 if G is a group, H a subgroup of G, and S is the set of all right cosets of H in G, then there is a homomorphism() of G into A(S) and the kernel t( (} is the largest normal subgroup of G which is contained in H. The case H = (e) just yields Cayley's theorem (Theorem 2.9.1). If H sbould happen to have no normal subgroup of G other than (e) in it, then (}must be an isomorphism of G into A(S). In this case we would have cut down the size of the S used in proving Theorem 2.9.1. This is interesting mostly for finite groups. For we shall use this observation both as a means of proving certain finite groups have nontrivial normal subgroups, and also as a means of representing certain finite groups as permutation groups on small sets. We examine these remarks a little more closely. Suppose that G has a StJbgroup H whose index i(H) (that is, the number of right cosets of H in G) satisfies i(H)! < o(G). Let S be the set of all right cosets of H in G. The · tp,apping, (), of Theorem 2.9.2 cannot be an isomorphism, for if it were, tJ(G) would have o(G) elements and yet would be a subgroup of A(S) which has i(H)! < o(G) elements. Therefore the kernel of() must be larger than (~); this kernel being the largest normal subgroup of G which is contained in H, we can conclude that H contains a nontrivial normal subgroup of G. However, the argument used above has implications even when i(H)! is not less than o(G). If o(G) does not divide i(H)! then by invoking Lagrange's fueorem we know that A(S) can have no subgroup of order o(G), hence no subgroup isomorphic to G. However, A(S) does contain (J(G), whence 8(G) cannot be isomorphic to G; that is, () cannot be an isomorphism. But then, ·~ above, H must contain a nontrivial normal subgroup of G. We summarize this as ·~MMA 2.9.1 If G is a finite group, and H ¥= G is a subgroup of G such that l~(G) ;f' i(H)! then H must contain a nontrivial normal subgroup of G. In particular, t! cannot be simple. ~~i' ~PPLICATIONS ~~: ~t I. Let G be a group of order 36. Suppose that G has a subgroup H of tder 9 (we shall see later that this is always the case). Then i(H) = 4, 74 Group Theory Ch. 2 4! = 24 < 36 = o(G) so that in H there must be a normal subgroup N i= (e), of G, of order a divisor of 9, that is, of order 3 or 9. 2. Let G be a group of order 99 and suppose that H is a subgroup of G of order 11 (we shall also see, later, that this must be true). Then i(H) = 9, and since 99 -f 9! there is a nontrivial normal subgroup N i= (e) of Gin H. Since His of order 11, which is a prime, its only subgroup other than (e) is itself, implying that N = H. That is, H itself is a normal subgroup of G. 3. Let G be a non-abelian group of order 6. By Problem 11, Section 2.3, there is an a i= e E G satisfying a2 = e. Thus the subgroup H = {e, a} is of order 2, and i(H) = 3. Suppose, for the moment, that we know that H is not normal in G. Since H has only itself and (e) as subgroups, H has no nontrivial normal subgroups of G in it. Thus G is isomorphic to a subgroup T of order 6 in A(S), where S is the set of right cosets of H in G. Since o(A(S)) = i(H)! = 3! = 6, T = S. In other words, G ~ A(S) = S3 . We would have proved that any non-abelian group of order 6 is isomorphic to S3 • All that remains is to show that His not normal in G. Since it might be of some interest we go through a detailed proof of this. If H = {e, a} were normal in G, then for every g E G, since gag- 1 E H and gag- 1 i= e, we would have that gag- 1 = a, or, equivalently, that ga = ag for every g E G. Let bEG, b ¢: H, and consider N(b) = {x E G I xb = bx}. By an earlier problem, N(b) is a subgroup of G, and N(b) ::::> H; N(b) i= H since bE N(b), b ¢:H. Since His a subgroup of N(b), o(H) I o(N(b)) 16. The only even number n, 2 < n ~ 6 which divides 6 is 6. So o(N(b)) = 6; whence b commutes with all elements of G. Thus every element of G com- mutes with every other element of G, making G into an abelian group, contrary to assumption. Thus H could not have been normal in G. This proof is somewhat long-winded, but it illustrates some of the ideas already developed. Problems 1. Let G be a group; consider the mappings of G into itself, A 9 , defined forgE G by xA9 = gx for all x E G. Prove that A 9 is one-to-one and onto, and that A 9h = AhA 9 • 2. Let A 9 be defined as in Problem 1,1: 9 as in the proofofTheorem 2.9.1. Prove that for any g, h E G, the mappings A 9 , 7:h satisfy A 97:h = 7:hAg· (Hint: For x E G consider x(A97:h) and x(7:hA 9 ).) 3. If 8 is a one-to-one mapping of G onto itself such that A- 98 = 8Ag for all g E G, prove that () = 7:h for some h E G. 4. (a) If H is a subgroup of G show that for every g E G, gHg- 1 is a subgroup of G. 1 Sec. 2.10 Permutation Groups 75 (b) Prove that W = intersection of all gHg- 1 is a normal subgroup of G. 5. Using Lemma 2.9.1 prove that a group of order p2 , where pis a prime number, must have a normal subgroup of order p. 6. Show that in a group G of order p 2 any normal subgroup of order p must lie in the center of G. 1. Using the result of Problem 6, prove that any group of order p2 is abelian. 8. If p is a prime number, prove that any group G of order 2p must have a subgroup of order p, and that this subgroup is normal in G. 9. If o(G) is pq where p and q are distinct prime numbers and if G has a normal subgroup of order p and a normal subgroup of order q, prove that G is cyclic. *10. Let o(G) be pq, p > q are primes, prove (a) G has a subgroup of order p and a subgroup of order q. (b) If q t!\" p - 1, then G is cyclic. (c) Given two primes p, q, q I p - 1, there exists a non-abelian group of order pq. (d) Any two non-abelian groups of order pq are isomorphic. 2.10 Permutation Groups We have seen that every group can be represented isomorphically as a sub- group of A(S) for some set S, and, in particular, a finite group G call,. be represented as a subgroup of sn, for some n, where sn is the symmetric group of degree n. This clearly shows that the groups Sn themselves merit closer examination. Suppose that S is a finite set having n elements x1 , x2 , •.• , xn. If t/J E A(S) = Sm then </J is a one-to-one mapping of S onto itself, and we COuld write </J out by showing what it does to every element, e.g., </> :x1 ~ x2 , .r2 -. x4 , x4 ~ x3 , x3 ~ x1. But this is very cumbersome. One short cut IIlight be to write </J out as ,,;;\"'Y.ut:Jre xik is the image of xi under </J. Returning to our example just above, might be represented by 76 Group Theory Ch. 2 While this notation is a little handier there still is waste in it, for there seems to be no purpose served by the symbol x. We could equally well represent the permutation as 2 Our specific example would read (21 2 3 4) 4 1 3 . Given two permutations 0, ljJ in Sn, using this symbolic representation of e and l/J, what would the representation of Oljl be? To compute it we could start and see what Oljl does to x1 (henceforth written as 1). 0 takes 1 into i 1 , while ljJ takes i 1 into k, say, then Oljl takes 1 into k. Then repeat this procedure for 2, 3, ... , n. For instance, if 0 is the permutation represented by (! and ljJ by G 2 2 3 3 4) 2 4 3 4) 2 4 ' then i 1 = 3 and ljJ takes 3 into 2, so k = 2 and Ol/J takes 1 into 2. Similarly Oljl :2 ---+- 1, 3 ---+- 3, 4 ---+- 4. That is, the representation for Oljl is (~ 2 lfwe write e (! 2 ~ !) and l/J=G 2 3 !) ' 3 2 then Oljl = (! 2 3 !) G 2 3 !) = (~ 2 . 3 !) . 2 3 2 3 This is the way we shall multiply the symbols of the form (:1 2 ~)' (~1 2 ~J· i2 zn k2 ' I Sec. 2.10 Permutation Groups 77 Let S be a set and e E A(S). Given two elements a, bE S we define a ;::: 6b if and only if b = aei for some integer i (i can be positive, negative, or 0). We claim this defines an equivalence relation on S. For . eo 1. a := 6a since a = a = ae. J. If a = 6b, then b = aei, so that a = be- i, whence b = 6a. s,. If a = ob, b = (Jc, then b = aei, c = bej = (aei)ej = aei+i, which implies that a = 6c. This equivalence relation by Theorem 1.1.1 induces a decomposition of S Jnto disjoint subsets, namely, the equivalence classes. We call the equivalence dass of an element s E S the orbit of s under e; thus the orbit of s under e consists of all the elements sei, i = 0, ± 1, ± 2, .... In particular, if S is a finite set and s E S, there is a smallest positive integer l = l(s) depending on s such that se1 = s. The orbit of s under e then consists of the elements s, se, se 2 , ••• , sez- 1 . By a cycle of e we mean the ordered set ( s, se, se 2 , ... , sez- 1 ). If we know all the cycles of e we clearly know e since we would know the image of any element under e. Before proceeding we illustrate these ideas with an example. Let 2 3 4 5 6) 3 5 6 4 ' where s consists of the elements 1, 2, ... ' 6 (remember 1 stands for x1, 2 for x2 , etc.). Starting with 1, then the orbit of 1 consists of 1 = le 0 , 18 1 = 2, Ie 2 = 2e = 1, so the orbit of 1 is the set of elements 1 and 2. This tells us the orbit of 2 is the same set. The orbit of 3 consists just of 3; that of 4 consists of the elements 4, 4e = 5, 4e 2 = 5e = 6, 4e 3 = 6e = 4. The cycles of e are (1, 2), (3), (4, 5, 6). We digress for a moment, leaving our particular e. Suppose that by the cycle (i1 , i2 , • •. , ir) we mean the permutation t/1 which sends i 1 into i2 , i2 into i3 • • • ir_ 1 into ir and ir into i1 , and leaves all other elements of S fixed. Thus, for instance, if S consists of the elements 1, 2, ... , 9, then the symbol (1, 3, 4, 2, 6) means the permutation (~ 2 3 4 5 6 7 8 ~). 6 4 2 5 1 7 8 We multiply cycles by multiplying the permutations they represent. Thus ~ain, if S has 9 elements, {1 2 3)(5 6 4 8) (~ 2 3 4 5 6 7 8 ~) (~ 2 3 4 5 6 7 8 ~) 3 .4 5 6 7 8 2 3 1 6 4 7 5 (~ 2 3 4 5 6 7 8 ~). 3 8 6 4 7 5 78 Group Theory Ch. 2 Let us return to the ideas of the paragraph preceding the last one, and ask: Given the permutation 2 3 4 5 6 3 8 6 4 7 8 9) 7 5 9 ' what are the cycles of e? We first find the orbit of 1; namely, 1, 1e = 2, 1e 2 = 2e = 3, 1e 3 = 3e = 8, 1e 4 = 8e = 5, 1e 5 = 5e = 6, 1e6 = 6e = 4, 1e 7 = 4e = 1. That is, the orbit of 1 is the set {1, 2, 3, 8, 5, 6, 4}. The orbits of 7 and 9 can be found to be {7}, {9}, respectively. The cycles of e thus are (7), (9), (1, 1e, 1e2 , ••• , 1e6 ) = (1, 2, 3, 8, 5, 6, 4). The reader should now verify that if he takes the product (as defined in the last para- graph) of (1, 2, 3, 8, 5, 6, 4), (7), (9) he will obtain e. That is, at least in this case, e is the product of its cycles. But this is no accident for it is now trivial to prove LEMMA 2.1 0.1 Every permutation is the product of its cycles. Proof. Let e be the permutation. Then its cycles are of the form (s, se, ... , se1- 1). By the multiplication of cycles, as d.efined above, and since the cycles of e are disjoint, the image of s' E Sunder e, which is s'e, is the same as the image of s' under the product, ljJ, of all the distinct cycles of e. So e, t/1 have the same effect on every element of S, hence e = t/1, which is what we sought to prove. If the remarks above are still not transparent at this point, the reader should take a given permutation, find its cycles, take their product, and verify the lemma. In doing so the lemma itselfwill become obvious. Lemma 2.10.1 is usually stated in the form every permutation can be uniquely expressed as a product of disJoint cycles. Consider the m-cycle (1, 2, ... , m). A simple computation shows that (1, 2, ... , m) = (1, 2) (1, 3) · · · (1, m). More generally the m-cycle (a1 , a2 , .•• , am) = (a 1 , a2)(a1 , a 3 ) · • · (a 1 , am)· This decomposition is not unique; by this we mean that an m-cycle can be written as a product of 2-cycles in more than one way. For instance, (1, 2, 3) = (1, 2)(1, 3) = (3, 1) (3, 2). Now, since every permutation is a product of disjoint cycles and every cycle is a product of 2-cycles, we have proved LEMMA 2.1 0.2 Every permutation isYJ product of 2-cycles. We shall refer to 2-cycles as transpositions. DEFINITION A permutation (} e Sn is said to be an even permutation if it can be represented as a product of an even number of transpositions. Sec. 2.10 Permutation Groups 79 The definition given just insists that(} have one representation as a product of an even number of transpositions. Perhaps it has other representations as a product of an odd number of transpositions. We first want to show that this cannot happen. Frankly, we are not happy with the proof we give of this fact for it introduces a polynomial which seems extraneous to the matter at hand. Consider the polynomial in n-variables p(xl, ... ' xn) = rr (xi - xj)• i<j If(} E Sn let (} act on the polynomial p(x1 , ••• , xn) by (} :p(xl, . .. ' xn) = rr (xi - xj) -+ rr (xo(i) - Xo(j)). i<j i<j It is clear that (}:p(x1 , ••• , xn) -+ ±p(x1 , ••• , xn)· For instance, m S5 , () = (134)(25) takes (x1 - x2 ) (x1 - x3) (x1 - x4 ) (x1 - x 5 ) (x2 - x3 ) X (x2 - x4 ) (x2 - x5 ) (x3 - x4 ) (x3 - x5 ) (x4 - x5 ) into (x3 - x5 ) (x3 - x4 ) (x3 - x1 ) (x3 - x2 ) (x 5 - x4 ) (x 5 - x1 ) x (x 5 - x2 ) (x 4 - x1) (x4 - x2 ) (x1 - x2 ), which can easily be verified to be -p(x 1 , ••• , x 5 ). If, in particular, (} is a transposition, ():p(xl, ... ' xn) -+ -p(xl, ... ' xn)· (Verify!) Thus if a permutation II can be represented as a product of an even number of transpositions in one representation, II must leave p(x1 , ••• , xn) fixed, so that any n~presentation of II as a product of tra ... ns- position must be such that it leaves p(x1 , • •• , xn) fixed; that is, in any representation it is a product of an even number of transpositions. This establishes that the definition given for an even permutation is a significant one. We call a permutation odd if it is not an even permutation. The following facts are now clear: 1. The product of two even permutations is an even permutation. 2. The product of an even permutation and an odd one is odd (likewise for the product of an odd and even permutation). 3. The product of two odd permutations is an even permutation. The rule for combining even and odd permutations is like that of com- bining even and odd numbers under addition. This is not a coincidence since this latter rule is used in establishing 1, 2, and 3. Let An be the subset of Sn consisting of all even permutations. Since the product of two even permutations is even, An must be a subgroup of Sn. We claim it is normal in Sn. Perhaps the best way of seeing this is as follows: 80 Group Theory Ch. 2 let W be the group of real numbers 1 and - 1 under multiplication. Define t/J :Sn ~ W by t/J(s) = 1 if s is an even permutation, t/J(s) = -1 if s is an odd permutation. By the rules 1, 2, 3 above t/1 is a homomorphism onto W. The kernel of t/1 is precisely An; being the kernel of a homomorphism An is a normal subgroup of Sn. By Theorem 2. 7.1 SnfAn ~ W, so, since 2 = o(W) = o(Sn) = o(Sn)' An o(An) we see that o(An) = -!-n!. An is called the alternating group of degree n. We summarize our remarks in LEMMA 2.1 0.3 Sn has as a normal subgroup of index 2 the alternating group, Am consisting of all even permutations. At the end of the next section we shall return to Sn again. Problems 1. Find the orbits and cycles of the following permutations: (a) (~ 2 3 4 5 6 7 8 ~). 3 4 5 6 7 9 (b) (! 2 3 4 5 ~). 5 4 3 1 2. Write the permutations in Problem 1 as the product of disjoint cycles. 3. Express as the product of disjoint cycles: (a) (1, 2, 3)(4, 5)(1, 6, 7, 8, 9)(1, 5). (b) (1, 2)(1, 2, 3)(1, 2). 4. Prove that (1, 2, ... , n)- 1 = (n, n- 1, n- 2, ... , 2, 1). 5. Find the cycle structure of all the powers of ( 1, 2, ... , 8). 6. (a) What is the order of ann-cycle? (b) What is the order of the product of the disjoint cycles of lengths m1, mz, ... ' mk? (c) How do you find the order of a given permuta!ion? 7. Compute a- 1ba, where (1) a= (1, 3, 5)(1, 2), b = (1, 5, 7, 9). (2) a = (5, 7, 9), b = (1, 2, 3). 8. (a) Given the permutation x = (1, 2)(3, 4), y = (5, 6)(1, 3), find a permutation a such that a- 1xa = y. (b) Prove that there is no a such that a- 1 (1, 2, 3)a = (1, 3) (5, 7, 8)· (c) Prove that there is no permutation a such that a- 1 (1, 2)a ,;::::; (3, 4)(1, 5). 9. Determine for what m an m-cycle is an_ even permutation. 1 Sec. 2.10 Permutation Groups 81 :\",~<10. Determine which of the following are even permutations: ~, (a) (1, 2, 3)(1, 2). (b) (1, 2, 3, 4, 5)(1, 2, 3)(4, 5). (c) (1, 2)(1, 3)(1, 4)(2, 5). Prove that the smallest subgroup of Sn contammg (1, 2) and (1, 2, ... , n) isS\"\" (In other words, these generate Sn-) Prove that for n ~ 3 the subgroup generated by the 3-cycles is An. Prove that if a normal subgroup of An contains even a single 3-cycle it must be all of An- Prove that As has no normal subgroups N =/= (e), As. Assuming the result of Problem 14, prove that any subgroup of As has order at most 12. Find all the normal subgroups in S4 • If n ~ 5 prove that An is the only nontrivial normal subgroup in Sn. Cayley's theorem (Theorem 2.9.1) asserts that every group is isomorphic a subgroup of A(S) for some S. In particular, it says that every finite up can be realized as a group ofpermutations. Let us call the realization the group as a group of permutations as given in the proof of Theorem 9.1 the permutation representation of G. ,f.I8. Find the permutation representation of a cyclic group of order n. $'\"19. Let G be the group {e, a, b, ab} of order 4, where a2 = b2 = e, '¥~' ab = ba. Find the permutation representation of G. 'i 20. Let G be the group S3 • Find the permutation representation of S3 • (Note: This gives an isomorphism of S3 into S6 .) Let G be the group {e, (), a, b, c, ()a, ()b, ()c }, where a 2 = b2 = c2 = (), () 2 = e, ab = ()ba = c, be = 8cb = a, ca = 8ac = b. (a) Show that ()is in the center Z of G, and that Z = {e, 8}. (b) Find the commutator subgroup of G. (c) Show that every subgroup of G is normal. (d) Find the permutation representation of G. (Note: G is often called the group of quaternion units; it, and algebraic systems constructed from it, will reappear in the book.) . Let G be the dihedral group of order 2n (see Problem 17, Section 2.6). Find the permutation representation of G. Let us call the realization of a group Gas a set of permutations given in 1, Section 2.9 the second permutation representation of G. Show that if G is an abelian group, then the permutation representation of G coincides with the second permutation representation of G (i.e., in the notation of the previous section, i!. 9 = -c 9 for all g E G.) 82 Group Theory Ch. 2 24. Find the second permutation representation of S3 • Verify directly from the permutations obtained here and in Problem 20 that A0 1:b = '!bAa for all a, b E s3. 25. Find the second permutation representation of the group G defined in Problem 21. 26. Find the second permutation representation of the dihedral group of order 2n. If His a subgroup of G, let us call the mapping {t9 I g E G} defined in the discussion preceding Theorem 2.9.2 the coset representation of G by H. This also realizes G as a group of permutations, but not necessarily iso- morphically, merely homomorphically (see Theorem 2.9.2). 27. Let G = (a) be a cyclic group of order 8 and let H = (a 4 ) be its subgroup of order 2. Find the coset representation of G by H. 28. Let G be the dihedral group of order 2n generated by elements a, b such that a2 = bn = e, ab = b- 1a. Let H = {e, a}. Find the coset representation of G by H. 29. Let G be the group of Problem 21 and let H = {e, 0}. Find the coset representation of G by H. 30. Let G be S\"' the symmetric group of order n, acting as permutations on the set { 1, 2, ... , n }. Let H = { cr E G I ncr = n }. (a) Prove that His isomorphic to Sn-t· (b) Find a set of elements a1, .•. , an E G such that Ha 1, ••• , Han give all the right cosets of H in G. (c) Find the coset representation of G by H. 2.11 Another Counting Principle Mathematics is rich in technique and arguments. In this great variety one of the most basic tools is counting. Yet, strangely enough, it is one of the most difficult. Of course, by counting we do not mean the creation of tables of logarithms or addition tables; rather, we mean the process of precisely accounting for all possibilities in highly complex situations. This can some- times be done by a brute force case-by-case exhaustion, but such a routine is invariably dull and violates a mathematician's sense of aesthetics. One prefers the light, deft, delicate touch to the hammer blow. But the most serious objection to case-by-case division is that it works far too rarely. Thus in various phases of mathematics we find neat counting devices which tell us exactly how many elements, in some fairly broad context, satisfy certain conditions. A great favorite with mathematicians is the process of counting up a given situation in two different ways; the comparison of the Sec. 2.11 Another Counting Principle two counts is then used as a means of drawing conclusions. Generally speaking, one introduces an equivalence relation on a finite set, measures the size of the equivalence classes under this relation, and then equates the number of elements in the set to the sum of the orders of these equivalence classes. This kind of an approach will be illustrated in this section. We shall introduce a relation, prove it is an equivalence relation, and then find a neat algebraic description for the size of each equivalence class. From this simple description there will flow a stream of beautiful and powerful results about finite groups. DEFINITION If a, bEG, then b is said to be a conjugate of a in G if there exists an element c E G such that b = c- 1ac. We shall write, for this, a ,...., b and shall refer to this relation as conjugacy. LEMMA 2.11 .1 Conjugacy is an equivalence relation on G. Proof. As usual, in order to establish this, we must prove that 1. a ,...., a; 2. a ,...., b implies that b ,...., a; 3. a ,...., b, b ,..., c implies that a ,...., c for all a, b, c in G. We prove each of these in turn. ' I. Since a = e- 1 ae, a ,...., a, with c = e serving as the c in the definition of conjugacy. 2. If a ,...., b, then b = x- 1ax for some x E G, hence, a = (x- 1) -lb(x- 1), and sincey = x- 1 E G and a = y- 1by, b ,...., a follows. 3. Suppose that a ,...., b and b ,...., c where a, b, c E G. Then b = x- 1ax, c = y- 1by for some x,y E G. Substituting for b in the expression for c we obtain c = y- 1 (x- 1ax) y = (xy)- 1a(~); since ~ E G, a ,...., c is a consequence. For a E G let C(a) = {x E G I a ,...., x}. C(a), the equivalence class of a in G under our relation, is usually called the conjugate class of a in G; it consists of the set of all distinct elements of the form y- 1ay as y ranges over G. Our attention now narrows to the case in which G is a finite group. Suppose that C(a) has c 0 elements. We seek an alternative description of C0 • Before doing so, note that o(G) = L C 0 where the sum runs over a set of a E G using one a from each conjugate class. This remark is, of course, merely a restatement of the fact that our equivalence relation-conjugacy- 83 84 Group Theory Ch. 2 induces a decomposition of G into disjoint equivalence classes-the conjugate classes. Of paramount interest now is an evaluation of ca. In order to carry this out we recall a concept introduced in Problem 13 Section 2.5. Since this concept is important-far too important to leave t~ the off-chance that the student solved the particular problem-we go over what may very well be familiar ground to many of the readers. DEFINITION If a E G, then N(a), the normalizer of a in G, Is the set N (a) = { x E G I xa = ax}. N(a) consists of precisely those elements in G which commute with a. LEMMA 2.11.2 N(a) is a subgroup of G. Proof. In this result the order of G, whether it be finite or infinite, is of no relevance, and so we put no restrictions on the order of G. Suppose that x,y E N(a). Thus xa = ax and ya = ay. Therefore, (xy)a = x(ya) = x(ay) = (xa) y = (ax) y = a(~), in consequence of which xy E N(a). From ax= xa it follows that x- 1a = x- 1 (ax)x- 1 = x- 1 (xa)x- 1 = ax-1, so that x- 1 is also in N(a). But then N(a) has been demonstrated to be a subgroup of G. We are now in a position to enunciate our counting principle. THEOREM 2.11.1 If G is a .finite group, then ca = o(G)fo(N(a)); in other words, the number of elements conjugate to a in G is the index of the normalizer of a in G. Proof. To begin with, the conjugate class of a in G, C(a), consists exactly of all the elements x- 1ax as x ranges over G. ca measures the number of distinct x- 1ax's. Our method of proof will be to show that two elements in the same right coset of N(a) in G yield the same conjugate of a whereas two elements in different right cosets of N(a) in G give rise to different conjugates of a. In this way we shall have a one-to-one correspondence between conjugates of a and right cosets of N(a). Suppose that x,y E G are in the same right coset of N(a) in G. Thus y = nx, where n E N(a), and so na = an. Therefore, sincey- 1 = (nx) -l ::::: x- 1n- 1 , y- 1ay = x- 1n- 1anx = x- 1n- 1nax = x- 1ax, whence x and J result in the same conjugate of a. If, on the other hand, x andy are in different right cosets of N (a) in G we claim that x- 1ax ::/= y- 1ay. Were this not the case, from x- 1ax = y- 1a) we would deduce that yx- 1a = ayx- 1 ; this in turn would imply that yx- 1 E N(a). However, this declares x andy to be in the sam<; right coset of N(a) in G, contradicting the fact that they are in different cosets. The proof is now complete. 1 I(>ROLLARY (,,''·'\" Sec. 2.11 Another Counting Principle 85 o(G) = L o(G) o(N(a)) e this sum runs over one element a in each conjugate class. o(G) = :Lea, using the theorem the corollary becomes i11 '~; The equation in this corollary is usually referred to as the class equation of G. 'L, Before going on to the applications of these results let us examine these ~.ncepts in some specific group. There is no point in looking at abelian Jitoups because there two elements are conjugate if and only if they are ~ual (that is, ca = 1 for every a). So we turn to our familiar friend, the ,jroup S3 • Its elements are e, (1, 2), (1, 3), (2, 3), (1, 2, 3), (1, 3, 2). We ~umerate the conjugate classes: C(e) C(1, 2) C(1, 2, 3) {e} {(1, 2), (1, 3)- 1 (1, 2)(1, 3), (2, 3)- 1 (1, 2)(2, 3), ( 1' 2, 3)- 1 (1' 2) ( 1' 2, 3)' ( 1' 3, 2)- 1 (1' 2) ( 1' 3, 2)} {(1, 2), (1, 3), (2, 3)} (Verify!) {(1, 2, 3), (1, 3, 2)} (after another verification). '',fThe student should verify that N((1, 2)) = {e, (1, 2)} and N((1, 2, 3)) :>{e, (1, 2, 3), (1, 3, 2) }, so that c(l,Z) = ! = 3, c( 1 ,2 , 3 ) = t = 2. ,:;f,:. ~ {I;.: ~;.Applications of Theorem 2.11.1 .,;,. f Theorem 2.11.1 lends itself to immediate and powerful application. We }ijl'leed no artificial constructs to illustrate its use, for the results below which 'it'' '~~reveal the strength of the theorem are themselves theorems of stature and portance. Let us recall that the center Z(G) of a group G is the set of all a E G ch that ax = xa for all x E G. Note the UBLEMMA a E Z if and only if N(a) = G. lj G is finite, a E Z if and lyifo(N(a)) = o(G). If a E Z, xa = ax for all x E G, whence N(a) = G. If, conversely, (a) = G, xa =ax for all x E G, so that a E Z. If G is finite, o(N(a)) = ' 1 (G) is equivalent to N(a) = G. 86 Group Theory Ch. 2 APPLICATION 1 THEOREM 2.11.2 If o(G) = pn where pis a prime number, then Z(G) =I= (e). Proof. If a E G, since N(a) is a subgroup of G, o(N(a)), being a divisor of o(G) = pn, must be of the form o(N(a)) = pna; a E Z (G) if and only if na = n. Write out the class equation for this G, letting z = o(Z(G)). We get pn = o(G) = \"'L(Pnfpna); however, since there are exactly z elements such that na = n, we find that Now look at this! p is a divisor of the left-hand side; since na < n for each term in the L of the right side, so that p is a divisor of each term of this sum, hence a divisor of this sum. Therefore, Since e E Z (G), z =1- 0; thus z is a positive integer divisible by the prime p. Therefore, z > 1! But then there must be an element, besides e, in Z(G)! This is the contention of the theorem. Rephrasing, the theorem states that a group of prime-power order must always have a nontrivial center. We can now simply prove, as a corollary for this, a result given in an earlier problem. COROLLARY If o(G) = p 2 where pis a prime number, then G is abelian. Proof. Our aim is to show that Z(G) = G. At any rate, we already know that Z(G) =1- (e) is a subgroup of G so that o(Z(G)) = p or p2 • If o(Z(G)) = p 2 , then Z(G) = G and we are done. Suppose that o(Z(G)) = p; let a E G, a¢ Z(G). Thus N(a) is a subgroup of G, Z(G) c N(a), 2 a E N(a),so that o(N(a)) > p, yet by Lagrange's theorem o(N(a)) I o(G) = P · The only way out is for o(N(a)) = p 2 , implying that a E Z(G), a con· tradiction. Thus o(Z(G)) =pis not an actual possibility. APPLICATION 2 We now use Theorem 2.11.1 to prove an important theorem due to Cauchy. The reader may remember that this theorem was already proved for abelian groups as an application of the results Ueveloped in the section on homomorphisms. In fact, we shall make use of this special 1 Sec. 2.11 Another Counting Principle in the proof below. But, to be frank, we shall prove, in the very next ~'•~ttcJn, a much stronger result, due to Sylow, which has Cauchy's theorem an immediate corollary, in a manner which completely avoids Theorem 1.1. To continue our candor, were Cauchy's theorem itself our ultimate only goal, we could prove it, using the barest essentials of group theory, few lines. [The reader should look up the charming, one-paragraph of Cauchy's theorem found by McKay and published in the American Monthly, Vol. 66 (1959), page 119.] Yet, despite all these arguments we present Cauchy's theorem here as a striking illustration Theorem 2 .11.1. If p is a prime number and p I o (G), then ~' Proof. We seek an element a =I= e E G satisfying aP = e. To prove its ~stence we proceed by induction on a( G); that is, we assume the theorem 1,~be true for all groups T such that o( T) < o(G). We need not worry .,.,ut starting the induction for the result is vacuously true for groups of ,frder 1. r~r If for any subgroup W of G, W =I= G, were it to happen that p I o( W), :~n by our induction hypothesis there would exist an element of order p in , and thus there would be such an element in G. Thus we may assume that not a divisor of the order of any proper subgroup of G. In particular, if ~~~f Z(G), since N(a) =I= G, p ,r o(N(a)). Let us write down the class !~~uation: o(G) o(G) = o(Z(G)) + L (N( )) N(a)t=G 0 a ~~ince pI a( G), p ,r o(N(a)) we have that I o(G) p o(N(a))' pI L o(G) ; N(a)t=G o(N(a)) we also have that pI o(G), we conclude that pI ( o(G) - L o(G) ) = o(Z(G)). N(a)t=G o(N(a)) is thus a subgroup of G whose order is divisible by p. But, after all, have assumed that p is not a divisor of the order of any proper subgroup so that Z(G) cannot be a proper subgroup of G. We are forced to 87 88 Group Theory Ch. 2 accept the only possibility left us, namely, that Z (G) = G. But then c is abelian; now we invoke the result already established for abelian groups to complete the induction. This proves the theorem. We conclude this section with a consideration of the conjugacy relation in a specific class of groups, namely, the symmetric groups sn\" Given the integer n we say the sequence of positive integers n1 , n2 , ... , n,, n1 ::; n2 ::; • • • ::; n, constitute a partition of n if n = n1 + n2 + · · · + n,. Let p(n) denote the number of partitions of n. Let us determine p(n) for small values of n: p( 1) p(2) p(3) 1 since 1 2 since 2 3 since 3 1 is the only partition of 1, 2 and 2 = 1 + 1, 3, 3 + 2, 3 = 1 + + 1, p(4) = 5 since 4 = 4, 4 1 + 3, 4 = 1 + 1 + 2, 4 = 1 + 1 + 1 + 1,4 = 2 + 2. Some others are p(5) = 7, p(6) = 11, p(61) = 1,121,505. There is a large mathematical literature on p(n). Every time we break a given permutation in Sn into a product of disjoint cycles we obtain a partition of n; for if the cycles appearing have lengths nv n2 , ... , n, respectively, n1 ::; n2 ::; • • • ::; n,, then n = n1 + n2 + · · · + n,. We shall say a permutation a E sn has the cycle decomposition {n1, nz, ... , n,} if it can be written as the product of disjoint cycles of lengths n1, n2 , •.• , n, n1 ::; n2 ::; • • • ::; n,. Thus in S9 a= G 2 3 3 4 5 6 2 5 6 4 7 8 7 9 ~) = (1) (2. 3)( 4, 5, 6)(7) (8, 9) has cycle decomposition {1, 1, 2, 2, 3}; note that 1 + 1 + 2 + 2 + 3 = 9. We now aim to prove that two permutations in Sn are conjugate if and only if they have the same cycle decomposition. Once this is proved, then sn will have exactly p(n) conjugate classes. To reach our goal we exhibit a very simple rule for computing the con- jugate of a given permutation. Suppose that a E sn and that a sends i ~ j. How do we find e- 1a8 where () E Sn? Suppose that () sends i ~ s and j ~ t; then ()- 1 a() sends s ~ t. In other words, to compute ()- 1 a() replace every symbol in a by its image under 8. For example, to determine e- 1 oB where () = ( 1, 2, 3) ( 4, 7) and a = (5, 6, 7)(3, 4, 2), then, since () :5 ~ 5, 6 ~ 6, 7 ~ 4, 3 ~ 1, 4 ~ 7, 2 ~ 3, e- 1a() is obtained from a by re- placing in a, 5 by 5, 6 by 6, 7 by 4, 3 by 1, 4 by 7, and 2 by 3, so that e- 1ae = (5, 6, 4)(1, 7, 3). , With this algorithm for computing conjugates it becomes clear that two permutations having the same cycle decomposition are conjugate. For if l Sec. 2.11 Another Counting Principle :::: (a1, a2, ... , anJ (b1, b2, ... , bn2) · · · (x1, x2, ... , xnJ and T = (oc1, oc2, •. ' ocnJ (/31, /32, ... ' !3nJ ... Cxt, X2, ... ' Xn), then L = e- 1aB, where could use as e the permutation (a1 a2 anl bl bn2 xl Xnr) OCt oc2 ocn1 f3t /3n2 Xt Xnr for instance, (1, 2)(3, 4, 5)(6, 7, 8) and (7, 5)(1, 3, 6)(2, 4, 8) can be ted as conjugates by using the conjugating permutation (~ 2 3 4 5 6 7 ~). 5 3 6 2 4 That two conjugates have the same cycle decomposition is now trivial by our rule, to compute a conjugate, replace every element in a given by its image under the conjugating permutation. We restate the result proved in the previous discussion as The number if conjugate classes in Sn is p(n), the number if Since we have such an explicit description of the conjugate classes in we can find all the elements commuting with a given permutation. We te this with a very special and simple case. Given the permutation (1, 2) in Sm what elements commute with it? · y any permutation leaving both 1 and 2 fixed does. There are - 2)! such. Also (1, 2) commutes with itself. This way we get 2(n - 2)! ts in the group generated by (1, 2) and the (n - 2)! permutations 1 and 2 fixed. Are there others? There are n(n- 1)/2 trims- IOSJLtlcms and these are precisely all the conjugates of (1, 2). Thus the con- class of (1, 2) has in it n(n - 1)/2 elements. If the order of the _..., ........ ,aa.<A .. J. of (1, 2) is r, then, by our counting principle, n(n - 1) = o(Sn) n! 2 r r us r = 2(n- 2)!. That is, the order of the normalizer of (1, 2) is (n - 2)!. But we exhibited 2(n - 2)! elements which commute with I, 2); thus the general element a commuting with (1, 2) is a = (1, 2)i-r, i = 0 or 1, -r is a permutation leaving both 1 and 2 fixed. As another application consider the permutation (1, 2, 3, ... ' n) E sn. claim this element commutes only with its powers. Certainly it does ute with all its powers, and this gives rise to n elements. Now, any is conjugate to (1, 2, ... , n) and there are (n - 1)! distinct U•I\"'<Tr>l·\"'\" in sn. Thus if u denotes the order of the normalizer of (1' 2, ... ' n) 89 90 Group Theory Ch. 2 in S\"' smce o(Sn) Ju (n- 1)!, number of conjugates of (1, 2, ... ' n) Ill sn :::::: n! u = n. (n - 1)! So the order of the normalizer of (1, 2, ... , n) in Sn is n. The powers of (1, 2, ... , n) having given us n such elements, there is no room left for others and we have proved our contention. Problems 1. List all the conjugate classes in S3 , find the c 0 's, and verify the class equation. 2. List all the conjugate classes in S4 , find the ca's and verify the class equation. 3. List all the conjugate classes in the group of quaternion units (see Problem 21, Section 2.10), find the c 0 's and verify the class equation. 4. List all the conjugate classes in the dihedral group of order 2n, find the c 0 's and verify the class equation. Notice how the answer depends on the parity of n. 1 I 5. (a) In Sn prove that there are- n. distinct r cycles. r (n - r)! (b) Using this, find the number of conjugates that the r-cycle (1, 2, ... ' r) has in sn. (c) Prove that any element a in Sn which commutes with (1, 2, ... , r) is of the form a = (1, 2, ... , r);r, where i = 0, 1, 2, ... , r, r is a permutation leaving all of 1, 2, ... , r fixed. 6. (a) Find the number of conjugates of (1, 2) (3, 4) inS\"' n ~ 4. (b) Find the form of all elements commuting with (1, 2) (3, 4) in Sn. 7. If p is a prime number, show that in S P there are (p - 1) ! + 1 elements x satisfying xP = e. 8. If in a finite group G an element a has exactly two conjugates, prove that G has a normal subgroup N =/= (e), G. 9. (a) Find two elements in As, the alternating group of degree 5, which are conjugate in Ss but not in As. (b) Find all the conjugate classes in As and the number of elements in each conjugate class. 10. (a) If N is a normal subgroup of G and a E N, show that every con- jugate of a in G is also in N. (b) Prove that o(N) = .L c 0 for some choices of a in N. 1 Sec. 2.12 Sylow's Theorem 91 (c) Using this and the result for Problem 9(b), prove that in As there is no normal subgroup N other than (e) and As. 1. Using Theorem 2.11.2 as a tool, prove that if o( G) = pn, p a prime number, then G has a subgroup of order pa. for all 0 :$; a :$; n. If o(G) = pn, p a prime number, prove that there exist subgroups Ni, i = 0, 1, ... , r (for some r) such that G = N0 ::J N 1 ::J N 2 ::J • • • ::::> Nr = (e) where Ni is a normal subgroup of Ni-l and where '.. Ni_ 1 fNi is abelian. 'Ll3. If o(G) = pn, p a prime number, and H =I= G is a subgroup of G, ·· show that there exists an x E G, x ¢ H such that x- 1Hx = H. 14. Prove that any subgroup of order pn- 1 in a group G of order pn, p a prime number, is normal in G. •15. If o(G) = pn, p a prime number, and if N =I= (e) is a normal subgroup of G, prove that N n Z =I= (e), where Z is the center of G. 16. If G is a group, Zits center, and if GfZ is cyclic, prove that G must be abelian. 17. Prove that any group of order 15 is cyclic. 18. Prove that a group of order 28 has a normal subgroup of order 7. 19. Prove that if a group G of order 28 has a normal subgroup of order 4, then G is abelian. t' 2.12 Sylow's Theorem ·if··:Lagrange's theorem tells us that the order of a subgroup of a finite grolJp is 'ta divisor of the order of that group. The converse, however, is false. There tare very few theorems which assert the existence of subgroups of prescribed ~~E:order in arbitrary finite groups. The most basic, and widely used, is a ~::J .. classic theorem due to the Norwegian mathematician Sylow. ,if: We present here three proofs of this result of Sylow. The first is a very :!)\\~ elegant and elementary argument due to Wielandt. It appeared in the journal Archiv der Matematik, Vol. 10 ( 1959), pages 401-402. The basic elements in Wielandt's proof are number-theoretic and combinatorial. It has the advantage, aside from its elegance and simplicity, of producing the ubgroup we are seeking. The second proof is based on an exploitation of duction in an interplay with the class equation. It is one of the standard lassical proofs, and is a nice illustration of combining many of the ideals developed so far in the text to derive this very important cornerstone due to Sylow. The third proof is of a completely different philosophy. The basic idea there is to show that if a larger group than the one we are considering satisfies the conclusion of Sylow's theorem, then our group also must. 92 Group Theory Ch. 2 This forces us to prove Sylow's theorem for a special family of groups-the symmetric groups. By invoking Cayley's theorem (Theorem 2.9.1) we are then able to deduce Sylow's theorem for all finite groups. Apart from this strange approach-to prove something for a given group, first prove it for a much larger one-this third proof has its own advantages. Exploiting the ideas used, we easily derive the so-called second and third parts of Sylow's theorem. One might wonder: why give three proofs of the same result when, clearly, one suffices? The answer is simple. Sylow's theorem is that important that it merits this multifront approach. Add to this the completely diverse nature of the three proofs and the nice application each gives of different things that we have learned, the justification for the whole affair becomes persuasive (at least to the author). Be that as it may, we state Sylow's theorem and get on with Wielandt's proof. THEOREM 2.12.1 (SYLow) If p is a prime number and prx I o(G), then G has a subgroup of order prx. Before entering the first proof of the theorem we digress slightly to a brief number-theoretic and combinatorial discussion. The number of ways of picking a subset of k elements from a set of n elements can easily be shown to be (~) = k!(n n~ k)! If n = prxm where p is a prime number, and if prIm but pr+ 1 % m, consider ( prxm) (prxm) ! prx = (prx) ! (prxm _ prx) ! prxm (prxm - 1) · · · (prxm - i) · · · (pam - pa + 1) pa (prx - 1) · · • (pa - i) • • • (pa - pa + 1) The question is, What power of p divides t;:)? Looking at this number, written out as we have written it out, one can see that except for the terrn m in the numerator, the power of p dividing (pam - i) is the same as that dividing prx - i, so all powers of p cancel out except the power which divides m. Thus 1 . Sec. 2.12 Sylow's Theorem 93 First Proof of the Theorem. Let .A be the set of all subsets of G which ~ve p• elements. Thus j{ has (p;~) elements. Given M 1 , M 2 E ,j/ ,~JJ is a subset of G having prx. elements, and likewise so is M 2 ) define ~-1 ,..., M 2 if there exists an element g E G such that M 1 = M 2g. It is ' · .•. mediate to verify that this defines an equivalence relation on .A. We im that there is at least one equivalence class of elements in .A such that ·,.·~Jhe number of elements in this class is not a multiple of pr+ 1, for if pr+ 1 is · · divisor of the size of each equivalence class, then pr + 1 would be a divisor l;)f the number of elements in Jt. Since Jt bas (p;~) elements and 'jf+l ,j' t;~} this cannot be the case. Let {M., ... , M.} be such an kequivalence class in .A where pr+ 1 % n. By our very definition of equivalence Jn ..It, if g E G, for each i = 1, ... , n, Mig = Mi for some J, 1 ::; J ::; n. We let H = {g E G I M 1g = M 1 }. Clearly H is a subgroup of G, for if ,a, bE H, then M 1a = M 1 , M 1b = M 1 whence M 1ab = (M 1a)b = M;b = ,;~IJ 1 • We shall be vitally concerned with o(H). We claim that no(H) = .... (G); we leave the proof to the reader, but suggest the argument used in ;. the counting principle in Section 2.11. Now no(H) = o(G) = prx.m; since 1 ';~JI+ 1 % n and prx.+r I prx.m = no(H), it must follow that Prx.l o(H), and so ~'.t~(H) ~ prx.. However, if m1 E M 1 , then for all hE H, m1h E M 1 • Thus ··· ;M1 has at least o(H) distinct elements. However, M 1 was a subset of G .rontaining prx. elements. Thus prx. ~ o(H). Combined with o(H) ~ prx. we i have that o(H) = prx.. But then we have exhibited a subgroup of G having exactly p« elements, namely H. This proves the theorem; it actually has done more- :it has constructed the required subgroup before our very eyes! ..,.. What is usually known as Sylow's theorem is a special case of Theorem ~.12.1, namely that {COROLLARY If pm I o(G), pm+ 1 % o(G), then G has a subgroup of order pm. A subgroup of G of order pm, where pm I o(G) but pm+ 1 % o(G), is called a Sylow subgroup of G. The corollary above asserts that a finite group has Sylow subgroups for every prime p dividing its order. Of course the njugate of a p-Sylow subgroup is a p-Sylow subgroup. In a short while e shall see how any two p-Sylow subgroups of G-for the same prime p- e related. We shall also get some information on how many p-Sylow hgroups there are in G for a given prime p. Before passing to this, we want give two other proofs of Sylow's theorem. We begin with a remark. As we observed just prior to the corollary, e corollary is a special case of the theorem. However, we claim that the 94 Group Theory Ch. 2 theorem is easily derivable from the corollary. That is, if we know that G possesses a subgroup of order pm, where pm I o(G) but pm+ 1 ,f' o(G), then we know that G has a subgroup of order pa. for any r:t such that Pa.l o(G). This follows from the result of Problem 11, Section 2.11. This result states that any group of order pm, p a prime, has subgroups of order pa. for any 0 ~ r:t ~ m. Thus to prove Theorem 2.12.1-as we shall proceed to do, again, in two more ways-it is enough for us to prove the existence of p-Sylow subgroups of G, for every prime p dividing the order of G. Second Proof of Sylow's Theorem. We prove, by induction on the order of the group G, that for every prime p dividing the order of G, G has a p-Sylow subgroup. If the order of the group is 2, the only relevant prime is 2 and the group certainly has a subgroup of order 2, namely itself. So we suppose the result to be correct for all groups of order less than o(G). From this we want to show that the result is valid for G. Suppose, then, that pm I o(G), pm+ 1 ,V o(G), where p is a prime, m 2 1. If pm I o(H) for any subgroup H of G, where H =P G, then by the induction hypothesis, H would have a subgroup T of order pm. However, since Tis a subgroup of H, and His a subgroup of G, T too is a subgroup of G. But then T would be the sought-after subgroup of order pm. We therefore may assume that pm ,f' o(H) for any subgroup H of G, where H =P G. We restrict our attention to a limited set of such subgroups. Recall that if a E G then N(a) = {x E G I xa = ax} is a subgroup of G; moreover, if a¢ Z, the center of G, then N(a) =P G. Recall, too, that the class equation of G states that o(G) - \" o(G) - ~ o(N(a))' where this sum runs over one element a from each conjugate class. We separate this sum into two pieces: those a which lie in Z, and those which don't. This gives o(G) = z + L o(G) , a¢Z o(N(a)) where z = o(Z). Now invoke the reduction we have made, namely, that pm ,f' o(H) for any subgroup H =P G of G, to those subgroups N(a) for a¢ Z. Since in this case, pm I o(G) and pm ,f' o(N(a) ), we must have that p . I o(G) o(N(a)) Restating this result, I o(G) p o(N(a)) I Sec. 2.12 Sylow's Theorem 95 4for every a E G where a ¢ Z. Look at the class equation with this information ill band. Since pm I o( G), we have that p I o(G); also I \" o(G) p LJ . a¢Z o(N(a)) us the class equation gives us that pI z. Since p I z = o(Z), by Cauchy's eorem (Theorem 2.11.3), Z has an element b i= e of order p. Let ;~'tJ = (b), the subgroup of G generated by b. B is of order p; moreover, :::j;nce bE Z, B must be normal in G. Hence we can form the quotient group .IJ = GfB. We look at G. First of all, its order is o( G) fo(B) = o(G) fp, >hence is certainly less than o( G). Secondly, we have pm- 1 I o(G), but J!' ,r o(G). Thus, by the induction hypothesis, G has a subgroup P of order f\"- 1• Let P =:' {x E G I xB E P}; by Lemma 2. 7.5, P is a subgroup of G. Moreover, P ~ PjB (Prove!); thus pm- 1 = o(P) = o(P) = o(P). o(B) p •;~This results in o(P) = pm. Therefore Pis the required p-Sylow subgroup of .J(]. This completes the induction and so proves the theorem. With this we have finished the second proof of Sylow's theorem. Note lthat this second proof can easily be adapted to prove that if pr~- I o(G), then ·· .. 6 has a subgroup of order pr~- directly, without first passing to the existence ,of a p-Sylow subgroup. (This is Problem 1 of the problems at the end of ':i,rus section.) ·· We now proceed to the third proof of Sylow's theorem. , Third Proof of Sylow's Theorem. Before going into the details of the ~;l.proof proper, we outline its basic strategy. We will first show that the ;·<~metric groups SPr' p a prime, all have p-Sylow subgroups. The next ;Step will be to show that if G is contained in M and M has a p-Sylow sub- ', group, then G has a p-Sylow subgroup. Finally we will show, via Cayley's j~~~eorem, that we can use SPk' for large enough k, as our M. With this we , 11 have all the pieces, and the theorem will drop out. In carrying out this program in detail, we will have to know how large P-Sylow subgroup of Spr should be. This will necessitate knowing what Wer of p divides (p')!. This will be easy. To produce the p-Sylow sub- oup of Spr will be harder. To carry out another vital step in this rough etch, it will be necessary to introduce a new equivalence relation in groups, d the corresponding equivalence classes known as double cosets. This ·n have several payoffs, not only in pushing through the proof of Sylow's eorem, but also in getting us the second and third parts of the full Sylow 16 Group Theory Ch. 2 So we get down to our first task, that of finding what power of a prime p exactly divides (pk)!. Actually, it is quite easy to do this for n! for any integer n (see Problem 2). But, for our purposes, it will be clearer and will suffice to do it only for (pk)!. Let n(k) be defined by pn(k) I (pk)! but pn(k)+ 1 .{ (pk)!. LEMMA 2.12.1 n(k) = 1 + p + . ·. + pk- 1 • Proof. If k = 1 then, since p! = 1 · 2 · · · (p - 1) · p, it is clear that PIP!butp 2 .{p!. Hencen(1) = 1,asitshouldbe. What terms in the expansion of (pk)! can contribute to powers of p dividing (pk) !? Clearly, only the multiples of p; that is, p, 2p, ... ,p k- 1p. In other words n(k) must be the power of p which divides p(2p)(3p) · · · (pk- 1p) = pPk- 1 (pk- 1)!. But then n(k) = pk- 1 + n(k - 1). Similarly, n(k - 1) = n(k - 2) + pk- 2 , and so on. Write these out as n ( k) - n ( k - 1) = pk- 1, n(k- 1) - n(k - 2) = pk- 2 , n(2) - n(l) = p, n(1) = 1. Adding these up, with the cross-cancellation that we get, we obtain n(k) = 1 + p + p 2 + · · · + pk- 1. This is what was claimed in the lemma, so we are done. We are now ready to show that Spk has a p-Sylow subgroup; that is, we shall show (in fact, produce) a subgroup of order pn(k) in spk· LEMMA 2.12.2 Spk has a p-Sylow subgroup. Proof. We go by induction on k. If k = 1, then the element (1 2 ... p), in SP is of order p, so generated a subgroup of order p. Since n(1) = 1, the result certainly checks out for k = 1. Suppose that the result is correct for k - 1; we want to show that it then must follow for k. Divide the integers 1, 2, ... , pk into p clumps, each with pk- 1 elements as follows: {1, 2, ... ,pk-1}, {pk-1 + 1,pk-1 + 2, ... , 2pk-1}, ... , {(p - 1)pk-1 + 1, ... 'pk}. The permutation u defined by u = (1,pk- 1 + 1, 2pk- 1 + 1, ... , (p-1)pk-1 + 1)···(j,pk-1 +j,2pk-1 +J, ... ,(p-1)pk-1 + 1 +})··· (pk -1, 2pk -1, ... , (p - 1 )pk- 1, pk) has the following properties: 1. uP = e. Sec. 2.12 Sylow's Theorem 97 2. If T is a permutation that leaves all i fixed for i > pk- 1 (hence, affects only 1, 2, ... , pk- 1 ), then u- 1-ru moves only elements in {pk- 1 + 1, pk- 1 + 2, ... , 2pk- 1 }, and more generally, a- i-rui moves only elements in {jpk-1 + l,jpk-1 + 2, ... , (j + l)pk-1 }. Consider A = {-r E spk I -r(i) = i if i > pk- 1 }. A is a subgroup of spk and elements in A can carry out any permutation on 1, 2, ... , pk- 1 . From this it follows easily that A ~ Spk-l· By induction, A has a subgroup P1 of order pn(k-1). Let T = p 1 ((I- 1 p 1 (I) ((I- 2 p 1 (I 2) . • • ((I- (p- 1) p 1 uP- 1) = p 1 p 2 ..• p n- 1' where Pi = a- ip1 ui. Each Pi is isomorphic to P1 so has order pn<k- 1 >. Also elements in distinct P/s influence nonoverlapping sets of integers, hence commute. Thus T is a subgroup of Spk· What is its order? Since Pin Pi = (e) if 0 :::;; i =I= j:::;; p - 1, we see that o(T) = o(P1)P = ppn(k-1). We are not quite there yet. Tis not the p-Sylow subgroup we seek! Since uP = e and a- ip1 ui =·Pi we have a- 1 Tu = T. Let P = {uit I t E T, 0 :::;; j :::;; p - 1 }. Since a¢= T and a- 1 Tu = T we have two things: firstly, T is a subgroup of Spk and, furthermore, o(P) = p · o( T) = p. pn(k- 1>P = pn(k- 1>P+ 1 . Now we are finally there! P is the sought-after p-Sy1ow subgroup of Spk· Why? Well, what is its order? It is pn<k- 1)P~ 1 . But n(k -1) = 1 +P + ··· +Pk- 2 , hencepn(k- 1) + 1 = 1 +P + ··· +Pk-l = n(k). Since now o(P) = pn<k>, Pis indeed a p-Sylow subgroup of Spk· Note something about the proof. Not only does it prove the lemma, it actually allows us to construct the p-Sylow subgroup inductively. We follow the procedure of the proof to construct a 2-Sylow subgroup in S4 • Divide 1, 2, 3, 4 into {1, 2} and {3, 4}. Let P 1 = ((1 2)) and a\"'~ (1 3)(2 4). Then P2 = u- 1P 1u = (3 4). Our 2-Sylow subgroup is then the group generated by (1 3)(2 4) and T = P1P2 = {(1 2), (3 4),(1 2)(3 4), e}. In order to carry out the program of the third proof that we outlined, we now introduce a new equivalence relation in groups (see Problem 39, Section 2.5). DEFINITION Let G be a group, A, B subgroups of G. If x,y E G define x \"' y if y = axb for some a E A, b E B. We leave to the reader the verification-it is easy-of LEMMA 2.12.3 The relation difined above is an equivalence relation on G. The equivalence class if x E G is the set AxB = {axb I a E A, b E B}. 18 Group Theory Ch. 2 We call the set AxB a double coset of A, Bin G. If A, B are finite subgroups of G, how many elements are there in the double coset AxB? To begin with, the mapping T:AxB --+ AxBx- 1 given by (axb) T = axbx- 1 is one-to-one and onto (verify). Thus o(AxB) = o(AxBx- 1 ). Since xBx- 1 is a subgroup ofG, oford.er o(B), by Theorem 2.5.1, o(AxB) = o(AxBx- 1 ) We summarize this in o(A)o(xBx- 1 ) o(A n xBx- 1 ) LEMMA 2.12.4 If A, B are finite subgroups of G then o(AxB) = o(A)o(B) o(A n xBx- 1 ) o(A)o(B) We now come to the gut step in this third proof of Sylow's theorem. LEMMA 2.12.5 Let G be a finite group and suppose that G is a subgroup of the finite group M. Suppose further that M has a p-Sylow subgroup Q. Then G has a p-Sylow subgroup P. lnfact, P = G n xQx- 1 for some x EM. Proof. Before starting the details of the proof, we translate the hypoth- eses somewhat. Suppose that pm I o(M), pm+ 1 % o(M), Q is a subgroup of M of order pm. Let o(G) = pnt where p ,f' t. We want to produce a sub- group PinG of order pn. Consider the double coset decomposition of M given by G and Q; M = U GxQ. By Lemma 2.12.4, o(GxQ) = o(G)o(Q) o(G n xQx- 1 ) Since G n xQx- 1 is a subgroup of xQx- 1 , its order is pmx. We claim that mx = n for some x E M. If not, then P nt11m o(GxQ) = _r_ = tpm+n-mx, pmx so is divisible by pm+ 1 • Now, since M = U GxQ, and this is disjoint union, o(M) = .L o(GxQ), the sum running over one element from each double coset. But pm+ 1 1 o(GxQ); hence pm+ 1 1 o(M). This contradicts pm+ 1 ,f' o(M). Thus mx = n for some x EM. But then o(G n xQx- 1 ) = pn. Since G n xQ x- 1 = Pis a subgroup of G a~d has order pn, the lemma is proved. We now can easily prove Sylow's theorem. By Cayley's theorem (Theorem 2.9.1) we can isomorphically embed our finite group Gin Sm the symmetric group of degree n. Pick k so that n < pk; then we can iso- morphically embed sn in spk (by acting on 1, 2, ... ' n only in the set 1 I Sec. 2.12 Sylow's Theorem 99 I, 2, ... , n, ... , pk), hence G is isomorphically embedded in Spk· By Lemma 2.12.2, Spk has a p-Sylow subgroup. Hence, by Lemma 2.12.5, G must have a p-Sylow subgroup. This finishes the third proof of Sylow's theorem. This third proof has given us quite a bit more. From it we have the machinery to get the other parts of Sylow's theorem. THEOREM 2.12.2 (SECOND PART OF SvLow's THEOREM) If G is a finite group, p a prime and pn I o(G) but pn+ 1 ,f' o(G), then atry two subgroups ofG of order pn are conjugate. Proof. Let A, B be subgroups of G, each of order pn. We want to show that A = gBg- 1 for some g E G. Decompose G into double cosets of A and B; G = U AxB. Now, by Lemma 2.12.4, o(AxB) = o(A)o(B) o(A n xBx- 1 ) If A =f:. xBx- 1 for every x E G then o(A n xBx- 1 ) = pm where m < n. Thus o(AxB) o(A)o(B) = P 2 n = p2n-m pm pm and 2n - m ~ n + 1. Since pn+ 1 I o(AxB) for every x and since o(G) L o(AxB), we would get the contradiction pn+ 1 I o(G). Thus A = gBg- 1 for some g E G. This is the assertion of the theorem. Knowing that for a given prime p all p-Sylow subgroups of G are conjugate allows us to count up precisely how many such p-Sylow subgroups there are in G. The argument is exactly as that given in proving Theorem 2.11.1. In some earlier problems (see, in particular, Problem 16, Section 2.5) we discussed the normalizer N(H), of a subgroup, defined by N(H) = {x E G I xHx- 1 = H}. Then, as in the proof of Theorem 2.11.1, we have that the number of distinct conjugates, xHx- 1 , of H in G is the index of N(H) in G. Since all p-Sylow subgroups are conjugate we have LEMMA 2.12.6 The number of p-Sylow subgroups in G equals o(G)fo(N(P)), where Pis any p-Sylow subgroup of G. In particular, this number is a divisor of o(G). However, much more can be said about the number of p-Sylow subgroups there are, for a given prime p, in G. We go into this now. The technique will involve double cosets again. ' 100 Group Theory Ch. 2 THEOREM 2.12.3 (THIRD PART OF SYLow's THEOREM) The number oj p-Sylow subgroups in G, for a given prime, is of the form 1 + kp. Proof. Let P be a p-Sylow subgroup of G. We decompose G into double cosets of P and P. Thus G = U PxP. We now ask: How many elements are there in PxP? By Lemma 2.12.4 we know the answer: o(PxP) = o(P) 2 o(P n xPx- 1) Thus, if P n xPx- 1 =1= P then pn+ 1 1 o(PxP), where pn = o(P). Para- phrasing this: ifx ¢: N(P) thenpn+ 1 I o(PxP). Also, ifx E N(P), then PxP = P(Px) = P 2 x = Px, so o(PxP) = pn in this case. Now o(G) = L o(PxP) + L o(PxP), xeN(P) x¢N(P) where each sum runs over one element from each double coset. However, if x E N(P), since PxP = Px, the first sum is merely LxeN(P) o(Px) over the distinct cosets of Pin N(P). Thus this first sum is just o(N(P)). What about the second sum? We saw that each ofits constituent terms is divisible by pn+ 1 , hence pn+ 1 I L o(PxP). x¢N(P) We can thus write this second sum as L o(PxP) = pn+ 1u. x ¢ N(P) Therefore o(G) = o(N(P)) + pn+ 1u, so o(G) pn+ 1u =1+--- o(N(P)) o(N(P)) Now o(N(P)) I o(G) since N(P) is a subgroup of G, hence pn+ 1ufo(N(P)) is an integer. Also, since pn+ 1 % o(G), pn+ 1 can't divide o(N(P) ). But then pn+ 1ufo(N(P)) must be divisible by p, so we can write pn+ 1ufo(N(P)) as kp, where k is an integer. Feeding this information back into our equation above, we have o(G) = 1 + k . o(N(P)) p Recalling that o(G)fo(N(P)) is the number of p-Sylow subgroups in G, we have the theorem. In Problems 20-24 in the Supplementary Problems at the end of this chapter, there is outlined another approach to proving the second and third parts of Sylow's theorem. We close this section by demonstrating how the various v.~.rts of Sylow's theorem can be used to gain a great deal of information about finite groups. Sec. 2.12 Sylow's Theorem 101 Let G be a group of order 11 2 ·13 2 . We want to determine how many 11-Sylow subgroups and how many 13-Sylow subgroups there are in G. The number of 11-Sylow subgroups, by Theorem 2.12.13, is of the form 1 +Ilk. By Lemma 2.12.5, this must divide 11 2 ·13 2 ; being prime to 11, it must divide 13 2 • Can 13 2 have a factor of the form 1 + Ilk? Clearly no, other than 1 itself. Thus I + Ilk= I, and so there must be only one II-Sylow subgroup in G. Since all II-Sylow subgroups are conjugate (Theorem 2.I2.2) we conclude that the II-Sylow subgroup is normal in G. What about the I3-Sylow subgroups? Their number is of the form I + 13k and must divide II 2 ·13 2 , hence must divide Il 2 . Here, too, we conclude that there can be only one I3-Sylow subgroup in G, and it must be normal. We now know that G has a normal subgroup A of order Il 2 and a normal subgroup B of order I3 2 . By the corollary to Theorem 2.I1.2, any group of order p2 is abelian; hence A and Bare both abelian. Since An B =(e), we easily get AB = G. Fi~ally, if a E A, b E B, then aba- 1b- 1 = a(ba- 1b- 1 ) EA since A is normal, and aba- 1b- 1 = (aba- 1 )b- 1 EB since B is normal. Thus aba- 1b- 1 E A n B = (e). This gives us aba- 1b- 1 = e, so ab = ba for a E A, bE 13. This, together with AB = G, A, B abelian, allows us to conclude that G is abelian. Hence any group of order II 2 ·I3 2 ust be abelian. We give one other illustration of the use of the various parts of Sylow's theorem. Let G be a group of order 72; o(G) = 2 3 3 2 • How many 3-Sylow subgroups can there beinG? If this number is t, then, according to Theorem 2.I2.3, t = I + 3k. According to Lemma 2.I2.5, t I 72, and since t is prime to 3, we must have t I 8. The only factors of 8 of the form I + 3k are I and 4; hence t = I or t = 4 are the only possibilities. In other Words G has either one 3-Sylow subgroup or 4 such. If G has 'only one 3-Sylow subgroup, since all 3-Sylow subgroups are conjugate, this 3-Sylow subgroup must be normal in G. In this case G Would certainly contain a nontrivial normal subgroup. On the other hand ifthe number of3-Sylow subgroups ofG is 4, by Lemma 2.12.5 the index of N in G is 4, where N is the normalizer of a 3-Sylow subgroup. But 72 ,Y 4! = (i(N))!. By Lemma 2.9.I N must contain a nontrivial normal subgroup of G (of order at least 3). Thus here again we can conclude that G contains a nontrivial normal subgroup. The upshot of the discussion is that any group of order 72 must have a nontrivial normal subgroup, hence cannot be simple. ;Problems I. Adapt the second proof given of Sylow's theorem to prove directly that ifp is a prime andpa I o(G), then G has a subgroup of order p«. 102 Group Theory Ch. 2 2. If x > 0 is a real number, define [x] to be m, where m is that integer such that m ~ x < m + 1. If p is a prime, show that the power of p which exactly divides n! is given by 3. Use the method for constructing the p-Sylow subgroup of Spk to find generators for (a) a 2-Sylow subgroup in S8 • (b) a 3-Sylow subgroup in S9 • 4. Adopt the method used in Problem 3 to find generators for (a) a 2-Sylow subgroup of S6 • (b) a 3-Sylow subgroup of S6 • 5. If p is a prime number, give explicit generators for a p-Sylow sub- group of sp2· 6. Discuss the number and nature of the 3-Sylow subgroups and 5- Sylow subgroups of a group of order 3 2 ·5 2 • 7. Let G be a group of order 30. (a) Show that a 3-Sylow subgroup or a 5-Sylow subgroup of G must be normal in G. (b) From part (a) show that every 3-Sylow subgroup and every 5-Sylow subgroup of G must be normal in G. (c) Show that G has a normal subgroup of order 15. (d) From part (c) classify all groups of order 30. (e) How many different nonisomorphic groups of order 30 are there? 8. If G is a group of order 231, prove that the 11-Sylow subgroup is in the center of G. 9. If G is a group of order 385 show that its 11-Sylow subgroup is normal and its 7-Sylow subgroup is in the center of G. 10. If G is of order 108 show that G has a normal subgroup of order 3\\ where k ;;::: 2. 11. If o(G) = pq, p and q distinct primes, p < q, show (a) if p ,Y (q - 1), then G is cyclic. *(b) if p I ( q - 1), then there exists a unique non-abelian group of order pq. * 12. Let G be a group of order pqr, p < q < r primes. Prove (a) the r-Sylow subgroup is normal in G. (b) G has a normal subgroup of order qr. (c) if q ,( (r- 1), the q-Sylow subgroup of G is normal in G. 13. If G is of order p 2 q, p, q primes, prove that G has a no·.1trivial nor- mal subgroup. I I Sec. 2.13 Direct Products 103 * 14. If G is of order p 2 q, p, q primes, prove that either a p-Sylow sub- group or a q-Sylow subgroup of G must be normal in G. 15. Let G be a finite group in which (ab)P = aPbP for every a, bEG, where pis a prime dividing o(G). Prove (a) The p-Sylow subgroup of G is normal in G. *(b) If P is the p-Sylow subgroup of G, then there exists a normal subgroup N of G with P n N = (e) and PN = G. (c) G has a nontrivial center. * * 16. If G is a finite group and its p-Sy1ow subgroup P lies in the center of G, prove that there exists a normal subgroup N of G with P n N = (e) and PN = G. *17. If His a subgroup of G, recall that N(H) = {x E G I xHx- 1 = H}. If Pis a p-Sylow subgroup of G, prove that N(N(P)) = N(P). * 18. Let P be a p-Sylow subgroup of G and suppose a, b are in the center of P. Suppose further that a = xbx- 1 for some x E G. Prove that there exists ayE N(P) such that a = yby- 1 • ** 19. Let G be a finite group and suppose that ¢is an automorphism of G such that ¢ 3 is the identity automorphism. Suppose further that ¢(x) = x implies that x = e. Prove that for every prime p which divides o(G), the p-Sylow subgroup is normal in G. #20. Let G be the group of n x n matrices over the integers modulo p, p a prime, which are invertible. Find a p-Sylow subgroup of G. 21. Find the possible number of 11-Sylow subgroups, 7-Sylow subgroups, and 5-Sylow subgroups in a group of order 5 2 • 7 ·11. 22. If G is S3 and A = ( ( 1 2)) in G, find all the double cosets AxA of A in G. 23. If G is S4 and A = ((1 2 3 4)), B = ((1 2)), find all the double cosets AxB of A, B in G. 24. If G is the dihedral group of order 18 generated by a 2 = b9 = e, ab = b- 1a, find the double cosets for H, K in G, where H = (a) and K = (b 3 ). 2.13 Direct Products On several occasions in this chapter we have had a need for constructing a new group from some groups we already had on hand. For instance, towards the end of Section 2.8, we built up a new group using a given group and one of its automorphisms. A special case of this type of construction has been seen earlier in the recurring example of the dihedral group. However, no attempt had been made for some systematic device for II- Group Theory Ch. 2 constructing new groups from old. We shall do so now. The method re- presents the most simple-minded, straightforward way of combining groups to get other groups. We first do it for two groups-not that two is sacrosanct. However, with this experience behind us, we shall be able to handle the case of any finite number easily and with dispatch. Not that any finite number is sacrosanct either; we could equally well carry out the discussion in the wider setting of any number of groups. However, we shall have no need for so general a situation here, so we settle for the case of any finite number of groups as our ultimate goal. Let A and B be any two groups and consider the Cartesian product (which we discussed in Chapter 1) G = A x B of A and B. G consists of all ordered pairs (a, b), where a E A and bE B. Can we use the operations in A and B to endow G with a product in such a way that G is a group? Why not try the obvious? Multiply componentwise. That is, let us define, for (a 1 , b1 ) and (a2 , b2 ) in G, their product via (at, bt)(a2 , b2 ) = (ata 2 , b1b2 ). Here, the product at a2 in the first component is the product of the elements a1 and a2 as calculated in the group A. The product bt b2 in the second component is that of bt and b2 as elements in the group B. With this definition we at least have a product defined in G. Is G a group relative to this product? The answer is yes, and is easy to verify. We do so now. First we do the associative law. Let (av b1 ), (a2 , b2 ), and (a 3 , b3 ) be three elements of G. Then ((a 1 , b1 )(a2 , b2 )) (a 3 , b3 ) = (at a2 , b1 b2 ) (a 3 , b3 ) = ((ata 2 )a3 , (btb 2 )b3 ), while (at, bt)((a2 , b2 )(a3 , b3 )) = (a 1 , bt)(a2a3 , b2b3 ) = (at.(a 2a3 ),bt(b2 b3 )). The associativity of the product in A and in B then show us that our product in G is indeed associative. Now to the unit element. What would be more natural than to try (e,j), where e is the unit element of A and f that of B, as the proposed unit element for G? We have (a, b) (e,J) = (ae, bf) = (a, b) and (e,J)(a, b) = (ea,jb) = (a, b). Thus (e,J) acts as a unit element in G. Finally, we need the inverse in G for any element of G. Here, too, why not try the obvious? Let (a, b) E G; try (a-\\ b- 1 ) as its inverse. Now (a, b)(a-1, b- 1 ) = (aa-1, bb- 1 ) = (e,J) and (a- 1 , b- 1 )(a, b) = (a- 1a, b- 1b) = (e,j), so that (a- 1, b- 1 ) does serve as the inverse for (a, b). With this we have verified that G = A x B is a group. We call it the external direct product of A and B. Since G = A x B has been built up from A and B in such a trivial manner, we would expect that the structure of A and B would reflect heavily in that of G. This is indeed the case. Knowing A and B completely gives us complete information, structurally, about A x B. The construction of G = A x B has been from the outside, external. Now we want to turn the affair around and try to carry it out internally in G. Sec. 2.13 Direct Products 105 Consider A = {(a,f) E G I a E A} c G = A x B, where f is the unit element of B. What would one expect of A? Answer: A is a subgroup of G and is isomorphic to A. To effect this isomorphism, define ¢:A ~ A by ¢(a) = (a,f) for a EA. It is trivial that ¢ is an isomorphism of A onto A. It is equally trivial that A is a subgroup of G. Furthermore, A is normal in G. For if (a,f) E A and (at, bt) E G, then (at, bt) (a,f) (at, bt)- 1 = (at,bt)(a,f)(at- 1,bt- 1 ) = (ataa 1 - 1,b 1jbt-l) = (ataat-\\f)EA. Sowe have an isomorphic copy, A, of A in G which is a normal subgroup of G. What we did for A we can also do for B. If B = {(e, b) E GIbE B}, then B is isomorphic to B and is a normal subgroup of G. We claim a little more, namely G = AB and every g E G has a unique decomposition in the form g = ab with a E A and b E B. For, g = (a, b) = (a,f) (e, b) and, since (a,f) E A and (e, b) E B, we do have g = ab with a = (a,f) and b = (e, b). Why is this unique? If (a, b) = ij, where x E A andj E B, then x = (x,f), x E A andj = (e,y), yE B; thus (a, b) = xj = (x,f)(e,y) = (x,y). This gives x = a and y = b, and so x =a andj = b. Thus we have realized Gas an internal product AB of two normal sub- groups, A isomorphic to A, B toBin such a way that every element g E G has a unique representation in the form g = ab, with a E A and bE B. We leave the discussion of the product of two groups and go to the case of n groups, n > 1 any integer. Let Gt, G2, . .. ' en be any n groups. Let G = Gt X G2 X ••• X en = { (gt, g2, ... , gn) I gi E GJ be the set of all ordered n-tuples, that is, the Cartesian product of Gt, G2, ... , Gn. We define a product in G via (gt, g2, ... , gn)(g~, g;, ... , g~) = (gtg~, g2g;, ... , gng~), that is, via com- ponentwise multiplication. The product in the ith component is carriea in the group Gi. Then G is a group in which (et, e2, ... , en) is the unit ele- ment, where each ei is the unit element ofGi, and where (gt, g2, ... , gn) - 1 = (gt- t, g2- 1 , ... , gn- t). We call this group G the external direct product of Gt, G2, ... ' Gn. In G = Gt X G2 X ••• X en let ei = {(ev e2, ... ' ei-V gi, ei+l' ... ' en) I gi E GJ. Then ei is a normal subgroup of G and is isomorphic to Gi. Moreover, G = ele2 ... en and every g E G has a unique decomposition g = g1g2 ... gm where gl Eel, ... ' gn E en. We leave the verification of these facts to the reader. Here, too, as in the case A x B, we have realized the group G internally as the product of normal subgroups e 1 , ... , en in such a way that every element is uniquely representable as a product of elements g1 • • • gm where each gi E ei. With this motivation we make the DEFINITION Let G be a group and Nt, N2, .. . , Nn normal subgroups of G such that I 106 Group Theory Ch. 2 1. G = N 1 N 2 • • • N n- 2. Given g E G then g = m1m2 • • • mm mi E Ni in a unique way. We then say that G is the internal direct product of N 1 , N 2 , •.• , Nn. Before proceeding let's look at an example of a group G which is the internal direct product of some of its subgroups. Let G be a finite abelian group of order p 1rx 1p 2 a2 • • • Pkak where p 1 , p2 , •.• , Pk are distinct primes and each rti > 0. If P 1 , ... , Pk are the PrSylow subgroup, ... , pk-Sylow subgroup respectively of G, then G is the internal direct product of P 1 , P2 , ..• , Pk (see Problem 5). We continue with the general discussion. Suppose that G is the internal direct product of the normal subgroups N 1, ... , Nn. The N 1, .•. , Nn are groups in their own right-forget that they are normal subgroups of G for the moment. Thus we can form the group T = N1 X N 2 x · · · X Nn, the external direct product of N 1, .•. , Nn. One feels that G and T should be related. Our aim, in fact, is to show that G is isomorphic to T. If we could establish this then we could abolish the prefix external and internal in the phrases external direct product, internal direct product-after all these would be the same group up to isomorphism-and just talk about the direct product. We start with LEMMA 2.13.1 Suppose that G is the internal direct product of N 1, •.. , Nn. Then fori i= j, Ni n Ni = (e), and if a E Ni, bE Ni then ab = ba. Proof. Suppose that x E Ni n Ni. Then we can write x as where et = e, viewing x as an element in Ni. Similarly, we can write x as x = e1 • • • ei · · · ei_ 1xei+ 1 ···em where et = e, viewing x as an element of Ni. But every element-and so, in particular x-has a unique representation in the form m1 m2 • • • mn, where mi E N 1, ... , mn E Nn. Since the two decompositions in this form for x must coincide, the entry from Ni in each must be equal. In our first decomposition this entry is x, in the other it is e; hence x = e. Thus Ni n Ni = (e) fori i= j. Suppose a E Ni, bE Ni, and i i= j. Then aba- 1 E Ni since Ni is normal; thus aba- 1b- 1 E Ni. Similarly, since a- 1 E Ni, ba- 1b- 1 E Ni, whence aba- 1b- 1 E Ni. But then aba- 1b- 1 E Ni n Ni = (e). Thus aba- 1b- 1 = e; this gives the desired result ab = ba. One should point out that if K 1 , ... , Kn are normal su~Toups of G such that G = K 1K 2 • • • Kn and Ki n Ki = (e) for i # j it need not be l Sec. 2.13 Direct Products 107 true that G is the internal direct product of K 1 , ... , Kn- A more stringent condition is needed (see Problems 8 and 9). We now can prove the desired isomorphism between the external and internal direct products that was stated earlier. T H E 0 R EM 2.1 3.1 Let G be a group and suppose that G is the internal direct product of N1, .•. , Nn. Let T = N1 X N2 X • • • X Nn. Then G and T are isomorphic. Proof. Define the mapping tjJ: T --. G by l/J((b1 , b2 , ... , bn)) = b1b2 • • • bn, where each bi E Ni, i = 1, ... , n. We claim that t/1 is an isomorphism ofT onto G. To begin with, 1/J is certainly onto; for, since G is the internal direct product of N 1 , ..• , Nm if x E G then x = a1a2 ···an for some a1 E Nv ... , an E Nn. But then t/1( (av a2 , ... , an)) = a1 a2 • • ·an = x. The mapping , f/1 is one-to-one by the uniqueness of the representation of every element as a product of elements from N1 , ... , Nn. For, if t/1( (a1 , ... , an)) = ~1/J((cv ... , en)), where ai E Ni, ci E Ni, for i = 1, 2, ... , n, then, by the definition of t/J, a1a2 ···an = c1c2 ···en- The uniqueness in the definition tof internal direct product forces a1 = c1 , a2 = c2 , ••• , an = en- Thus 1/J ''is one-to-one. I All that remains is to show that t/J is a homomorphism of Tonto G. ,:If X = (a1 , ... , an), Y = (b1 , ... , bn) are elements ofT then l/J(XY) = t/J((a1 , .•. , an)(b1 , ••• , bn)) = t/J(al bl, azbz, ... ' anbn) = a 1 b1 a2 b2 • • • anbn- However, by Lemma 2.13.1, aibi = biai if i =I= j. This tells us that ~a1b1a2b2 • • ·anbn = a1a2 • • ·anb 1b2 • ··bn. Thus 1/J(XY) = a1a2 •• ·anb1b2 • • ·bn. \\lBut we can recognize a1a2 ···an as t/J((a1, a2 , .•. , an)) = t/J(X) and b1b2 • • ·bn 'as 1/J(Y). We therefore have t/J(XY) = t/J(X)t/J(Y). In short, we have shown ,that l/J is an isomorphism of Tonto G. This proves the theorem. fi Note one particular thing that the theorem proves. If a group G is iisomorphic to an external direct product of certain groups Gi, then G is, 'in fact, the internal direct product of groups Gi isomorphic to the Gi. We :simply say that G is the direct product of the Gi (or G i). In the next section we shall see that every finite abelian group is a direct :product of cyclic groups. Once we have this, we have the structure of all ~finite abelian groups pretty well under our control. f One should point out that the analog of the direct product of groups (exists in the study of almost all algebraic structures. We shall see this later 108 Group Theory Ch. 2 for vector-spaces, rings, and modules. Theorems that describe such an algebraic object in terms of direct products of more describable algebraic objects of the same kind (for example, the case of abelian groups above) are important theorems in general. Through such theorems we can reduce the study of a fairly com.plex algebraic situation to a much simpler one. Problems 1. If A and Bare groups, prove that A x B is isomorphic to B x A. 2. If Gu G2 , G3 are groups, prove that (G1 x G2 ) x G3 is isomorphic to G1 x G2 x G3 • Care to generalize? 3. If T = G1 x G2 X • • • X G n prove that for each i = 1, 2, ... , n there is a homomorphism ¢i ofT onto Gi. Find the kernel of ¢i· 4. Let G be a group and let T = G x G. (a) Show that D = {(g, g) E G x GIg E G} is a group isomorphic to G. (b) Prove that D is normal in T if and only if G is abelian. 5. Let G be a finite abelian group. Prove that G is isomorphic to the direct product of its Sylow subgroups. 6. Let A, B be cyclic groups of order m and n, respectively. Prove that A x B is cyclic if and only if m and n are relatively prime. 7. Use the result of Problem 6 to prove the Chinese Remainder Theorem; namely, if m and n are relatively prime integers and u, v any two integers, then we can find an integer x such that x = u mod m and x = v mod n. 8. Give an example of a group G and normal subgroups N 1, ••• , Nn such that G = N1 N2 • • • Nn and Ni n Ni = (e) for i =I= j and yet G is not the internal direct product of N1, ... , Nw 9. Prove that G is the internal direct product of the normal subgroups N 1 , •.. , Nn if and only if 1. G = N 1 • · • Nn. 2. Ni n (N1 N2 • • ·Ni_ 1 Ni+ 1 • • • Nn) = (e) fori= 1, ... , n. 10. Let G be a group, K 1, .•. , Kn normal subgroups of G. Suppose that K 1 n K 2 n · · · n Kn = (e). Let Vi = G/Ki. Prove that there is an isomorphism of G into vl X vi X ••• X vn\" * 11. Let G be a finite abelian group such that it contains a subgroup H 0 =I= (e) which lies in every subgroup H =1= (e). Prove that G must be cyclic. What can you say about o(G)? 12. Let G be a finite abelian group. Using Problem 11 show that G is isomorphic to a subgroup of a direct product of a finiu number of finite cyclic groups. Sec. 2.14 Finite Abelian Groups 109 13. Give an example of a finite non-abelian group G which contains a subgroup H 0 =f. (e) such that H 0 c H for all subgroups H =f. (e) of G. 14. Show that every group of order p 2 , p a prime, is either cyclic or is isomorphic to the direct product of two cyclic groups each of order p. *15. Let G = A x A where A is cyclic of order p, p a prime. How many automorphisms does G have? 16. If G = K 1 x K 2 X • • • x Kn describe the center of G in terms of those of the K i· 17. IfG = K 1 x K 2 x · · · xKn and g E G, describe N(g) = {x E G I xg = gx}. 18. If G is a finite group and N 1 , ... , Nn are normal subgroups of G such that G = N1 N2 • • • Nn and o(G) = o(N1 )o(N2 ) • • • o(Nn), prove that G is the direct product ·of N 1 , N2 , . .• , Nn. Finite Abelian Groups close this chapter with a discussion (and description) of the structure an arbitrary finite abelian group. The result which we shall obtain is a famous classical theorem, often referred to as the Fundamental Theorem on Finite Abelian Groups. It is a highly satisfying result because of its de- cisiveness. Rarely do we come out with so compact, succinct, and crisp a result. In it the structure of a finite abelian group is completely revealed, and by means of it we have a ready tool for attacking any structural problem about finite abelian groups. It even has some arithm.etic consequen~~s. instance, one of its by-products is a precise count of how many non-. J<~.nrnr .... T\\hu· abelian groups there are of a given order. In all fairness one should add that this description of finite abelian groups r is not as general as we can go and still get so sharp a theorem. As you shall ~see in Section 4.5, we completely describe all abelian groups generated by (a finite set of elements-a situation which not only covers the finite abelian ~group case, but much more. ~· We now state this very fundamental result. f h f!HEOREM 2.14.1 Every finite abelian group is the direct product of cyclic tgroups. ~~ ~ Proof. Our first step is to reduce the problem to a slightly easier one. ~We have already indicated in the preceding section (see Problem 5 there) ~hat any finite abelian group G is the direct product of its Sylow subgroups. ~f we knew that each such Sylow subgroup was a direct product of cyclic ~oups we could put the results together for these Sylow subgroups to 10 Group Theory Ch. 2 realize G as a direct product of cyclic groups. Thus it suffices to prove the theorem for abelian groups of order pn where p is a prime. So suppose that G is an abelian group of order pn. Our objective is to find elements a 1, ... , ak in G such that every element x E G can be written in a unique fashion as x = a1 !1. 1a2 11.2 • • • ak l1.k. Note that if this were true and a 1 , ••• , ak were of order pn 1 , •• • , Pn\\ where n1 ~ n2 ~ • • • ~ nk, then the maximal order of any element in G would be pn1 (Prove!). This gives us a cue of how to go about finding the elements a 1, .•• , ak that we seek. The procedure suggested by this is: let a 1 be an element of maximal order in G. How shall we pick a2 ? Well, if A 1 = (a 1) the subgroup generated by a 1, then a2 maps into an element of highest order in GJA 1 . If we can successfully exploit this to find an appropriate a2 , and if A 2 = (a 2), then a3 would map into an element of maximal order in G/A 1A2 , and so on. With this as guide we can now get down to the brass tacks of the proof. Let a 1 be an element in G of highest possible order, pn1 , and let A 1 = (a 1). Pick b2 in G such that 52 , the image of b2 in G = GJA 1 , has maximal order pn 2 • Since the order of 52 divides that of b2 , and since the order of a1 is maximal, we must have that n1 ~ n2 • In order to get a direct product of A 1 with (b2 ) we would need A1 n (b2 ) = (e); this might not be true for the initial choice of b2 , so we may have to adapt the element b2 • Suppose that A 1 n (b2 ) =f:. (e); then, since b2 P\" 2 E A 1 and is the first power of b2 to fall in A1 (by our mechanism of choosing b2 ) we have that b2 P\"2 = a/ Therefore (a 1i)P\"t -n 2 = (b2 P\" 2 )P\"1 -n 2 = b/\" 1 = e, whence a 1 iP\"t-\" 2 = e. Since a1 is of order Pn 1 we must have that pn1 I ipn1 -n 2 , and so pn 2 I i. Thus, re- calling what i is, we have b2 P\"2 = a1i = a/P\"2 • This tells us that if a2 = a 1 - ib 2 then a2 P\"2 = e. The element a2 is indeed the element we seek. Let A 2 = (a 2 ). We claim that A 1 n A 2 = (e). For, suppose that a2 t E A 1 ; since a2 = a 1 -ib 2 , we get (a 1 -ib 2 )tEA 1 and so b2tEA 1 • By choice ofb 2, this last relation forces pn 2 I t, and since a/\" 2 = ewe must have that a2 t = e. In short A 1 n A 2 = (e). We continue one more step in the program we have outlined. Let b3 E G map into an element of maximal order in Gf(A 1A 2 ). If the order of the image of b3 in Gf(A 1A2 ) is pn\\ we claim that n3 ~ n2 ~ n1 . Why? By the choice of n2 , bl\" 2 E A 1 so is certainly in A 1A 2 • Thus n 3 ~ n2 • Since b3 P\"3 E A 1A 2 , bl\" 3 = a1 i 1a2 i 2 • We claim that pn3 I i 1 and pn3 1 i 2 . For, b3 P\"2 E A 1 hence (a 1i 1a2 i2 )P\"2 -\" 3 = (bl\" 3 )P\"2 -\" 3 = b3 P\"2 E A 1 . This tells us that a2 i 2 p\"2 -n 3 E A 1 and so pn2 I i 2 pn2 -n 3 , which is to say, pn3 I i 2 . Also b3P\"t ::::: e, hence (a 1i 1a2 i 2 )P\"t-\" 3 = bl\" 1 = e; this says that a 1itP\" 1 -\" 3 E A 2 n A 1 = (e), that is, a 1itP\"t -n 3 = e. This yields that pn3 I i 1 . Let i 1 = JtPn3 , i 2 = J2 pn 3 ; thus b3 P\" 3 = a/ 1 P\" 3a/ 2 P\"3 • Let a3 = a1 -ha 2 -hb 3 , A 3 = (a 3); note that al\" 3 =e. We claim that A 3 n (A 1A 2 ) = (e). For if a3 t E A 1A 2 then (a 1 -,' 1a2 -hb 3 )t E A 1A 2 , giving us b3 t E A 1A 2 • But then pn 3 I t, whence, since a3 P\" 3 = e, we have a3 t = e. In other words, A3 n (A1A2 ) = (e). Sec. 2.14 Finite Abelian Groups 111 Continuing this way we get cyclic subgroups A 1 = (a1 ), A 2 = (a 2 ), • •• , Ak = (ak) of order pnt, pn2 , ••• , Pn\\ respectively, with n1 ~ , n2 ~ • • • ~ nk such that G = A 1A 2 • • • Ak and such that, for each i, Ai n (A 1A 2 • • • Ai_ 1 ) = (e). This tells us that every x E G has a unique c' I ': representation as x = a~ a; · · ·a~ where a~ E A 1, ... , a~ E Ak. In other \\words, G is the direct product of the cyclic subgroups A1 , A 2 , •.• , Ak. , The theorem is now proved. DEFINITION If G is an abelian group of order pn, p a prime, and G = A1 x A 2 x · · · x Ak where each Ai is cyclic of order pn; with n1 ~ n2 ~ · · · ~ nk > 0, then the integers n1, n2 , .•• , nk are called the invariants of G. Just because we called the integers above the invariants of G does not ,mean that they are really the invariants of G. That is, it is possible that we ,can assign different sets of invariants to G. We shall soon show that the 'invariants of G are indeed unique and completely describe G. Note one other thing about the invariants of G. If G = A1 x · · · x Ak, here Ai is cyclic of order pn1 , n1 ~ n2 ~ • • • ~ nk > 0, then o(G) = (A 1 )o(A 2 ) • • • o(Ak), hence pn = pntpn2 • • • pnk = pn 1 +n2 + · · · +n\\ whence n = 1 + n2 + · · · + nk. In other words, n1, n2 , ••. , nk give us a partition of n. e have already run into this concept earlier in studying the conjugate lasses in the symmetric group. Before discussing the uniqueness of the invariants of G, one thing should e made absolutely clear: the elements a 1 , ... , ak and the subgroups 1, ••. , Ak which they generate, which arose above to give the decom- osition of G into a direct product of cyclic groups, are not unique. Let~ see this in a very simple example. Let G = {e, a, b, ab} be an abelian group of order 4 where a2 = b2 = e, ab = ba. Then G == A x B where 'A = (a), B = (b) are cyclic groups of order 2. But we have another decomposition of G as a direct product, namely, G = C x B where = (ab) and B = (b)o So, even in this group of very small order, we can et distinct decompositions of the group as the direct product of cyclic oups. Our claim-which we now want to substantiate-is that while ese cyclic subgroups are not unique, their orders are EFINITION If G is an abelian group and sis any integer, then G(s) x E G 1 ~ = e}. Because G is abelian it is evident that G(s) is a subgroup of G. We now .if G and G' are isomorphic abelian groups, then for every fter s, G(s), and G'(s) are isomorphic. 112 Group Theory Ch. 2 Proof. Let 4> be an isomorphism of G onto G'. We claim that </> maps G (s) isomorphically onto G' (s). First we show that </>( G (s)) c G' (s). For, if x E G (s) then X5 = e, hence <f>(x 5 ) = <f>(e) = e'. But <f>(x 5 ) = <f>(x) 5 ; hence <f>(x)5 = e' and so <f>(x) is in G'(s). Thus <f>(G(s)) c G'(s). On the other hand, if u' E G'(s) then (u') 5 = e'. But, since 4> is onto, u' = <f>(y) for some y E G. Therefore e' = (u') 5 = <f>(y)s = </>(f). Be- cause 4> is one-to-one, we haveys = e and soy E G(s). Thus 4> maps G(s) onto G' (s). Therefore since 4> is one-to-one, onto, and a homomorphism from G (s) to G'(s), we have that G(s) and G'(s) are isomorphic. We continue with LEMMA 2.14.2 Let G be an abelian group of order pn, p a prime. Suppose that G = A 1 X A 2 X • • • X Ak, where each Ai = (aJ is cyclic of order pn1 , and n1 ~ n 2 ~ • • • ~ nk > 0. If m is an integer such that n1 > m ~ n1 + 1 then G(pm) = B 1 x · · · x B1 x At+l x · · · x Ak where Bi is cyclic of order pm, generated by a/\"1 - m, for i ~ t. The order of G (pm) is p\", where k u = mt + L ni. i=t+l Proof. First of all, we claim that At+l' ... , Ak are all in G(pm). For, since m ~ nt+l ~ · · · ~ nk > 0, if j ~ t + 1, afm = (af\"J)Pm-ni = e. Hence Aj, for j ~ t + 1 lies in G (pm). Secondly, if i ~ t then ni > m and (a/\"1 -m)pm = a/\" 1 = e, whence each such a/\"1 -m is in G(pm) and so the subgroup it generates, Bi, is also in G(pm). Since B 1 , .•• , Bt, At+ v ... , Ak are all in G (pm), their product (which is direct, since the product A 1A2 • • • Ak is direct) is in G (pm). Hence G(pm) ~ B 1 X • • • X B 1 X At+l X • • • X Ak. On the other hand, if x = a/ 1a/ 2 • • • a/k is in G (pm), since it then satisfies xPm = e, we set e = xPm = a/ 1Pm · · · a/kPm. However, the product of the subgroups A1 , .•. , Ak is direct, so we get Thus the order of ai, that is, pn; must divide ).ipm for i = 1, 2, ... , k. If i ~ t + 1 this is automatically true whatever be the choice of At+ 1, ... , },k since m ~ nt+l ~ · · · ~ nk, hence pn; I pm, i ~ t + 1. However, for i ~ t, we get from pn; I ).ipm that pn;-m I ).i· Therefore ),i = vipn;-m for some integer vi. Putting all this information into the values of the J../s in the expression for x as x = a/ 1 • • • a/k we see that I' I. Sec. 2.14 Finite Abelian Groups 113 This says that x E B 1 x · · · x Bt x At+ 1 x · · · x Ak. Now since each Bi is of order pm and since o(Ai) = pn; and since G = B1 X ••• X Bt X At+1 X ••• ~ Ak, o(G) = o(B1)o(B2 ) • • • o(Bt)o(At+ 1) · · · o(Ak) = pmpm. · ·pmpnt+t .. ·pnk . Thus, ifwe write o(G) = pu, then The lemma is proved. k u = mt + L ni. i=t+1 ._-v---\" t-times COROLLARY !JG is as in Lemma 2.14.2, then o(G(p)) = pk. Proof. Apply the lemma to the case m = 1. Then t = k, hence u = 1k = k and so o(G) = pk. We now have all the pieces required to prove the uniqueness of the invariants of an abelian group of order pn. THEOREM 2.14.2 Two abelian groups of order pn are isomorphic if and only if they have the same invariants. In other words, if G and G' are abelian groups of order pn and G = A 1 X • • • x Ak, where each Ai is a cyclic group of order pn;, n1 ~ · · · ~ nk > 0, and G' = B; x · · · x B~, where each B~ is a cyclic group of order ph;, h1 ~ · · · ~ hs > 0, then G and G' are isomorphic if and only if k = s andfor each i, ni = hi. Proof. One way is very easy, namely, if G and G' have the same ... in- variants then they are isomorphic. For then G = A 1 x · · · x Ak where Ai = (ai) is cyclic of order pn;, and G' = B~ X • • • X B~ where B~ = (bD is cyclic of order pn1• Map G onto G' by the map cp(a 1a 1 • • • atk) = (h~t 1 • • • (b£)ak. We leave it to the reader to verify that this defines an isomorphism of G onto G'. Now for the other direction. Suppose that G = A 1 x · · · x Ak, G' = B~ x · · · x B~, Ai, B; as described above, cyclic of orders pn1 , ph1, respectively, where n1 ~ · · · ~ nk > 0 and h1 ~ · · · ~ hs > 0. We · Want to show that if G and G' are isomorphic then k = s and each ni = hi. If G and G' are isomorphic then, by Lemma 2.14.1, G(pm) and G'(pm) must be isomorphic for any integer m ~ 0, hence must have the same order. Let's see what this gives us in the special case m = 1 ; that is, what in- formation can we garner from o(G(p)) = o(G'(p)). According to the corollary to Lemma 2.14.2, o(G(p)) = pk and o(G'(p)) = ps. Hence pk = ps and so k = s. At least we now know that the number of invariants for G and G' is the same. 14 Group Theory Ch. 2 If ni =f:. hi for some i, let t be the first i such that nt =f:. ht; we may sup- pose that nt > ht. Let m = ht. Consider the subgroups, H = {xPm I x E G} and H' = {(x')Pm I x' E G}, of G and G', respectively. Since G and G' are isomorphic, it follows easily that H and H' are isomorphic. We now ex- amine the invariants of Hand H'. Because G = A 1 X • • • X Ak, where Ai = (ai) is of order pn1, we get that H = C1 x · · · x ct x · · · x c,, where ci = (atm) is of order pn1 -m, and where r is such that n, > m = ht ~ n,_ 1. Thus the invariants of H are n1 - m, n2 - m, ... , n, - m and the number of invariants of His r ~ t. Because G' = B~ x · · · x B~, where Bi = (hi) is cyclic of order ph 1 , we get that H' = D~ x · · · x n;_ 1, where D~ = ((bi)Pm) is cyclic of order ph•-m. Thus the invariants of H' are h1 - m, ... , ht_ 1 - m and so the number of invariants of H' is t - 1. But H and H' are isomorphic; as we saw above this forces them to have the same number of invariants. But we saw that assuming that ni =f:. hi for some i led to a discrepancy in the number of their invariants. In con- sequence each ni = hi, and the theorem is proved. An immediate consequence of this last theorem is that an abelian group of order pn can be decomposed in only one way-as far as the orders of the cyclic subgroups is concerned-as a direct product of cyclic subgroups. Hence the invariants are indeed the invariants of G and completely determine G. If n1 ~ • • • ~ nk > 0, n = n1 + · · · + nk, is any partition of n, then we can easily construct an abelian group of order pn whose invariants are n1 ~ • • • ~ nk > 0. To do this, let Ai be a cyclic group of order pnt and let G = A 1 x · · · x Ak be the external direct product of A 1, .•• , Ak. Then, by the very definition, the invariants of G are n1 ~ • • • ~ nk > 0. Finally, two different partitions of n give rise to nonisomorphic abelian groups of order pn. This, too, comes from Theorem 2.14.2. Hence we have THEOREM 2.14.3 The number of nonisomorphic abelian groups of order p\", p a prime, equals the number of partitions of n. Note that the answer given in Theorem 2.14.3 does not depend on the prime p; it only depends on the exponent n. Hence, for instance, the number of nonisomorphic abelian groups of order 2 4 equals that of orders 3 4, or 54, etc. Since there are five partitions of 4, namely: 4 = 4, 3 + I, 2 + 2, 2 + I + I, I + I + I + I, then there are five nonisomorphic abelian groups of order p 4 for any prime p. Since any finite abelian group is a direct product of its Sylow subgroups, and two abelian groups are isomorphic if and only if their corresponding Sylow subgroups are isomorphic, we have the Sec. 2.14 Finite Abelian Groups 115 COROLLARY The number of nonisomorphic abelian groups of order p1a 1 • • ·p/r, where the Pi are distinct primes and where each rti > 0, is p(rt 1)p(rt2 ) • • · p(rt,), where p(u) denotes the number of partitions of u. Problems I. If G is an abelian group of order pn, p a prime and n1 ~ n2 ~ • • • ~ nk > 0, are the invariants of G, show that the maximal order of any element in G is pn 1 • 2. If G is a group, Av ... , Ak normal subgroups of G such that Ai n (A 1A 2 • • • Ai_ 1) = (e) for all i, show that G is the direct product of A 1 , ... , Ak if G = A 1A 2 • • • Ak. 3. Using Theorem 2.14.1, prove that if a finite abelian group has sub- groups of orders m and n, then it has a subgroup whose order is the least common multiple of m and n. 4. Describe all finite abelian groups of order (a) 26 • (b) 11 6 . (c) 7 5 . (d) 2 4 • 3 4 • 5. Show how to get all abelian groups of order 2 3 • 3 4 • 5. 6. If G is an abelian group of order pn with invariants n1 ~ • • • ~ nk > 0 and H =I= (e) is a subgroup of G, show that if h.1 ~ • • • ~ hs > 0 are the invariants of H, then k ~ sand for each i, hi ::; ni fori = 1, 2, ... , s. If G is an abelian group, let G be the set of all homomorphisms of G into the group of nonzero complex numbers under multiplication. If¢ 1 , ¢ 2 E G, define ¢ 1 • ¢ 2 by (¢ 1 • ¢ 2)(g) = ¢ 1 (g)¢2(g) for allg E G. 7. Show that G is an abelian group under the operation defined. 8. If 4> E G and G is finite, show that ¢(g) is a root of unity for every gE G. 9. If G is a finite cyclic group, show that G is cyclic and o(G) = o(G), hence G and G are isomorphic. 10. If g1 =1= g2 are in G, G a finite abelian group, prove that there is a 4> E G with ¢(g1) =I= cf>(g2) · II. If G is a finite abelian group prove that o(G) = o(G) and G is iso- morphic to G. 12. If 4> =I= 1 E G where G is an abelian group, show that L: cf>(g) = 0. geG Supplementary Problems There is no relation between the order in which the problems appear and the order of appearance of the sections, in this chapter, which might be relevant to their solutions. No hint is given regarding the difficulty of any problem. 16 Group Theory Ch. 2 1. (a) If G is a finite abelian group with elements a1 , a2 , ••• , an, prove that a 1 a2 • • ·an is an element whose square is the identity. (b) If the Gin part (a) has no element of order 2 or more than one element of order 2, prove that a1 a2 • • • an = e. (c) If G has one element, y, of order 2, prove that a 1 a2 • • • an = y. (d) (Wilson's theorem) If pis a prime number show that (p - 1)! = -1 (p). 2. If p is an odd prime and if 1 1 1 a 1 + - + - + ... + -- = -b, 2 3 p- 1 where a and b are integers, prove that p I a. If p > 3, prove that p21 a. 3. If p is an odd prime, a ¢ 0 (p) is said to be a quadratic residue of p if there exists an integer x such that x 2 = a(p). Prove (a) The quadratic residues of p form a subgroup Q of the group of nonzero integers mod p under multiplication. (b) o(Q) = (p - 1)/2. (c) If q E Q, n ¢ Q (n is called a nonresidue), then nq is a nonresidue. (d) If n1, n2 are nonresidues, then n1n2 is a residue. (e) If a is a quadratic residue of p, then a<P- 1 >12 = + 1 (p). 4. Prove that in the integers mod p, p a prime, there are at most n solutions of xn = 1 (p) for every integer n. 5. Prove that the nonzero integers mod p under multiplication form a cyclic group if pis a prime. 6. Give an example of a non-abelian group in which (xy) 3 = x 3y 3 for all x andy. 7. If G is a finite abelian group, prove that the number of solutions of xn = e in G, where n I o(G) is a multiple of n. 8. Same as Problem 7, but do not assume the group to be abelian. 9. Find all automorphisms of S3 and S4 , the symmetric groups of degree 3 and 4. DEFINITION A group G is said to be solvable if there exist subgroups G == N0 :::::> N 1 :::::> N2 :::::> • • • :::::> Nr = (e) such that each Ni is normal in Ni_ 1 and Ni _ 1 / Ni is abelian. 10. Prove that a subgroup of a solvable group and the homomorphic image of a solvable group must be solvable. 11. If G is a group and N is a normal subgroup of G such that both N and G/ N are solvable, prove that G is solvable. 12. If G is a group, A a subgroup of G and N a normal subgroup of G, prove that if both A and N are solvable then so is AN. Sec. 2.14 Finite Abelian Groups 117 13. If G is a group, define the sequence of subgroups G(i) of G by ( 1) G< 1) = commutator subgroup of G = subgroup of G generated by all aba- 1b- 1 where a, bE G. (2) G<i) = commutator subgroup of G(i- 1> if i > 1. Prove (a) Each G(i) is a normal subgroup of G. (b) G is solvable if and only if G(k) = (e) for some k ~ 1. 14. Prove that a solvable group always has an abelian normal subgroup M =1- (e). If G is a group, define the sequence of subgroups G(i) by (a) G(t) = commutator subgroup of G. (b) G(i) = subgroup of G generated by all aba- 1b- 1 where a E G, bE G(i-1)• G is said to be nilpotent if G(k) = (e) for some k ~ 1. 15. (a) Show that each G(i) is a normal subgroup of G and G(i) ::::> G(i). (b) If G is nilpotent, prove it must be solvable. (c) Give an example of a group which is solvable but not nilpotent. 16. Show that any subgroup and homomorphic image of a nilpotent group must be nilpotent. 17. Show that every homomorphic image, different from (e), of a nil- potent group has a nontrivial center. 18. (a) Show that any group of order pn, p a prime, must be nilpotent. (b) If G is nilpotent, and H =1- G is a subgroup of G, prove that N(H) =1- H where N(H) = {x E G I xHx- 1 = H}. 19. If G is a finite group, prove that G is nilpotent if and only if G is the direct product of its Sylow subgroups. 20. Let G be a finite group and H a subgroup of G. For A, B subgroups of G, define A to be conjugate to B relative to H if B = x- 1Ax for some x E H. Prove (a) This defines an equivalence relation on the set of subgroups of G. (b) The number of subgroups of G conjugate to A relative to H equals the index of N(A) n H in H. 21. (a) If G is a finite group and if P is a p-Sylow subgroup of G, prove that Pis the only p-Sylow subgroup in N(P). (b) If Pis a p-Sylow subgroup of G and if aPk = e then, if a E N(P), a must be in P. (c) Prove that N(N(P)) = N(P). 22. (a) If G is a finite group and P is a p-Sylow subgroup of G, prove that the number of conjugates of P in G is not a multiple of p. 18 Group Theory Ch. 2 (b) Breaking up the conjugate class of P further by using conjugacy relative to P, prove that the conjugate class of P has 1 + kp distinct subgroups. (Hint: Use part (b) of Problem 20 and Problem 21. Note that together with Problem 23 this gives an alternative proof of Theorem 2.12.3, the third part of Sylow's theorem.) 23. (a) If Pis a p-Sylow subgroup of G and B is a subgroup of G of order pk, prove that if B is not contained in some conjugate of P, then the number of conjugates of P in G is a multiple of p. (b) Using part (a) and Problem 22, prove that B must be contained in some conjugate of P. (c) Prove that any two p-Sylow subgroups of G are conjugate in G. (This gives another proof of Theorem 2.12.2, the second part of Sylow's theorem.) 24. Combine Problems 22 and 23 to give another proof of all parts of Sylow's theorem. 25. Making a case-by-case discussion using the results developed in this chapter, prove that any group of order less than 60 either is of prime order or has a nontrivial normal subgroup. 26. Using the result of Problem 25, prove that any group of order less than 60 is solvable. 27. Show that the equation x 2ax = a- 1 is solvable for x in the group G if and only if a is the cube of some element in G. 28. Prove that (I 2 3) is not a cube of any element in Sn. 29. Prove that xax = b is solvable for x in G if and only if ab is the square of some element in G. 30. If G is a group and a E G is of finite order and has only a finite number of conjugates in G, prove that these conjugates of a generate a finite normal subgroup of G. 31. Show that a group cannot be written as the set-theoretic union of two proper subgroups. 32. Show that a group G is the set-theoretic union of three proper sub- groups if and only if G has, as a homomorphic image, a noncyclic group of order 4. #33. Let p be a prime and let ZP be the integers mod p under addition and multiplication. Let G be the group where a, b, c, d E ZP (ca db) are such that ad - be = 1. Let and let LF(2, p) = GJC. Sec. 2.14 Finite Abelian Groups 119 (a) Find the order of LF(2,p). (b) Prove that LF(2,p) is simple ifp ~ 5. #34. Prove that LF(2, 5) is isomorphic to A 5 , the alternating group of degree 5. #35. Let G = LF(2, 7); according to Problem 33, G is a simple group of order 168. Determine exactly how many 2-Sylow, 3-Sylow, and 7-Sylow subgroups there are in G. Supplementary Reading BuRNSIDE, W., Theory of Groups of Finite Order, 2nd ed. Cambridge, England: Cambridge University Press, 1911; New York: Dover Publications, 1955. HALL, MARSHALL, Theory of Groups. New York: The Macmillan Company, 1961. Topics for Class Discussion ALPERIN, J. L., \"A classification of n-abelian groups,\" Canadian Journal of Math- ematics, Vol. XXI (1969), pages 1238-1244. McKAY, jAMES, H., \"Another proof of Cauchy's group theorem,\" American Math- ematical Monthly, Vol. 66 (1959), page 119. , SEGAL, I. E., \"The automorphisms of the symmetric group,\" Bulletin of the American Mathematical Society, Vol. 46 (1940), page 565. 3 Ring Theory 3.1 Definition and Examples of Rings As we indicated in Chapter 2, there are certain algebraic systems which serve as the building blocks for the structures comprising the subject which is today called modern algebra. At this stage of the development we have learned something about one of these, namely groups. It is our purpose now to introduce and to study a second such, namely rings. The abstract concept of a group has its origins in the set of mappings, or permutations, of a set onto itself. In con- trast, rings stem from another and more familiar source, the set of integers. We shall see that they are patterned after, and are gen- eralizations of, the algebraic aspects of the ordinary integers. In the next paragraph it will become clear that a ring is quite different from a group in that it is a two-operational system; these operations are usually called addition and multiplication. Yet, despite the differences, the analysis of rings will follow the pattern already laid out for groups. We shall require the appropriate analogs of homomorphism, normal subgroups, factor groups, etc. With the experience gained in our study of groups we shall be able to make the requisite definitions, intertwine them with meaningful theorems, and end up proving results which are both interesting and important about mathematical objects with which we have had long acquaintance. To cite merely one instance, later on in the book, using the tools developed here, we shall prove that it is impossible to trisect an angle of 60° using only a straight-edge and compass. 120 Sec. 3.1 Definition and Examples of Rings DEFINITION A nonempty set R is said to be an associative ring if in R there are defined two operations, denoted by + and · respectively, such that for all a, b, c in R: 1. a + b is in R. 2. a + b = b + a. 3. (a + b) + c = a + (b + c). 4. There is an element 0 in R such that a + 0 = a (for every a in R). 5. There exists an element -a in R such that a + (-a) = 0. 6. a · b is in R. 7. a· (b ·c) = (a· b)· c. 8. a· (b + c) = a· b + a· c and (b + c) ·a = b ·a + c ·a (the two distrib- utive laws). Axioms 1 through 5 merely state that R is an abelian group under the operation +,which we call addition. Axioms 6 and 7 insist that R be closed under an associative operation ·, which we call multiplication. Axiom 8 serves to interrelate the two operations of R. Whenever we speak of ring it will be understood we mean associative ring. Nonassociative rings, that is, those in which axiom 7 may fail to hold, do occur in mathematics and are studied, but we shall have no occasion to consider them. It may very well happen, or not happen, that there is an element 1 in R such that a ·1 = 1 ·a = a for every a in R; if there is such we shall describe R as a ring with unit element. If the multiplication of R is such that a · b = b · a for every a, b in R, then we call R a commutative ring. Before going on to work out some properties of rings, we pause to exarnine some examples. Motivated by these examples we shall define various special types of rings which are of importance. Example 3.1.1 R is the set of integers, positive, negative, and 0; + is the usual addition and · the usual multiplication of integers. R is a com- mutative ring with unit element. Example 3.1 .2 R is the set of even integers under the usual operations of addition and multiplication. R is a commutative ring but has no unit element. Example 3.1 .3 R is the set of rational numbers under the usual addition and multiplication of rational numbers. R is a commutative ring with unit element. But even more than that, note that the elements of R different from 0 form an abelian group under multiplication. A ring with this latter property is called afield. 121 122 Ring Theory Ch. 3 Example 3.1 .4 R is the set of integers mod 7 under the addition and multiplication mod 7. That is, the elements of R are the seven symbols 0, T, 2, 3, 4, 5, 6, where l. 'i\" +] = \"k where k is the remainder of i +Jon division by 7 (thus, for instance, 4 + 5 = 2 since 4 + 5 = 9, which, when divided by 7, leaves a remainder of 2). 2. \"i ·] = m where m is the remainder of iJ on division by 7 (thus, 5 · 3 = 1 since 5 · 3 = 15 has 1 as a remainder on division by 7). The student should verify that R is a commutative ring with unit element. However, much more can be shown; namely, since f. T = T = 6 · 6, 2·4 = 1 = 4·2, 3 · 5 = I = 5 · 3, the nonzero elements of R form an abelian group under multiplication. R is thus a field. Since it only has a finite number of elements it is called a finite field. Example 3.1.5 R is the set of integers mod 6 under addition and multiplication mod 6. If we denote the elements in R by 0, T, 2, ... , 5, one sees that 2 · 3 = 0, yet 2 # 0 and 3 # 0. Thus it is possible in a ring R that a · b = 0 with neither a = 0 nor b = 0. This cannot happen in a field (see Problem 10, end of Section 3.2), thus the ring R in this example is certainly not a field. Every example given so far has been a commutative ring. We now present a noncommutative ring. Example 3.1.6 R will be the set of all symbols 2 ocueu + ocueu + oc21e21 + OC22e22 = L ociieii' i,j= 1 where all the ocii are rational numbers and where we decree 2 2 L ociieii = L f3iieii i,j= 1 i,j= 1 ifandonlyifforalli,J = 1,2, ocii = f3ii, 2 2 2 L ociieii + L f3iieii = L (ocii + f3ii)eii' i,j= 1 i,j= 1 i,j= 1 (1) (2) (3) Sec. 3.1 Definition and Examples of Rings where 2 Yii = L aivf3vj = ai1{31j + ai2{32j• v=1 This multiplication, when first seen, looks rather complicated. However, it is founded on relatively simple rules, namely, multiply 'Lai/ii by 'Lf3iieii formally, multiplying out term by term, and collecting terms, and using the relations eii · ek1 = 0 for j =I= k, eii · ei1 = eu in this term-by-term collecting. (Of course those of the readers who have already encountered some linear algebra will recognize this example as the ring of all 2 x 2 matrices over the field of rational num hers.) To illustrate the multiplication, if a e22 + 3e12 , then a· b = (e11 - e21 + e22) · (e22 + 3e12) = eu ·e22 + 3eu ·e12 - e21 ·e22 - 3e21 ·e12 + e22 ·e22 + 3e22 ·e12 = 0 + 3e12 - 0 - 3e22 + e22 + 0 = 3e12 - 3e22 + e22 = 3e12 - 2e22 • Note that e11 • e12 = e12 whereas e12 · e11 = 0. Thus the multiplication in R is not commutative. Also it is possible for u · v = 0 with u =1= 0 and v =I= 0. The student should verify that R is indeed a ring. It is called the ring of 2 x 2 rational matrices. It, and its relative, will occupy a good deal of our time later on in the book. Example 3.1.7 Let C be the set of all symbols (a, {3) where a, {3 are real numbers. We define (a, {3) = (y, <5) if and only if a = y and {3 = <5. (1) In C we introduce an addition by defining for x = (a, {3), y = (y, <5) x + y = (a, {3) + (y, <5) = (a + y, {3 + <5). (2) Note that x + y is again in C. We assert that Cis an abelian group under this operation with (0, 0) serving as the identity element for addition, and (-a, - {3) as the inverse, under addition, of (a, {3). Now that Cis endowed with an addition, in order to make of C a ring we still need a multiplication. We achieve this by defining for X = (a, {3), Y = (y, <5) in C, X· Y = (a, {3) · (y, <5) = (ay - {3<5, ab + {3y). (3) 123 124 Ring Theory Ch. 3 Note that X· Y = Y·X. Also X· (1, 0) = (1, 0) ·X= X so that (1, 0) is a unit element for C. Again we notice that X· Y E C. Also, if X = (oc, p) =I= (0, 0) then, since oc, p are real and not both 0, oc2 + P 2 =I= 0; thus is in C. Finally we see that All in all we have shown that C is a field. If we write (oc, p) as oc + pi, the reader may verify that C is merely a disguised form of the familiar complex numbers. Example 3.1.8 This last example is often called the ring of real quaternions. This ring was first described by the Irish mathematician Hamilton. Initially it was extensively used in the study of mechanics; today its primary interest is that of an important example, although it still plays key roles in geometry and number theory. Let Q be the set of all symbols oc0 + oc1 i + oc2j + oc3k, where all the numbers oc0 , au oc2, and oc3 are real numbers. We declare two such symbols, oc0 + oc1 i + oc2j + oc3k and Po + P1 i + P2j + {33k, to be equal if and only if oct = Pt for t = 0, 1, 2, 3. In order to make Q into a ring we must de- fine a + and a · for its elements. To this end we define 1. For any X = oc0 + oc1 i + oc2j + oc3k, Y = {30 + P1 i + P2 j + P3k in Q, X + Y = (oc0 + oc1 i + oc2j + oc3k) + (Po + P1 i + P2j + {33k) = (oco + Po) + (oc1 + fJ1)i + (oc2 + P2)j + (oc3 + P3)k and 2. X· Y = (oc0 + oc1i + oc2j + oc3k) · (Po + P1i + P2j + P3k) = (ocoPo - oc1P1 - oc2fJ2 - oc3{33) + (ocoP1 + oc1Po + oc2{33 - oc3f3z)i + (ocoP2 + oc2Po + oc3P1 - oc1P3)j + (ocoP3 + oc3Po + oc1P2 - OCzf3l)k. Admittedly this formula for the product seems rather formidable; however, it looks much more complicated than it actually is. It comes from multi- plying out two such symbols formally and collecting terms using the relations i2 =j 2 = k2 = ijk = -1, ij = -ji = k,jk = -kj = i, ki = -ik =J. The latter part of these relations, called the multiplication table of the quaternion units, can be remembered by the little diagram on page 125. As you go around clockwise you read off the product, e.g., ij = k~ jk = i, ki = j; while going around counterclockwise you read off the negatives. , 1 Sec. 3.2 Some Special Classes of Rings Notice that the elements ±I, ±i, ±J, ±k form a non-abelian group of order 8 under this product. In fact, this is the group we called the group of quaternion units in Chapter 2. The reader may prove that Q is a noncommutative ring in which 0 0 + Oi + Oj + Ok and I = 1 + Oi + Oj + Ok serve as the zero and unit elements respectively. Now if X = cx0 + cx1 i + cx2 j + cx3k is not 0, then not all of cx0 , CXv cx2, cx3 are 0; since they are real, {3 = cx0 2 + cx1 2 + cx2 2 + cx3 2 =f:. 0 follows. Thus Y __ <Xo CX1 • CX2 • CX3 k Q --Z--)-- E. {3 {3 {3 {3 A simple computation now shows that X· Y = I. Thus the nonzero elements of Q form a non-abelian group under multiplication. A ring in which the nonzero elements form a group is called a division ring or skew- field. Of course, a commutative division ring is a field. Q affords us a division ring which is not a field. Many other examples of noncommutative division rings exist, but we would be going too far afield to present one Bere. The investigation of the nature of division rings and the attempts to classify them form an important part of algebra. 3.2 Some Special Classes of Rings The examples just discussed in Section 3.1 point out clearly that although rings are a direct generalization of the integers, certain arithmetic facts to which we have become accustomed in the ring of integers need not hold in general rings. For instance, we have seen the possibility of a· b = 0 with neither a nor b being zero. Natural examples exist where a· b =I= b ·a. All these run counter to our experience heretofore. For simplicity of notation we shall henceforth drop the dot in a · b and merely write this product as ab. DEFINITION If R is a commutative ring, then a =I= 0 E R is said to be a zero-divisor if there exists a b e R, b =f:. 0, such that ab = 0. 126 Ring Theory Ch. 3 DEFINITION A commutative ring is an integral domain if it has no zero- divisors. The ring of integers, naturally enough, IS an example of an integral domain. DEFINITION A ring is said to be a division ring if its nonzero elements form a group under multiplication. The unit element under multiplication will be written as 1, and the inverse of an element a under multiplication will be denoted by a- 1 • Finally we make the definition of the ultra-important object known as a field. DEFINITION Afield is a commutative division ring. In our examples in Section 3.1, we exhibited the noncommutative division ring of real quaternions and the following fields: the rational numbers, complex numbers, and the integers mod 7. Chapter 5 will con- cern itself with fields and their properties. We wish to be able to compute in rings in much the same manner in which we compute with real numbers, keeping in mind always that there are differences-it may happen that ab =P ba, or that one cannot divide. To this end we prove the next lemma, which asserts that certain things we should like to be true in rings are indeed true. LEMMA 3.2.1 If R is a ring, thenfor all a, bE R 1. aO = Oa = 0. 2. a ( - b) = ( -a) b = - ( ab). 3. (-a)( -b) = ab. IJ, in addition, R has a unit element 1, then 4. (-l)a =-a. 5. ( -1) ( -1) = 1. Proof. 1. If a E R, then aO = a(O + 0) = aO + aO (using the right distributive law), and since R is a group under addition, this equation implies that aO = 0. Similarly, Oa = (0 + O)a = Oa + Oa, using the left distributive law, and so here too, Oa = 0 follows. 2. In order to show that a(- b) = - (ab) we must demonstrate that ab + a( -b) = 0. But ab + a( -b) = a(b + (-b)) = aO = 0 by use of Sec. 3.2 Some Special Classes of Rings the distributive law and the result of part 1 of this lemma. Similarly ( - a) b = - ( ab) . 3. That (-a)( -b) = ab is really a special case of part 2; we single it out since its analog in the case of real numbers has been so stressed in our early education. So on with it: (-a)( -b) -(a(-b)) - (- (ab)) ab (by part 2) (by part 2) since - ( -x) = x is a consequence of the fact that in any group (u-1)-1 = u. 4. Suppose that R has a unit element 1; then a + ( -l)a = Ia + ( -l)a = (1 + ( -l))a = Oa = 0, whence ( -l)a = -a. In particular, if a = -1, ( -1)( -1) = - ( -1) = 1, which establishes part 5. With this lemma out of the way we shall, from now on, feel free to compute with negatives and 0 as we always have in the past. The result of Lemma 3.2.1 is our permit to do so. For convenience, a + (-b) will be written a- b. The lemma just proved, while it is very useful and important, is not very exciting. So let us proceed to results of greater interest. Before we do so, we enunciate a principle which, though completely trivial, provides a mighty weapon when wielded properly. This principle says no more or less than the following: if a postman distributes 101 letters to 100 mailboxes then some mailbox must receive at least two letters. It does not sound very promising as a tool, does it? Yet it will surprise us! Mathematical ideas can often be very difficult and obscure, but no such argument can be made against this very simple-minded principle given above. We formalize it ... ~nd even give it a name. THE_ PIGEONHOLE PRINCIPLE Ij n objects are distributed over m places, and if n > m, then some place receives at least two objects. An equivalent formulation, and one which we shall often use is: If n objects are distributed over n places in such a way that no place receives more than one object, then each place receives exactly one object. We immediately make use of this idea in proving LEMMA 3.2.2 Afinite integral domain is afield. Proof. As we may recall, an integral domain is a commutative ring such that ab = 0 if and only if at least one of a or b is itself 0. A field, on the other hand, is a commutative ring with unit element in which every non- zero element has a multiplicative inverse in the ring. 1~ I 128 Ring Theory Ch. 3 Let D be a finite integral domain. In order to prove that Dis a field we must 1. Produce an element 1 ED such that al = a for every a ED. 2. For every element a =I= 0 E D produce an element b E D such that ab = I. Let x1, x2 , ••• , xn be all the elements of D, and suppose that a =I= 0 ED. Consider the elements x1a, x2a, ... , xna; they are all in D. We claim that they are all distinct! For suppose that xia = xia for i=l= j; then (xi- xi)a = 0. Since D is an integral domain and a =I= 0, this forces xi - xi = 0, and so xi = xi, contradicting i =I= j. Thus x1 a, x2a, ... , xna are n distinct elements lying in D, which has exactly n elements. By the pigeonhole principle these must account for all the elements of D; stated otherwise, every elementy ED can be written as xia for some xi. In particular, since a ED, a = xi0 a for some xio ED. Since D is commutative, a =-= xi0a = axio· We propose to show that xio acts as a unit element for every element of D. For, if y E D, as we have seen, y = xia for some xi E D, and so yxio = (xia)xio = xi(axio) = xia = y. Thus xio is a unit element for D and we write it as 1. Now 1 E D, so by our previous argument, it too is realizable as a multiple of a; that is, there exists a b E D such that 1 = ba. The lemma is now completely proved. COROLLARY If p is a prime number then ]p, the ring of integers mod p, is a field. Proof. By the lemma it is enough to prove that ]p is an integral domain, since it only has a finite number of elements. If a, b E ]p and ab = 0, then p must divide the ordinary integer ab, and so p, being a prime, must divide a or b. But then either a = 0 mod p or b = 0 mod p, hence in ]p one of these is 0. The corollary above assures us that we can find an infinity of fields having a finite number of elements. Such fields are called finite fields. The fields ]p do not give all the examples of finite fields; there are others. In fact, in Section 7.1 we give a complete description of all finite fields. We point out a striking difference between finite fields and fields such as the rational numbers, real numbers, or complex numbers, with which we are more familiar. Let F be a finite field having q elements (if you wish, think of ]p with its p elements). Viewing F merely as a group under addition, sL1ce F has q elements, by Corollary 2 to Theorem 2.4.1, a + a + · · · + a = qa = 0 q-times Sec. 3.2 Some Special Classes of Rings 129 for any a E F. Thus, in F, we have qa = 0 for some positive integer q, even if a =I= 0. This certainly cannot happen in the field of rational numbers, for instance. We formalize this distinction in the definitions we give below. In these definitions, instead of talking just about fields, we choose to widen the scope a little and talk about integral domains. DEFINITION An integral domain D is said to be of characteristic 0 if the relation ma = 0, where a =I= 0 is in D, and where m is an integer, can hold only ifm = 0. The ring of integers is thus of characteristic 0, as are other familiar rings such as the even integers or the rationals. DEFINITION An integral domain D is said to be of finite characteristic if there exists a positive integer m such that ma = 0 for all a ED. If D is of finite characteristic, then we define the characteristic of D to be the smallest positive integer p such that pa = 0 for all a ED. It is not too hard to prove that if D is of finite characteristic, then its characteristic is a prime number (see Problem 6 below). As we pointed out, any finite field is of finite characteristic. However, an integral domain may very well be infinite yet be of finite characteristic (see Problem 7). One final remark on this question of characteristic: Why define it for integral domains, why not for arbitrary rings? The question is perfectly reasonable. Perhaps the example we give now points out what can happen ifwe drop the assumption \"integral domain.\" Let R be the set of all triples (a, b, c), where a E ] 2 , the integers mod 2, bE ] 3 , the integers mod 3, and c !s any integer. We introduce a + and a · to make of R a ring. We do so by defining (a1 , hu c1 ) + (a 2 , h2 , c2 ) = (a1 + a2 , b1 + b2 , c1 + c2 ) and (a1 , b1 , c1 ) • (a2 , b2 , c2 ) = (a 1a2 , b1b2 , c1c2 ). It is easy to verify that R is a commutative ring. It is not an integral domain since (1, 2, 0) · (0, 0, 7) = (0, 0, 0), the zero-element of R. Note that in R, 2(1, 0, 0) = (1, 0, 0) + (1, 0, 0) = (2, 0, 0) = (0, 0, 0) since addition in the first component is in ] 2 • Similarly 3(0, 1, 0) = (0, 0, 0). Finally, for no positive integer m is m(O, 0, 1) = (0, 0, 0). Thus, from the point of view of the definition we gave above for charac- teristic, the ring R, which we just looked at, is neither fish nor fowl. The definition just doesn't have any meaning for R. We could generalize the notion of characteristic to arbitrary rings by doing it locally, defining it relative to given elements, rather than globally for the ring itself. We say that R has n-torsion, n > 0, if there is an element a =1= 0 in R such that na = 0, and ma ::j:. 0 for 0 < m < n. For an integral domain D, it turns 130 Ring Theory Ch. 3 out that if D has n-torsion, even for one n > 0, then it must be of finite characteristic (see Problem 8). Problems R is a ring in all the problems. 1. If a, b, c, dE R, evaluate (a + b)(c + d). 2. Prove that if a, bE R, then (a + b) 2 = a2 + ab + ba + b2 , where by x 2 we mean xx. 3. Find the form of the binomial theorem in a general ring; in other words, find an expression for (a + ht, where n is a positive integer. 4. If every x E R satisfies x 2 = x, prove that R must be commutative. (A ring in which x 2 = x for all elements is called a Boolean ring.) 5. If R is a ring, merely considering it as an abelian group under its addition, we have defined, in Chapter 2, what is meant by na, where a E R andn is an integer. Prove that if a, bE Rand n, m are integers, then (na)(mb) = (nm)(ab). 6. If D is an integeral domain and D is of finite characteristic, prove that the characteristic of Dis a prime number. 7. Give an example of an integral domain which has an infinite number of elements, yet is of finite characteristic. 8. If D is an integral domain and if na = 0 for some a =/:. 0 in D and some integer n =/:. 0, prove that D is of finite characteristic. 9. If R is a system satisfying all the conditions for a ring with unit ele- ment with the possible exception of a + b = b + a, prove that the axiom a + b = b + a must hold in R and that R is thus a ring. (Hint: Expand (a + b)(l + 1) in two ways.) 10. Show that the commutative ring D is an integral domain if and only if for a, b, c ED with a =/:. 0 the relation ab = ac implies that b = c. 11. Prove that Lemma 3.2.2 is false if we drop the assumption that the integral domain is finite. 12. Prove that any field is an integral domain. 13. Useing the pigeonhole principle, prove that if m and n are relatively prime integers and a and b are any integers, there exists an integer x such that x = a mod m and x = b mod n. (Hint: Uonsider the re- mainders of a, a + m, a + 2m, ... , a + (n - l)m on division by n.) 14. Using the pigeonhole principle, prove that the decimal expansion of a rational number must, after some point, become repeating. Sec. 3.3 Homomorphisms 131 3.3 Homomorphisms In studying groups we have seen that the concept of a homomorphism turned out to be a fruitful one. This suggests that the appropriate analog for rings could also lead to important ideas. To recall, for groups a homo- morphism was defined as a mapping such that qy(ab) = ¢(a)¢(b). Since a ring has two operations, what could be a more natural extension of this type of formula than the DEFINITION A mapping¢ from the ring R into the ring R' is said to be a homomorphism if I. ¢(a + b) = ¢(a) + ¢(b), 2. ¢(ab) = ¢(a)¢(b), for all a, b E R. As in the case of groups, let us again stress here that the + and · occurring on the left-hand sides of the relations in 1 and 2 are those of R, whereas the + and · occurring on the right-hand sides are those of R'. A useful observation to make is that a homomorphism of one ring, R, into another, R', is, if we totally ignore the multiplications in both these rings, at least a homomorphism of R into R' when we consider them as abelian groups under their respective additions. Therefore, as far as addition is concerned, all the properties about homomorphisms of groups proved in Chapter 2 carry over. In particular, merely restating Lemma 2. 7.2 for the case of the additive group of a ring yields for us LEMMA 3.3.1 lf ¢is a homomorphism of R into R', then I. ¢(0) = 0. 2. ¢(~a) = -¢(a) for every a E R. A word of caution: if both R and R' have the respective unit elements and 1' for their multiplications it need not follow that ¢(1) = 1'. However, if R' is an integral domain, or if R' is arbitrary but¢ is onto, then c/>( 1) = 1' is indeed true. In the case of groups, given a homomorphism we associated with this homomorphism a certain subset of the group which we called the kernel of the homomorphism. What should the appropriate definition of the kernel of a homomorphism be for rings? After all, the ring has two operations, addition and multiplication, and it might be natural to ask which of these should be singled out as the basis for the definition. However, the choice is clear. Built into the definition of an arbitrary ring is the condition that the ring forms an abelian group under addition. The ring multiplication 132 Ring Theory Ch. 3 was left much more unrestricted, and so, in a sense, much less under our control than is the addition. For this reason the emphasis is given to the operation of addition in the ring, and we make the DEFINITION If</> is a homomorphism of R into R' then the kernel of</>, I(<f>), is the set of all elements a E R such that <f>(a) = 0, the zero-element of R'. LEMMA 3.3.2 If</> is a homomorphism of R into R' with kernel I(</>), then 1. I ( </>) is a subgroup of R under addition. 2. If a E I(</>) andrE R then both ar and ra are in I(<f>). Proof. Since </> is, in particular, a homomorphism of R, as an additive group, into R', as an additive group, ( 1) follows directly from our results in group theory. To see (2), suppose that a E I(</>), r E R. Then <f>(a) = 0 so that <f>(ar) = <f>(a)<f>(r) = O<f>(r) = 0 by Lemma 3.2.1. Similarly <f>(ra) = 0. Thus by defining property of I ( </>) both ar and ra are in I ( </>). Before proceeding we examine these concepts for certain examples. Example 3.3.1 Let Rand R' be two arbitrary rings and define <f>(a) = 0 for all a E R. Trivially </> is a homomorphism and I ( </>) = R. </> is called the zero-homomorphism. Example 3.3.2 Let R be a ring, R' = R and define <f>(x) = x for every x E R. Clearly </> is a homomorphism and I ( </>) consists only of 0. Example 3.3.3 Let 1( -J2) be all real numbers of the form m + n-J2 where m, n are integers; 1 ( -J2) forms a ring under the usual addition and multiplication of real numbers. (Verify!) Define <f>:1(-J2) -+1(-J2) by cP(m + n-J2) = m - n-J2. </> is a homomorphism of 1( -J2) onto 1( -J2) and its kernel I(</>), consists only of 0. (Verify!) Example 3.3.4 Let 1 be the ring of integers, 1n, the ring of integers modulo n. Define </> :1---+ 1n by <f>(a) = remainder of a on division by n. The student should verify that</> is a homomorphism of 1 onto 1n and that the kernel, I(<f>), of</> consists of all multiples of n. Example 3.3.5 Let R be the set of all continuous, real-valued functions on the closed unit interval. R is made into a ring by the usual addition and multiplication of functions; that it is a ring is a consequence of the fact that the sum and product of two continuous functions are continuous· Sec. 3.4 Ideals and Quotient Rings 133 functions. Let F be the ring of real numbers and define ¢ :R ~ F by t/J(f(x)) =J(t). ¢is then a homomorphism of R onto F and its kernel consists of all functions in R vanishing at x = t. All the examples given here have used commutative rings. Many beautiful examples exist where the rings are noncommutative but it would be premature to discuss such an example now. DEFINITION A homomorphism of R into R' is said to be an isomorphism if it is a one-to-one mapping. DEFINITION Two rings are said to be isomorphic if there is an isomorphism of one onto the other. The remarks made in Chapter 2 about the meaning of an isomorphism and of the statement that two groups are isomorphic carry over verbatim to rings. Likewise, the criterion given in Lemma 2. 7.4 that a homomorphism be an isomorphism translates directly from groups to rings in the form LEMMA 3.3.3 The homomorphism ¢ of R into R' is an isomorphism if and only if I(¢) = (0). 3.4 Ideals and Quotient Rings Once the idea of a homomorphism and its kernel have been set up for rings, based on our experience with groups, it should be fruitful to carry over some analog to rings of the concept of normal subgroup. Once this\" is achieved, one would hope that this analog would lead to a construction in rings like that of the quotient group of a group by a normal subgroup. Finally, if one were an optimist, one would hope that the homomorphism theorems for groups would come over in their entirety to rings. Fortunately all this can be done, thereby providing us with an incisive technique for analyzing rings. The first business at hand, then, seems to be to define a suitable \"normal subgroup\" concept for rings. With a little hindsight this is not difficult. If you recall, normal subgroups eventually turned out to be nothing else than kernels of homomorphisms, even though their primary defining conditions did not involve homomorphisms. Why not use this observation as the keystone to our definition for rings? Lemma 3.3.2 has already provided us with some conditions that a subset of a ring be the kernel of a homomorphism. We now take the point of view that, since no other in- formation is at present available to us, we shall make the conclusions of Lemma 3.3.2 as the starting point of our endeavor, and so we define 134 Ring Theory Ch. 3 DEFINITION A nonempty subset U of R is said to be a (two-sided) ideal of R if 1. U is a subgroup of R under addition. 2. For every u E U andrE R, both ur and ru are in U. Condition 2 asserts that U \"swallows up\" multiplication from the right and left by arbitrary ring elements. For this reason U is usually called a two-sided ideal. Since we shall have no occasion, other than in some of the problems, to use any other derivative concept of ideal, we shall merely use the word ideal, rather than two-sided ideal, in all that follows. Given an ideal U of a ring R, let Rf U be the set of all the distinct cosets of U in R which we obtain by considering U as a subgroup of R under addition. We note that we merely say coset, rather than right coset or left coset; this is justified since R is an abelian group under addition. To restate what we have just said, Rf U consists of all the cosets, a + U, where a E R. By the results of Chapter 2, Rf U is automatically a group under addition; this is achieved by the composition law (a+ U) + (b + U) =(a+ b) + U. In order to impose a ring structure on Rf U we must define, in it, a multi- plication. What is more natural than to define (a + U) (b + U) = ab + U? However, we must make sure that this is meaningful. Otherwise put, we are obliged to show that if a + U = a' + U and b + U = b' + U, then under our definition of the multiplication, (a + U) (b + U) = (a' + U) ( b' + U). Equivalently, it must be established that ab + U = a' b' + U. To this end we first note that since a + U = a' + U, a = a' + Uv where u1 E U; similarly b = b' + u2 where u2 E U. Thus ab = (a' + u1)(b + u2 ) = a'b' + u1b' + a'u2 + u1u2 ; since U is an ideal of R, u1 b' E U, a' u2 E U, and u1 u2 E U. Consequently u1 b' + a' u2 + u1 u2 = u3 E U. But then ab = a'b' + u3 , from which we deduce that ab + U = a' b' + u3 + U, and since u3 E U, u3 + U = U. The net consequence of all this is that ab + U = a' b' + U. We at least have achieved the principal step on the road to our goal, namely of introducing a well-defined multiplication. The rest now becomes routine. To establish that Rf U is a ring we merely have to go through the various axioms which define a ring and check whether they hold in Rf U. All these verifications have a certain sameness to them, so we pick one axiom, the right distributive law, and prove it holds in Rf U. The rest we leave to the student as informal exercises. If X= a + U, Y = b + U, Z = c + U are three elements of RfU, where a, b, c E R, then (X+ Y)Z = ((a + U) + (b + U))(e + U) = ((a + b) + U)(c + U) = (a + b)e + U = ae + be + U = (ae + U) + (be + U) = (a + U)( c + U) + ( b + U) ( e + U) = XZ + YZ. Rf U has now been made into a ring. Clearly, if R is commutative then so is RjU, for (a + U)(b + U) = ab + U = ba + U = (b + U)(a + U). (The converse to this is false.) If R has a unit element 1, then Rf U has a· Sec. 3.4 Ideals and Quotient Rings 135 unit element 1 + U. We might ask: In what relation is RJU toR? With the experience we now have in hand this is easy to answer. There is a homomorphism ¢ of R onto Rj U given by </J(a) = a + U for every a E R, whose kernel is exactly U. (The reader should verify that ¢ so defined is a homomorphism of R onto Rj U with kernel U.) We summarize these remarks in LEMMA 3.4.1 If U is an ideal of the ring R, then Rf U is a ring and is a homomorphic image of R. With this construction of the quotient ring of a ring by an ideal satisfactorily accomplished, we are ready to bring over to rings the homomorphism theorems of groups. Since the proof is an exact verbatim translation of that for groups into the language of rings we merely state the theorem without proof, referring the reader to Chapter 2 for the proof. THEOREM 3.4.1 Let R, R' be rings and¢ a homomorphism of R onto R' with kernel U. Then R' is isomorphic to Rf U. Moreover there is a one-to-one correspondence between the set of ideals of R' and the set of ideals of R which contain U. This correspondence can be achieved by associating with an ideal W' in R' the ideal Win R defined by W = {x E R I </J(x) E W'}. With W so defined, RfW is isomorphic to R'/W'. Problems 1. If U is an ideal of R and 1 E U, prove that U = R. 2. IfF is a field, prove its only ideals are (0) and F itself. 3. Prove that any homomorphism of a field is either an isomorphism or takes each element into 0. 4. If R is a commutative ring and a E R, (a) Show that aR = {ar I r E R} is a two-sided ideal of R. (b) Show by an example that this may be false if R is not commutative. 5. If U, V are ideals of R, let U + V = { u + v I u E U, v E V}. Prove that U + Vis also an ideal. 6. If U, V are ideals of R let UV be the set of all elements that can be written as finite sums of elements of the form uv where u E U and v E V. Prove that UV is an ideal of R. 7. In Problem 6 prove that UV c U n V. 8. If R is the ring of integers, let U be the ideal consisting of all multiples of 17. Prove that if Vis an ideal of R and R :::> V :::> U then either V = R or V = U. Generalize! 136 Ring Theory Ch. 3 9. If U is an ideal of R, let r(U) = {x E R I xu = 0 for all u E U}. Prove that r( U) is an ideal of R. 10. If U is an ideal of R let [R:UJ = {x E R I rx E U for every r E R}. Prove that [ R: UJ is an ideal of R and that it contains U. 11. Let R be a ring with unit element. Using its elements we define a ring R by defining a ffi b = a + b + 1, and a · b = ab + a + b, where a, b E R and where the addition and multiplication on the right-hand side of these relations are those of R. (a) Prove that R is a ring under the operations ffi and (b) What acts as the zero-element of R? (c) What acts as the unit-element of R? (d) Prove that R is isomorphic toR. * 12. In Example 3.1.6 we discussed the ring of rational 2 X 2 matrices. Prove that this ring has no ideals other than (0) and the ring itself. *13. In Example 3.1.8 we discussed the real quaternions. Using this as a model we define the quaternions over the integers mod p, p an odd prime number, in exactly the same way; however, now considering all symbols of the form cx0 + cx1 i + cx2j + cx3k, where cx0 , cx1 , cx2 , cx3 are integers mod p. (a) Prove that this is a ring with p 4 elements whose only ideals are (0) and the ring itself. **(b) Prove that this ring is not a division ring. If R is any ring a subset L of R is called a left-ideal of R if 1. L is a subgroup of R under addition. 2 r E R, a E L implies ra E L. (One can similarly define a right-ideal.) An ideal is thus simultaneously a left- and right-ideal of R. 14. For a E R let Ra = {xa I x E R}. Prove that Ra is a left-ideal of R. 15. Prove that the intersection of two left-ideals of R is a left-ideal of R. 16. What can you say about the intersection of a left-ideal and right-ideal of R? 17. If R is a ring and a E R let r(a) = {x E R I ax = 0}. Prove that r(a) is a right-ideal of R. 18. If R is a ring and Lis a left-ideal of R let ).(L) = {x E R I xa = 0 for all a E L}. Prove that A ( L) is a two-sided ideal of R. * 19. Let R be a ring in which x 3 = x for every x E R. Prove that R is a commutative ring. 20. If R is a ring with unit element 1 and l/J is a homomorphism of R onto R' prove that l/J(l) is the unit element of R'. Sec. 3.5 More Ideals and Quotient Rings 137 21. If R is a ring with unit element 1 and ¢ is a homomorphism of R into an integral domain R' such that I ( ¢) =I= R, prove that ¢( 1) is the unit element of R'. 3.5 More Ideals and Quotient Rings We continue the discussion of ideals and quotient rings. Let us take the point of view, for the moment at least, that a field is the most desirable kind of ring. Why? If for no other reason, we can divide in a field, so operations and results in a field more closely approximate our experience with real and complex numbers. In addition, as was illustrated by Problem 2 in the preceding problem set, a field has no homomorphic images other than itself or the trivial ring consisting of 0. Thus we cannot simplify a field by applying a homomorphism to it. Taking these remarks into consideration it is natural that we try to link a general ring, in some fashion, with fields. What should this linkage involve? We have a machinery whose component parts are homomorphisms, ideals, and quotient rings. With these we will forge the link. But first we must make precise the rather vague remarks of the preceding paragraph. We now ask the explicit question: Under what conditions is the homomorphic image of a ring a field? For commutative rings we give a complete answer in this section. Essential to treating this question is the converse to the result of Problem 2 of the problem list at the end of Section 3·.4. LEMMA 3.5.1 Let R be a commutative ring with unit element whose only ideals are (0) and R itself. Then R is a field. Proof. In order to effect a proof of this lemma for any a =I= 0 E R we must produce an element b =I= 0 E R such that ab = 1. So, suppose that a =1= 0 is in R. Consider the set Ra = {xa I x E R}. We claim that Ra is an ideal of R. In order to establish this as fact we must show that it is a subgroup of R under addition and that if u E Ra and r E R then ru is also in Ra. (We only need to check that ru is in Ra for then ur also is since ru = ur.) Now, if u, vERa, then u = r1a, v = r2a for some rv r2 E R. Thus u + v = r1a + r2a = (r1 + r2 )a ERa; similarly -u = -r 1a = ( -r 1)a ERa. Hence Ra is an additive subgroup of R. Moreover, if r E R, ru = r(r1a) = (rr1 )a E Ra. Ra therefore satisfies all the defining conditions for an ideal of R, hence is an ideal of R. (Notice that both the distributive law and associative law of multiplication were used in the proof of this fact.) By our assumptions on R, Ra = (0) or Ra = R. Since 0 =I= a = Ia ERa, Ra =I= (0); thus we are left with the only other possibility, namely that Ra = R. This last equation states that every element in R is a multiple of 138 Ring Theory Ch. 3 a by some element of R. In particular, 1 E R and so it can be realized as a multiple of a; that is, there exists an element b E R such that ba = 1. This completes the proof of the lemma. DEFINITION An ideal M =j:. R in a ring R is said to be a maximal ideal of R if whenever U is an ideal of R such that M c U c R, then either R = U orM = U. In other words, an ideal of R is a maximal ideal if it is impossible to squeeze an ideal between it and the full ring. Given a ring R there is no guarantee that it has any maximal ideals! If the ring has a unit element this can be proved, assuming a basic axiom of mathematics, the so-called axiom of choice. Also there may be many distinct maximal ideals in a ring R; this will be illustrated for us below in the ring of integers. As yet we have made acquaintance with very few rings. Only by con- sidering a given concept in many particular cases can one fully appreciate the concept and its motivation. Before proceeding we therefore examine some maximal ideals in two specific rings. When we come to the discussion of polynomial rings we shall exhibit there all the maximal ideals. Example 3.5.1 Let R be the ring of integers, and let U be an ideal of R. Since U is a subgroup of R under addition, from our results in group theory, we know that U consists of all the multiples of a fixed integer n0 ; we write this as U = (n0 ). What values of n0 lead to maximal ideals? We first assert that if p is a prime number then P = (p) is a maximal ideal of R. For if U is an ideal of R and U ::::> P, then U = (n0 ) for some integer n0 . Since p E P c U, p = mn0 for some integer m; because p is a prime this im:tJlies that n0 = 1 or n0 = p. If n0 = p, then P c U = (n0 ) c P, so that U = P follows; if n0 = 1, then 1 E U, hence r = lr E U for all r E R whence U = R follows. Thus no ideal, other than R or P itself, can be put between P and R, from which we deduce that Pis maximal. Suppose, on the other hand, that M = (n0 ) is a maximal ideal of R. We claim that n0 must be a prime number, for if n0 = ab, where a, b are positive integers, then U = (a) ::::> M, hence U = R or U = M. If U = R, then a = 1 is an easy consequence; if U = M, then a E M and so a = rn0 for some integer r, since every element of M is a multiple of n0 • But then n0 = ab = rn0 b, from which we get that rb = 1, so that b = 1, n0 = a. Thus n0 is a prime number. In this particular example the notion of maximal ideal comes alive-it corresponds exactly to the notion of prime number. One should not, however, jump to any hasty generalizations; this kind of correspondence does not usually hold for more general rings. I ' · .. l ~ Sec. 3.5 More Ideals and Quotient Rings 139 Example 3.5.2 Let R be the ring of all the real-valued, continuous functions on the closed unit interval. (See Example 3.3.5.) Let M = {f(x) E R lf(f) = 0}. is certainly an ideal of R. Moreover, it is a maximal ideal of R, for if the U contains M and U =I M, then there is a function g(x) E U, ¢ M. Since g(x) ¢ M, g(f) = C< =I 0. Now h(x) = g(x) - C< is such that h(f) = g(!) - C< = 0, so that h(x) EM c U. But g(x) is also in U; C< = g(x) - h(x) E U and so 1 = riC<- 1 E U. Thus for any function t(x) E R, t(x) = lt(x) E U, in consequence of which U = R. M is therefore a maximal ideal of R. Similarly if y is a real number 0 :::;; y:::;; 1, then MY = {f(x) E R lf(y) = 0} is a maximal ideal of R. It can be shown (see Problem 4 at the end of this section) that every maximal ideal is of this form. Thus here the maximal ideals correspond to the points on the unit interval. Having seen some maximal ideals in some concrete rings we are ready to continue the general development with THEOREM 3.5.1 If R is a commutative ring with unit element and M is an ideal of R, then M is a maximal ideal of R if and only if Rf M is a field. Proof. Suppose, first, that M is an ideal of R such that Rf M is a field. Since RfM is a field its only ideals are (0) and RfM itself. But by Theorem 3.4.1 there is a one-to-one correspondence between the set of ideals of RfM and the set of ideals of R which contain M. The ideal M of R corre- sponds to the ideal (0) of Rf M whereas the ideal R of R correspond;· to the ideal RfM of R/M in this one-to-one mapping. Thus there is no ideal between M and R other than these two, whence M is a maximal ideal. On the other hand, if M is a maximal ideal of R, by the correspondence mentioned above R/M has only (0) and itself as ideals. Furthermore RfM is commut~tive and has a unit element since R enjoys both these properties. All the conditions of Lemma 3.5.1 are fulfilled for R/ M so we can conclude, by the result of that lemma, that RfM is a field. We shall have many occasions to refer back to this result in our study of polynomial rings and in the theory of field extensions. Problems 1. Let R be a ring with unit element, R not necessarily commutative, such that the only right-ideals of R are (0) and R. Prove that R is a division ring. 140 Ring Theory Ch. 3 *2. Let R be a ring such that the only right ideals of R are (0) and R. Prove that either R is a division ring or that R is a ring with a prime number of elements in which ab = 0 for every a, b E R. 3. Let J be the ring of integers, p a prime number, and (p) the ideal of J consisting of all multiples of p. Prove (a) J/ (p) is isomorphic to J P' the ring of integers mod p. (b) Using Theorem 3.5.1 and part (a) of this problem, that ]p is a field. **4. Let R be the ring of all real-valued continuous functions on the closed unit interval. If M is a maximal ideal of R, prove that there exists a real number y, O~y~ 1, such that M=My={f(x)ERIJ(y) =0}. 3.6 The Field of Quotients of an Integral Domain Let us recall that an integral domain is a commutative ring D with the additional property that it has no zero-divisors, that is, if ab = 0 for some a, b E D then at least one of a or b must be 0. The ring of integers is, of course, a standard example of an integral domain. The ring of integers has the attractive feature that we can enlarge it to the set of rational numbers, which is a field. Can we perform a similar construction for any integral domain? We will now proceed to show that indeed we can ! DE FIN ITI 0 N A ring R can be imbedded in a ring R 1 if there is an isomorphism of R into R 1 • (If R and R 1 have unit elements 1 and 1 1 we insist, in addition, that this isomorphism takes 1 onto 11 .) R 1 will be called an over-ring or extension of R if R can be imbedded in R 1 • With this understanding of imbedding we prove THEOREM 3.6.1 Every integral domain can be imbedded in afield. Proof. Before becoming explicit in the details of the proof let us take an informal approach to the problem. Let D be our integral domain; roughly speaking the field we seek should be all quotients ajb, where a, bED and b =j:. 0. Of course in D, afb may very well be meaningless. What should we require of these symbols afb? Clearly we must have an answer to the following three questions: 1. When is afb = cfd? 2. What is (afb) + (c/d)? 3. What is (afb) (cfd)? In answer to 1, what could be more natural than to insist that afb = cfd Sec. 3.6 Field of Quotients of Integral Domain 141 if and only if ad = be? As for 2 and 3, why not try the obvious, that is, define ~ + !_ = ad + be and ~ !_ = ae . b d bd b d bd In fact in what is to follow we make these considerations our guide. So let us leave the heuristics and enter the domain of mathematics, with !llt· ...,,rpr•t<:P definitions and rigorous deductions. Let .A be the set of all ordered pairs (a, b) where a, b E D and b =1= 0. (Think of (a, b) as ajb.) In .A we now define a relation as follows: (a, b) \"' (e, d) if and only if ad= be. We claim that this defines an equivalence relation on .A. To establish this we check the three defining conditions for an equivalence relation for this particular relation. 1. If (a, b) E .A, then (a, b) ,...., (a, b) since ab = ba. 2. If (a, b), (e, d) E .A and (a, b) \"' (e, d), then ad = be, hence eb = da, and so (e, d) \"' (a, b). 3. If (a, b), (e, d), (e, f) are all in .A and (a, b) \"' (e, d) and (e, d) \"' (e,j), then ad = be and cf = de. Thus bcf = bde, and since be = ad, it follows that adf = bde. Since D is commutative, this relation becomes afd = bed; since, moreover, D is an integral domain and d =1= 0, this relation further implies that af = be. But then (a, b) \"' (e,f) and our relation is transitive. Let [a, b] be the equivalence class in .A of (a, b), and let F be the set of all such equivalence classes [a, b] where a, b E D and b =1= 0. F is the candidate for the field we are seeking. In order to create out ofF a fi~ld we must introduce an addition and a multiplication for its elements and then show that under these operations F forms a field. We first dispose of the addition. Motivated by our heuristic discussion at the beginning of the proof we define [a, b] -r [e, d] = [ad + be, bd]. Since D is an integral domain and both b =1= 0 and d =I= 0 we have that bd =I= 0; this, at least, tells us that [ad + be, bd] E F. We now assert that this addition is well defined, that is, if [a, b] = [a', b'] and [e, d] = [e', d'], then [a, b] + [e, d] = [a', b'] + [e', d']. To see that this is so, from [a, b] = [a', b'] we have that ab' = ba'; from [e, d] = [e', d'] we have that ed' = de'. What we need is that these relations force the equality of [a, b] + [e, d] and [a', b'] + [e', d']. From the definition of addition this boils down to showing that [ad+ be, bd] = [a'd' + b'e', b'd'], or, in equiva- lent terms, that (ad+ be)b'd' = bd(a'd' + b'e'). Using ab' = ba', ed' =de' 142 Ring Theory Ch. 3 this becomes: (ad+ bc)b'd' = adb'd' + bcb'd' = ab'dd' + bb'cd' = ba'dd' + bb'dc' = bd(a'd' + b'c'), which is the desired equality. Clearly [0, b] acts as a zero-element for this addition and [-a, b] as the negative of [a, b]. It is a simple matter to verify that F is an abelian group under this addition. We now turn to the multiplication in F. Again motivated by our pre- liminary heuristic discussion we define [a, b][c, d] = [ac, bd]. As in the case of addition, since b # 0, d # 0, bd # 0 and so [ ac, bd] E F. A com- putation, very much in the spirit of the one just carried out, proves that if [a, b] = [a', b'] and [c, d] = [c', d'] then [a, b][c, d] = [a', b'][c', d']. One can now show that the nonzero elements ofF (that is, all the elements [a, b] where a -f:. 0) form an abelian group under multiplication in which [d, d] acts as the unit element and where [c, d]- 1 = [d, c] (since c -f:. 0, [d, c] is in F). It is a routine computation to see that the distributive law holds in F. F is thus a field. All that remains is to show that D can be imbedded in F. We shall exhibit an explicit isomorphism of D into F. Before doing so we first notice that for x -f:. 0, y -f:. 0 in D, [ax, x] = [ay,y] because (ax) y = x(ay); let us denote [ax, x] by [a, 1]. Define ¢:D ~ F by ¢(a) = [a, 1] for every a E D. We leave it to the reader to verify that 4> is an isomorphism of D into F, and that if D has a unit element 1, then ¢(1) is the unit element of F. The theorem is now proved in its entirety. F is usually called the field of quotients of D. In the special case in which D is the ring of integers, the F so constructed is, of course, the field of rational numbers. Problems 1. Prove that if [a, b] = [a', b'] and [c, d] = [c', d'] then [a, b][c, d] == [a', b'][c', d']. 2. Prove the distributive law in F. 3. Prove that the mapping ¢:D ~ F defined by ¢(a) = [a, 1] is an isomorphism of D into F. 4. Prove that if K is any field which contains D then K contains a subfield isomorphic to F. (In this sense F is the smallest field containing D.) * 5. Let R be a commutative ring with unit element. A nonempty subset S of R is called a multiplicative system if 1. 0 ¢ s. 2. s1, s2 E S implies that s1 s2 E S. Sec. 3.7 Euclidean Rings 143 Let At be the set of all ordered pairs (r, s) where r E R, s E S. In At define (r, s) \"' (r', s') if there exists an element s\" E S such that s\" (rs' - sr') = 0. (a) Prove that this defines an equivalence relation on At. Let the equivalence class of (r, s) be denoted by [r, s], and let R8 be the set of all the equivalence classes. In R 8 define [r1 , s1] + [r2 , s2 ] = [r1s2 + r2s1 , s1s2 ] and [r1 , s1][r2 , s2 ] = [r1r2 , s1s2 ]. (b) Prove that the addition and multiplication described above are well defined and that R8 forms a ring under these operations. (c) Can R be imbedded in R8 ? (d) Prove that the mapping ¢:R ~ Rs defined by ¢(a) = [as, s] is a homomorphism of R into R8 and find the kernel of¢. (e) Prove that this kernel has no element of S in it. (f) Prove that every element of the form [sv s2 ] (where sv s2 E S) in R8 has an inverse in R8 • 6. Let D be an integral domain, a, bE D. Suppose that an = bn and am = bm for two relatively prime positive integers m and n. Prove that a= b. 7. Let R be a ring, possibly noncommutative, in which xy = 0 implies x = 0 or y = 0. If a, b E R and an = bn and am = bm for two relatively prime positive integers m and n, prove that a = b. 3.7 Euclidean Rings The class of rings we propose to study now is motivated by several existing examples-the ring of integers, the Gaussian integers (Section 3.8), and polynomial rings (Section 3.9). The definition of this class is designed to incorporate in it certain outstanding characteristics of the three concrete examples listed above. DEFINITION An integral domain R is said to be a Euclidean ring if for every a #- 0 in R there is defined a nonnegative integer d(a) such that I. For all a, bE R, both nonzero, d(a) ~ d(ab). 2. For any a, b E R, both nonzero, there exist t, r E R such that a = tb + r where either r = 0 or d(r) < d(b). We do not assign a value to d(O). The integers serve as an example of a Euclidean ring, where d(a) = absolute value of a acts as the required ·.·function. In the next section we shall see that the Gaussian integers also form a Euclidean ring. Out of that observation, and the results developed in this part, we shall prove a classic theorem in number theory due to 144 Ring Theory Ch. 3 Fermat, namely, that every prime number of the form 4n + 1 can be written as the sum of two squares. We begin with THEOREM 3.7.1 Let R be a Euclidean ring and let A be an ideal of R. Then there exists an element a0 E A such that A consists exactly of all a0 x as x ranges over R. Proof. If A just consists of the element 0, put a0 = 0 and the conclusion of the theorem holds. Thus we may assume that A =1= (0); hence there is an a =/::. 0 in A. Pick an a0 E A such that d ( ao) is minimal. (Since d takes on nonnegative integer values this is always possible.) Suppose that a E A. By the properties of Euclidean rings there exist t, r E R such that a = ta0 + r where r = 0 or d(r) < d(a0 ). Since a0 E A and A is an ideal of R, ta0 is in A. Combined with a E A this results in a - ta0 E A; but r = a - ta0 , whence rEA. Ifr =I= 0 then d(r) < d(a0 ), giving us an element r in A whose d-value is smaller than that of a0 , in contradiction to our choice of a0 as the element in A of minimal d-value. Consequently r = 0 and a = ta0 , which proves the theorem. We introduce the notation (a) = {xa I x E R} to represent the ideal of all multiples of a. DEFINITION An integral domain R with unit element is a principal ideal ring if every ideal A in R is of the form A = (a) for some a E R. Once we establish that a Euclidean ring has a unit element, in virtue of Theorem 3. 7.1, we shall know that a Euclidean ring is a principal ideal ring. The converse, however, is false; there are principal ideal rings which are not Euclidean rings. [See the paper by T. Motzkin, Bulletin of the American Mathematical Society, Vol. 55 ( 1949), pages 1142-1146, entitled \"The Euclidean algorithm.\"] COROLLARY TO THEOREM 3.7.1 A Euclidean ring possesses a unit element. Proof. Let R be a Euclidean ring; then R is certainly an ideal of R, so that by Theorem 3. 7.1 we may conclude that R = (u0 ) for some u0 E R. Thus every element in R is a multiple of u0 • Therefore, in particular, u0 = u0c for some c E R. If a E R then a = xu0 for some x E R, hence ac = (xu0 )c = x(u0c) = xu0 = a. Thus c is seen to be the required unit element. DEFINITION If a =/::. 0 and b are in a commutative ring R then a is said to divide b if there exists a c E R such that b = ac. We shall use the symbol Sec. 3.7 Euclidean Rings 145 a 1 b to represent the fact that a divides b and a~ b to mean that a does not divide b. The proof of the next remark is so simple and straightforward that we omit it. REMARK I. If a I b and b I c then a I c. 2. If a I b and a I c then a I ( b ± c) . 3. If a I b then a I bx for all x E R. DEFINITION If a, bE R then dE R is said to be a greatest common divisor of a and b if I. d I a and d I b. 2. Whenever c I a and c I b then c I d. We shall use the notation d = (a, b) to denote that dis a greatest common divisor of a and b. LEMMA 3.7.1 Let R be a Euclidean ring. Then any two elements a and b in 'R have a greatest common divisor d. Moreover d = Aa + Jlb for some A, f.1 E R. Proof. Let A be the set of all elements ra + sb where r, s range over R. We claim that A is an ideal of R. For suppose that x, yEA; therefore ~ = r1a + s1b, y = r2a + s2b, and so x ±Y = (r1 ± r2 )a + (s1 ± s2 )b EA. Similarly, for any u E R, ux = u(r1a + s1b) = (ur1 )a + (us1 )b EA. Since A is an ideal of R, by Theorem 3. 7.1 there exists an element dE A such that every element in A is a mutiple of d. By dint of the fact that dE A and that every element of A is of the form ra + sb, d = Aa + \"'jtb for some A, f.1 E R. Now by the corollary to Theorem 3.7.1, R has a unit element 1; thus a = la + Ob E A, b = Oa + Ib EA. Being in A, they are both multiples of d, whence d I a and d I b. Suppose, finally, that c I a and c I b; then c I Aa and c I Jlb so that c certainly divides Aa + Jlb = d. Therefore d has all the requisite conditions for a greatest common divisor and the lemma is proved. DEFINITION Let R be a commutative ring with unit element. An element a E R is a unit in R if there exists an element b E R such that ab = 1. Do not confuse a unit with a unit element! A unit in a ring is an element Whose inverse is also in the ring. LEMMA 3.7.2 Let R be an integral domain with unit element and suppose that for a, b E R both a I b and b I a are true. Then a = ub, where u is a unit in R. 146 Ring Theory Ch. 3 Proof. Since a I b, b = xa for some x E R; since b I a, a = yb for some y E R. Thus b = x(yb) = (xy)b; but these are elements of an integral domain, so that we can cancel the b and obtain xy = 1; y is thus a unit in R and a = yb, proving the lemma. DEFINITION Let R be a commutative ring with unit element. Two elements a and bin Rare said to be associates if b = ua for some unit u in R. The relation of being associates is an equivalence relation. (Problem 1 at the end of this section.) Note that in a Euclidean ring any two greatest common divisors of two given elements are associates (Problem 2). Up to this point we have, as yet, not made use of condition 1 in the definition of a Euclidean ring, namely that d(a) ~ d(ab) for b i= 0. We now make use of it in the proof of LEMMA 3.7.3 Let R be a Euclidean ring and a, bE R. If b i= 0 is not a unit in R, then d(a) < d(ab). Proof. Consider the ideal A = (a) = {xa I x E R} of R. By condition 1 for a Euclidean ring, d(a) ~ d(xa) for x i= 0 in R. Thus the d-value of a is the minimum for the d-value of any element in A. Now ab E A; if d(ab) = d(a), by the proof used in establishing Theorem 3.7.1, since the d-value of ab is minimal in regard to A, every element in A is a multiple of ab. In particular, since a E A, a must be a multiple of ab; whence a = abx for some x E R. Since all this is taking place in an integral domain we obtain bx = 1. In this way b is a unit in R, in contradiction to the fact that it was not a unit. The net result of this is that d(a) < d(ab). DEFINITION In the Euclidean ring R a nonunit n is said to be a prime element of R if whenever n = ab, where a, b are in R, then one of a or b is a unit in R. A prime element is thus an element in R which cannot be factored in R in a nontrivial way. LEMMA 3.7.4 Let R be a Euclidean ring. Then every element in R is either a unit in R or can be written as the product of a finite number of prime elements of R. Proof. The proof is by induction on d (a). If d(a) = d(l) then a is a unit in R (Problem 3), and so in this case, the assertion of the lemma is correct. We assume that the lemma is true for all elements x in R suth that d(x) < d(a). On the basis of this assumption we aim to prove it for a. This would complete the induction and prove the lemma. Sec. 3.7 Euclidean Rings 147 If a is a prime element of R there is nothing to prove. So suppose that a= bcwhereneitherbnorcisaunitinR. ByLemma3.7.3,d(b) < d(bc) = j(a) and d(c) < d(bc) = d(a). Thus by our induction hypothesis b and c can be written as a product of a finite number of prime elements of R; •JJ = n1n2 • • • nn, c = n~n~ · · · n;,. where then's and n\"s are prime elements ;,of R. Consequently a = be = n 1n 2 • • • nnn~n~ · · · n;,. and in this way a !bas been factored as a product of a finite number of prime elements. This completes the proof. DEFINITION In the Euclidean ring R, a and bin Rare said to be relatively prime if their greatest common divisor is a unit of R. Since any associate of a greatest common divisor is a greatest common divisor, and since I is an associate of any unit, if a and b are relatively prime we may assume that (a, b) = 1. LEMMA 3.7.5 Let R be a Euclidean ring. Suppose that for a, b, c E R, a I be but (a, b) = 1. Then a I c. Proof. As we have seen in Lemma 3.7.1, the greatest common divisor of a and b can be realized in the form .Aa + Jlb. Thus by our assumptions, A.a + Jlb = 1. Multiplying this relation by c we obtain .Aac + Jlbc = c. Now a I .Aac, always, and a I Jlbc since a I be by assumption; therefore a I ( .Aac + Jlbc) = c. This is, of course, the assertion of the lemma. We wish to show that prime elements in a Euclidean ring play the same role that prime numbers play in the integers. If n in R is a prime element of R and a E R, then either n I a or (n, a) = I, for, in particular, (n, d) is a divisor of n so it must be n or I (or any unit). If (n, a) = 1, one-half our assertion is true; if ( n, a) = n, since ( n, a) I a we get n I a, and the other half of our assertion is true. LEMMA 3}.6 {f n is a prime element in the Euclidean ring R and n I ab where a, b E R then n divides at least one of a or b. Proof. Suppose that n does not divide a; then (n, a) Lemma 3. 7.5 we are led ton I b. 1. Applying COROLLARY If n is a prime element in the Euclidean ring Rand n I a1a2 ···an then n divides at least one a1 , a2 , ••• , an. We carry the analogy between prime elements and prime numbers further and prove 148 Ring Theory Ch. 3 THEOREM 3.7.2 (UNIQUE FACTORIZATION THEOREM) Let R be a Eu- clidean ring and a =P 0 a no nun it in R. Suppose that a = n 1 n 2 • • • n n ::::: n~ n; · · · n:n where the ni and nj are prime elements of R. Then n = m and each ni, 1 :::; i :::; n is an associate of some nj, 1 :::; j :::; m and conversely each n'k is an associate of some nq. Proof. Lookatthe relation a= n 1 n2 • • • nn = n~ n; · · · n:n. Butn 1 I n 1 n2 • • • nm hence n 1 I n~n; · · · n:n. By Lemma 3.7.6, n 1 must divide some n~; since n 1 and n~ are both prime elements of R and n 1 In~ they must be associates and n~ = u1n 1, where u1 is a unit in R. Thus n 1n2 • • • nn = n~n~ · · · n:n = u1n 1n; · · · n~_1n;+l · · · n:n; cancel off n 1 and we are left with n2 • • • nn = u1n; · · · n~_ 1n~+l · · · n:n. Repeat the argument on this relation with n2 • After n steps, the left side becomes 1, the right side a product of a certain number of n' (the excess of m over n). This would force n :::; m since the n' are not units. Similarly, m :::; n, so that n = m. In the process we have also showed that every ni has some ni as an associate and conversely. Combining Lemma 3.7.4 and Theorem 3.7.2 we have that every nonzero element in a Euclidean ring R can be uniquely written (up to associates) as a product of prime elements or is a unit in R. We finish the section by determining all the maximal ideals in a Euclidean ring. In Theorem 3.7.1 we proved that any ideal A in the Euclidean ring R is of the form A = (a0 ) where (a0 ) = {xa0 I x E R}. We now ask: What con- ditions imposed on a0 insure that A is a maximal ideal of R? For this question we have a simple, precise answer, namely LEMMA 3.7.7 The ideal A = (a0 ) is a maximal ideal of the Euclidean ring R if and only if a0 is a prime element of R. Proof. We first prove that if a0 is not a prime element, then A = (a0 ) is not a maximal ideal. For, suppose that a0 = be where b, c E R and neither b nor cis a unit. Let B = (b); then certainly a0 E B so that A c B. We claim that A =P Band that B =P R. If B = R then 1 E B so that 1 = xb for some x E R, forcing b to be a unit in R, which it is not. On the other hand, if A = B then b E B = A whence b = xa0 for some x E R. Combined with a0 = be this results in a0 = xca0 , in consequence of which xc = 1. But this forces c to be a unit in R, again contradicting our assumption. Therefore B is neither A nor R and since A c B, A cannot be a maximal ideal of R. Conversely, suppose that a0 is a prime element of R and that U is an ideal of R such that A= (a0 ) c U cR. By Theorem 3.7.1, U.= (u0 )· Since a0 E A c U = (u0 ), a0 = xu0 for some x E R. But a0 is a prime element of R, from which it follows that either x or u0 is a unit in R. If Uo Sec. 3.8 A Particular Euclidean Ring 149 is a unit in R then U = R (see Problem 5). If, on the other hand, x is a unit in R, then x- 1 E R and the relation a0 = xu0 becomes u0 = x- 1a0 E A since A is an ideal of R. This implies that U c A; together with A c U we conclude that U = A. Therefore there is no ideal of R which fits strictly between A and R. This means that A is a maximal ideal of R. Problems I. In a commutative ring with unit element prove that the relation a is an associate of b is an equivalence relation. 2. In a Euclidean ring prove that any two greatest common divisors of a and b are as so cia tes. 3. Prove that a necessary and sufficient condition that the element a m the Euclidean ring be a unit is that d(a) = d(l). 4. Prove that in a Euclidean ring (a, b) can be found as follows: b = qoa + r1, where d(r1) < d(a) a q1r1 + r2, where d (r2) < d (r1) r1 q2r2 + r3, where d(r3) < d(r2) rn-1 qnrn and rn (a, b). 5. Prove that if an ideal U of a ring R contains a unit of R, then U = R. 6. Prove that the units in a commutative ring with a unit element form an abelian group. 7. Given two elements a, b in the Euclidean ring R their least common multiple c E R is an element in R such that a I c and b I c and such that whenever a I x and b I x for x E R then c I x. Prove that any two elements in the Euclidean ring R have a least common multiple in R. 8. In Problem 7, if the least common multiple of a and b is denoted by [a, b], prove that [a, b] = abf(a, b). 3.8 A Particular Euclidean Ring An abstraction in mathematics gains in substance and importance when, particularized to a specific example, it sheds new light on this example. We are about to particularize the notion of a Euclidean ring to a concrete · ring, the ring of Gaussian integers. Applying the general results obtained about Euclidean rings to the Gaussian integers we shall obtain a highly nontrivial theorem about prime numbers due to Fermat. 150 Ring Theory Ch. 3 Let 1[i] denote the set of all complex numbers of the form a + hi where a and h are integers. Under the usual addition and multiplication of com- plex numbers 1[i] forms an integral domain called the domain of Gaussian integers. Our first objective is to exhibit 1[i] as a Euclidean ring. In order to do this we must first introduce a function d (x) defined for every nonzero element in 1[i] which satisfies 1. d ( x) is a nonnegative integer for every x =/= 0 E 1 [ i]. 2. d(x) :::;; d(xy) for every y =/= 0 in 1[i]. 3. Given u, v E 1[i] there exist t, r E 1[i] such that v = tu + r where r = 0 or d(r) < d(u). Our candidate for this function dis the following: if x =a+ hiE 1 [i], then d(x) = a2 + h2 . The d(x) so defined certainly satisfies property 1; in fact, if x =f. 0 E 1[i] then d (x) ~ 1. As is well known, for any two com- plex numbers (not necessarily in 1[i]) x, y, d (xy) = d (x)d (y); thus if x andy are in addition in 1[i] andy =f. 0, then since d(y) ~ 1, d(x) = d (x) 1 :::;; d (x)d (y) = d (xy), showing that condition 2 is satisfied. All our effort now will be to show that condition 3 also holds for this function din j[i]. This is done in the proof of THEOREM 3.8.1 1[i] is a Euclidean ring. Proof. As was remarked in the discussion above, to prove Theorem 3.8.1 we merely must show that, given x,y E 1[i] there exists t, r E 1[i] such thaty = tx + r where r = 0 or d(r) < d(x). We first establish this for a very special case, namely, where y is arbitrary in 1[i] but where x is an (ordinary) positive integer n. Suppose that y = a + hi; by the division algorithm for the ring of integers we can find integers u, v such that a = un + u1 and h = vn + v1 where u1 and v1 are integers satisfying lu1 1:::;; tn and lv1 1:::;; tn. Let t = u + vi and r = u1 + v1i; then y = a + hi = un + u1 + (vn + v1)i = (u + vi)n + u1 + v1i = tn + r. Sinced(r) = d(u 1 + v1i) = u12 + v12 :::;; n2 /4 + n 2f4 < n 2 = d(n), we see that in this special case we have shown that y = tn + r with r = 0 or d ( r) < d ( n) . We now go to the general case; let x =1= 0 andy be arbitrary elements in 1[i]. Thus xx is a positive integer n where xis the complex conjugate of x. Applying the result of the paragraph above to the elements yx and n we see that there are elements t, r E 1[i] such that yx = tn + r with r = 0 or d(r) < d(n). Putting into this relation n = xx we obtain d(yx- txx) < d(n) = d(xx); applying to this the fact that d(yx- txx) = d(y- tx)d(x) and d(xx) = d(x)d(x) we obtain that d(y - tx)d(x) < d(x)d(x). Since x =/= 0, d(x) is a positive integer, so this inequality simplifies to d(y- tx) <. d(x). We represent y = tx + r0 , where r0 = y - tx; thus t and r0 are in Sec. 3.8 A Particular Euclidean Ring 151 and as we saw above, r0 = 0 or d(r0 ) = d(y - tx) < d(x). This the theorem. Since J[i] has been proved to be a Euclidean ring, we are free to use the u; .. ·.-\"'\"'~44~~ established about this class of rings in the previous section to the lf!];!;uc:u<Iea.n ring we have at hand, J[i]. 3.8.1 Let p be a prime integer and suppose that for some integer c ll;r-etal~zvety prime to p we can find integers x andy such that x 2 + y 2 = cp. Then can be written as the sum of squares of two integers, that is, there exist integers and b such thatp = a 2 + b2 • Proof. The ring of integers is a subring of J[i]. Suppose that the integer pis also a prime element of j[i]. Since cp = x 2 + y 2 = (x + yi)(x - yi), by Lemma 3. 7.6, pI (x + yi) or pI (x - yi) in J[i]. But if pI (x + yi) then x + yi = p(u + vi) which would say that x = pu and y = pv so that p also would divide x- yi. But then p 2 I (x + yi) (x - yi) = cp from which we would conclude that p I c contrary to assumption. Similarly if p I (x - yi). p is not a prime element in J[i] ! In consequence of this, p = (a + hi) (g + di) a + hi and g + di are in j[i] and where neither a + hi nor g + di is a unit in J[i]. But this means that neither a2 + b2 = 1 nor g 2 + d 2 = 1. (See Problem 2.) From p = (a+ bi)(g + di) it follows easily that p = (a - hi) (g - di). Thus p 2 = (a+ bi)(g + di)(a - hi) (g- di) = (a 2 + b2 )(g 2 + d2 ). Therefore (a 2 + b2 ) I p 2 so a2 + b2 = 1, p or p 2 ; a2 + b2 =I= 1 since a + bi is not a unit, in J[i]; a 2 + b2 =1= p 2 , otherwise g 2 + d2 = 1, con- trary to the fact that g + di is not a unit in J[i]. Thus the only feasibility left is that a2 + b2 = p and the lemma is thereby established. The odd prime numbers divide into two classes, those which have a remainder ,of 1 on division by 4 and those which have a remainder of 3 on division by 4. We aim to show that every prime number of the first kind can be written as the sum of two squares, whereas no prime in the second class can be so represented. LEMMA 3.8.2 If p is a prime number of the form 4n + 1, then we can solve the congruence x 2 = - 1 mod p. Proof. Let x = 1 · 2 · 3 · · · (p - 1 )/2 . Since p - 1 = 4n, in this prod- · Uct for x there are an even number of terms, in consequence of which X= (-J)(-2){-3) •• • ( -( p;J )} 152 Ring Theory Ch. 3 Butp - k = -k modp, so that x 2 ~ (1·2···P; ')<-1)(-2)···( -~; ')) - 1·2···p- 1 P + 1 ···(p- 1) 2 2 _ (p - 1) ! = - 1 mod p. We are using here Wilson's theorem, proved earlier, namely that if p is a prime number (p - 1)! = -1 (p). To illustrate this result, if p = 13, x = 1 · 2 · 3 · 4 · 5 · 6 = 720 = 5 mod 13 and 5 2 = -1 mod 13. THEOREM 3.8.2 (FERMAT) If p is a prime number of the form 4n + 1, then p = a2 + b2 for some integers a, b. Proof. By Lemma 3.8.2 there exists an x such that x 2 = -1 mod p. The x can be chosen so that 0 ~ x ~ p - 1 since we only need to use the remainder of x on division by p. We can restrict the size of x even further, namely to satisfy lxl ~ pf2. For if x > pf2, then y = p - x satisfies y 2 = -1 mod p but I Yl ~ pf2. Thus we may assume that we have an integer x such that lxl ~ pf2 and x 2 + 1 is a multiple of p, say cp. Now cp = x 2 + 1 ~ p 2 f4 + 1 < p 2 , hence c < p and so p ,{'c. Invoking Lemma 3.8.1 we obtain that p = a 2 + b2 for some integers a and b, proving the theorem. Problems 1. Find all the units in j[i]. 2. If a + hi is not a unit of j[i] prove that a2 + b 2 > 1. 3. Find the greatest common divisor in j[i] of (a) 3 + 4i and 4 - 3i. (b) 11 + 7i and 18 - i. 4. Prove that if p is a prime number of the form 4n + 3, then there is no x such that x 2 = - 1 mod p. 5. Prove that no prime of the form 4n + 3 can be written as a2 + b2 where a and bare integers. 6. Prove that there is an infinite number of primes of the form 4n + 3. *7. Prove there exists an infinite number of primes of the form 4n + 1. *8. Determine all the prime elements in j[i]. *9. Determine all positive integers which can be written as a sum of two squares (of integers). Sec. 3.9 Polynomial Rings 153 3.9 Polynomial Rings Very early in our mathematical education-in fact in junior high school or early in high school itself-we are introduced to polynomials. For a seemingly endless amount of time we are drilled, to the point of utter boredom, in factoring them, multiplying them, dividing them, simplifying them. Facility in factoring a quadratic becomes confused with genuine mathematical talent. Later, at the beginning college level, polynomials make their appearance in a somewhat different setting. Now they are functions, taking on values, and we become concerned with their continuity, their derivatives, their integrals, their maxima and minima. We too shall be interested in polynomials but from neither of the above viewpoints. To us polynomials will simply be elements of a certain ring and we shall be concerned with algebraic properties of this ring. Our primary interest in them will be that they give us a Euclidean ring whose properties will be decisive in discussing fields and extensions of fields. Let F be a field. By the ring rif polynomials in the indeterminate, x, written as F[x], we mean the set of all symbols a0 + a1x + · · · + anx\", where n can be any nonnegative integer and where the coefficients a1 , a2 , ••• , an are all in F. In order to make a ring out of F[x] we must be able to recognize when two elements in it are equal, we must be able to add and multiply elements of F[x] so that the axioms defining a ring hold true for F[x]. This will be our initial goal. We could avoid the phrase \"the set of all symbols\" used above by intro- ducing an appropriate apparatus of sequences but it seems more desirable to follow a path which is somewhat familiar to most readers. DEFINITION If p(x) = a0 + a1x + · · · + amxm and q(x) = b0 + b1x + · · · + bnx\" are in F[x], then p(x) = q(x) if and only if for every integer i ;;::: 0, ai = bi. Thus two polynomials are declared to be equal if and only if their corre- sponding coefficients are equal. DEFINITION If p(x) = a0 + a1x + · · · + amxm and q(x) = b0 + b1x + · · · + bnx\" are both in F[x], then p(x) + q(x) = c0 + c1x + · · · + ctxt where for each i, ci = ai + bi. In other words, add two polynomials by adding their coefficients and collecting terms. To add 1 + x and 3 - 2x + x 2 we consider 1 + x as 1 + x + Ox 2 and add, according to the recipe given in the definition, to obtain as their sum 4 - x + x 2 • 154 Ring Theory Ch. 3 The most complicated item, and the only one left for us to define for F[x], is the multiplication. DEFINITION If p(x) = a0 + a1x + · · · + amxm and q(x) = b0 + b1x + · · · + bnx\", then p(x)q(x) = c0 + c1x + · · · + ckxk where ct = atbo + at-lbl + at-2b2 + ... + aobt. This definition says nothing more than: multiply the two polynomials by multiplying out the symbols formally, use the relation xrx.xP = xa+P, and collect terms. Let us illustrate the definition with an example: p(x) = 1 + x - x 2, q(x) = 2 + x 2 + x 3 • Here a0 = 1, a 1 = 1, a2 = - 1, a3 = a4 = · · · = 0, and b0 = 2, b1 = 0, b2 = 1, b3 = 1, b 4 = b 5 = · · · = 0. Thus c0 = a0 b0 = 1.2 = 2, c1 = a1b0 + a0 b1 = 1.2 + 1.0 = 2, c2 = a2b0 + a1b1 + a0 b2 = ( -1)(2) + 1.0 + 1.1 -1, c3 =a 3b0 + a2b1 + a1b2 + a0 b3 = (0)(2) + ( -1)(0) + 1.1 + 1.1 = 2, c4 = a4 b0 + a3b1 + a2b2 + a1b3 + a0 b4 = (0)(2) + (0)(0) + ( -1)(1) + (1)(1) + 1(0) = 0, c5 = a5b0 + a 4 b1 + a3b2 + a2b3 + a 1b4 + a0 b5 = (0)(2) + (0)(0) + (0)(1) + (-1)(1) + (1)(0) + (0)(0) = -1, c6 = a6 b0 + a5b1 + a4 b2 + a3b3 + a2b4 + a 1 b5 + a0 b6 = (0)(2) + (0)(0) + (0)(1) + (0)(1) + ( -1)(0) + (1)(0) + (1)(0) = 0, c7 = c8 = · · · = 0. Therefore according to our definition, (1 + x- x2 )(2 + x2 + x3 ) = c0 + c1x + · · · = 2 + 2x- x 2 + 2x 3 - x 5 . If you multiply these together high-school style you will see that you get the same answer. Our definition of product is the one the reader has always known. Without further ado we assert that F[x] is a ring with these operations, its multiplication is commutative, and it has a unit element. We leave the verification of the ring axioms to the reader. DEFINITION If f(x) = a0 + a1x + · · · + anx\" =I= 0 and an =I= 0 then the degree off (x), written as degf (x), is n. That is, the degree off (x) is the largest integer i for which the ith co- efficient off (x) is not 0. We do not define the degree of the zero poly- nomial. We say a polynomial is a constant if its degree is 0. The degree Sec. 3.9 Polynomial Rings 155 function defined on the nonzero elements of F [ x] will provide us with the function d(x) needed in order that F[x] be a Euclidean ring. LEMMA 3.9.1 lff(x), g(x) are two nonzero elements of F[x], then deg (f(x)g(x)) = degf(x) + degg(x). Proof. Suppose that f (x) = a0 + a1 x + · · · + amxm and g(x) = b0 + b1x + · · · + bnxn and that am =f:. 0 and bn =f:. 0. Therefore deg f (x) = m and deg g(x) = n. By definition, f (x) g(x) = c0 + c1x + · · · + ckxk where c1 = atbo + at_ 1b1 + · · · + a1bt-l + a0 bt. We claim that cm+n = ambn =f:. 0 and ci = 0 for i > m + n. That cm+n = ambn can be seen at a glance by its definition. What about C; for i > m + n? ci is the sum of terms of the form aibi- i; since i = J + (i - J) > m + n then either J > m or (i- J) > n. But then one of ai or bi-i is 0, so that aibi-i = 0; since c; is the sum of a bunch of zeros it itself is 0, and our claim has been established. Thus the highest nonzero coefficient off (x) g(x) is em+ m whence deg f(x)g(x) = m + n = deg f(x) + deg g(x). COROLLARY If f(x), g(x) are nonzero elements in F[x] then deg f(x) :5; deg f(x)g(x). Proof. Since deg f(x)g(x) = degf(x) + deg g(x), and since deg g(x) ~ 0, this result is immediate from the lemma. COROLLARY F[x] is an integral domain. We leave the proof of this corollary to the reader. Since F[x] is an integral domain, in light of Theorem 3.6.1 we ~an construct for it its field of quotients. This field merely consists of all quotients of polynomials and is called the field of rational functions in x over F. The function deg f (x) defined for all f (x) =f:. 0 in F[x] satisfies I. deg f (x) is a nonnegative integer. 2. deg f (x) :5; deg f (x) g(x) for all g(x) =f:. 0 in F[ x]. In order for F[x] to be a Euclidean ring with the degree function acting as the d-function of a Euclidean ring we still need that given f (x), g(x) E F[x], there exist t(x), r(x) in F[x] such thatf (x) = t(x)g(x) + r(x) where either r(x) = 0 or deg r(x) < deg g(x). This is provided us by LEMMA 3.9.2 (THE DIVISION ALGORITHM) Given two polynomials f(x) and g(x) =f:. 0 in F[x], then there exist two polynomials t(x) and r(x) in F[x] such . thatf(x) = t(x)g(x) + r(x) where r(x) = 0 or deg r(x) < deg g(x). Proof. The proof is actually nothing more than the \"long-division\" process we all used in school to divide one polynomial by another. 156 Ring Theory Ch. 3 If the degree off (x) is smaller than that of g(x) there is nothing to prove, for merely put t (x) = 0, r(x) = f (x), and we certainly have that f (x) = Og(x) + f (x) where deg f (x) < deg g(x) or f (x) = 0. So we may assume thatf (x) = a0 + a1x + · · · + amxm and g(x) = b0 + b1x + · · · + bnxn where am =I- 0, bn =I- 0 and m ~ n. Let j 1 (x) =J(x)- (amfbn)xm-ng(x); thus degj1 (x)::; m- 1, so by induction on the degree ofj(x) we may assume thatf1 (x) = t1(x)g(x) + r(x) where r(x) = 0 ordeg r(x) < deg g(x). Butthenf (x) - (amfbn)xm-ng(x) = t1 (x) g(x) + r(x), from which, by transposing, we arrive at f (x) = ((amfbn)xm-n + t1(x))g(x) + r(x). If we put t(x) = (amfbn)xm-n + t1(x) we do indeed have that f(x) = t(x)g(x) + r(x) where t(x), r(x) E F[x] and where r(x) = 0 or deg r(x) < deg g(x). This proves the lemma. This last lemma fills the gap needed to exhibit F[x] as a Euclidean ring and we now have the right to say THEOREM 3.9.1 F[x] is a Euclidean ring. All the results of Section 3. 7 now carry over and we list these, for our particular case, as the following lemmas. It could be very instructive for the reader to try to prove these directly, adapting the arguments used in Section 3. 7 for our particular ring F[x] and its Euclidean function, the degree. LEMMA 3.9.3 F[x] is a principal ideal ring. LEMMA 3.9.4 Given two polynomials f(x), g(x) in F[x] they have a greatest common divisor d (x) which can be realized as d (x) = J..(x) f (x) + p(x) g(x). What corresponds to a prime element? DEFINITION A polynomial p(x) in F[x] is said to be irreducible over F if whenever p(x) = a(x)b(x) with a(x), b(x) E F[x], then one of a(x) or b(x) has degree 0 (i.e., is a constant). Irreducibility depends on the field; for instance the polynomial x 2 + 1 is irreducible over the real field but not over the complex field, for there x2 + 1 = (x + i) (x - i) where i 2 = -1. LEMMA 3.9.5 Any polynomial in F[x] can be written in a unique manner as a product of irreducible polynomials in F [ x]. LEMMA 3.9.6 The ideal A = (p(x)) in F[x] is a maximal ideal if and only if p(x) is irreducible over F. Sec. 3.9 Polynomial Rings 157 In Chapter 5 we shall return to take a much closer look at this field F[x]f(p(x)), but for now we should like to compute an example. Let F be the field of rational numbers and consider the polynomial p(x) = x 3 - 2 in F[x]. As is easily verified, it is irreducible over F, whence F[x]f(x 3 - 2) is a field. What do its elements look like? Let A = (x 3 - 2), the ideal in F[x] generated by x 3 - 2. Any element in F[x]f(x 3 - 2) is a coset of the formf(x) +A of the ideal A with f (x) in F[x]. Now, given any polynomial f (x) E F[x], by the division algorithm, f (x) = t (x) (x 3 - 2) + r(x), where r(x) = 0 or deg r(x) < deg (x 3 - 2) = 3. Thus r(x) = a0 + a1x + a2x 2 where a0 , a 1 , a2 are in F; consequently f (x) + A = a0 + a 1 x + a2x 2 + t (x) (x 3 - 2) + A = a0 + a1x + a2 x 2 +A since t(x)(x 3 - 2) is in A, hence by the addi- tion and multiplication in F[x]f(x 3 - 2), f (x) + A = (a0 + A) + a1(x +A) + a2 (x + A) 2 • If we put t = x +A, then every element in F[x]f(x 3 - 2) is of the form a0 + a1t + a2 t2 with a0 , a 1, a2 in F. What about t? Since t 3 - 2 = (x + A) 3 ~ 2 = x 3 - 2 + A = A = 0 (since A is the zero element of F[x]f(x 3 - 2)) we see that t 3 = 2. Also, if a0 + a1t + a2 t 2 = b0 + b1t + b2 t 2 , then (a0 - b0 ) + (a 1 - b1 )t + (a2 - b2 )t 2 = 0, whence (a0 - b0 ) + (a1 - b1 )x + (a2 - b2 )x 2 is in A = (x 3 - 2). How can this be, since every element in A has degree at least 3? Only if a0 - b0 + (a1 - b1 )x + (a2 - b2 )x 2 = 0, that is, only if a0 = b0 , a1 = b1 , a2 = b2 • Thus every element in F[x]f(x 3 - 2) has a unique representation as a0 + a1t + a2t 2 where a0 , a 1 , a2 E F. By Lemma 3.9.6, F[x]f(x 3 - 2) is a field. It would be instructive to see this directly; all that it entails is proving that if a0 + a1 t + a2 t 2 =/= 0 then it has an inverse of the form a + {Jt + yt 2 • Hence we must solve for a, {3, y in the relation (a0 + a1t + a2 t 2 )(a + {Jt + yt 2 ) = 1, where not all of a0 \"-1 , a2 are 0. Multiplying the relation out and using t 3 = 2 we obtain (aoa + 2a2 {3 + 2a1 y) + (a1a + a0 {3 + 2a2 y)t + (a2 a + a1{3 + a0 y)t 2 = 1; thus a0 a + 2a2 {3 + 2a1 y = 1, a1a + a0 {3 + 2a2 y = 0, a2a + a1{3 + a0 y = 0. We can try to solve these three equations in the three unknowns a, {3, y. When we do so we find that a solution exists if and only if a0 3 + 2a 1 3 + 4a2 3 - 6a0 a1 a2 =/= 0. Therefore the problem of proving directly that F[x]f(x 3 - 2) is a field boils down to proving that the only solution in rational numbers of (1) 158 Ring Theory Ch. 3 is the solution a0 = a1 = a2 = 0. We now proceed to show this. If a solution exists in rationals, by clearing of denominators we can show that a solution exists where a0 , au a2 are integers. Thus we may assume that a0 , a1 , a2 are integers satisfying (1). We now assert that we may assume that a0 , a1 , a2 have no common divisor other than 1, for if a0 = b0d, a1 = b1 d, and a2 = b2d, where d is their greatest common divisor, then substituting in (1) we obtain d 3 (b 0 3 + 2b 1 3 + 4b 2 3 ) = d 3 (6b0 b1b2 ), and so b0 3 + 2b 1 3 + 4b 2 3 = 6b0 b1 b2 • The problem has thus been reduced to proving that ( 1) has no solutions in integers which are relatively prime. But then ( 1) implies that a0 3 is even, so that a0 is even; substituting a0 = 2(.{ 0 in (1) gives us 4(.{0 3 + a1 3 + 2a/ = 6a0 a1a2 • Thus a 1 3 , and so, a1 is even; a1 = 2a1 . Substituting in (1) we obtain 2(.{0 3 + 4a 1 3 + a2 3 = 6a0 a1a2 . Thus a2 3 , and so a2 , is even! But then a0 , a1 , a2 have 2 as a common factor! This contradicts that they are relatively prime, and we have proved that the equation a0 3 + 2a 1 3 + 4a2 3 = 6a0a1 a2 has no rational solution other than a0 = a1 = a2 = 0. Therefore we can solve for a, p, y and F[x]f(x 3 - 2) is seen, directly, to be a field. Problems 1. Find the greatest common divisor of the following polynomials over F, the field of rational numbers: (a) x 3 - 6x 2 + x + 4 and x 5 - 6x + 1. (b) x 2 + 1 and x6 + x 3 + x + 1. 2. Prove that (a) x 2 + x + 1 is irreducible over F, the field of integers mod 2. (b) x 2 + 1 is irreducible over the integers mod 7. (c) x 3 - 9 is irreducible over the integers mod 31. (d) x 3 - 9 is reducible over the integers mod 11. 3. Let F, K be two fields F c K and suppose f (x), g(x) E F[ x] are re- latively prime in F[x]. Prove that they are relatively prime in K[x]. 4. (a) Prove that x 2 + 1 is irreducible over the field F of integers mod 11 and prove directly that F[x]f(x 2 + 1) is a field having 121 elements. (b) Prove that x 2 + x + 4 is irreducible over F, the field of integers mod 11 and prove directly that F[x]f(x 2 + x + 4) is a field having 121 elements. * (c) Prove that the fields of part (a) and part (b) are isomorphic. 5. Let F be the field of real numbers. Prove that F[x]f(x 2 + 1) is a field isomorphic to the field of complex numbers. *6. Define the derivativef'(x) of the polynomial f (x) = a0 + a1x + · · · + anxn as f'(x) = a1 + 2a2 x + 3a3x 2 + · · · + nan~- 1 . 1 ·~ Sec. 3.10 Polynomials over the Rational Field 159 Prove that ifj (x) E F[x], where F is the field of rational numbers, then f(x) is divisible by the square of a polynomial if and only ifj(x) and f'(x) have a greatest common divisor d(x) ofpositive degree. 7. Ifj(x) is in F[x], where F is the field of integers mod p, p a prime, andf(x) is irreducible over Fofdegree n prove that F[x]f(f(x)) is a field with pn elements. 3.10 Polynomials over the Rational Field We specialize the general discussion to that of polynomials whose co- efficients are rational numbers. Most of the time the coefficients will actually be integers. For such polynomials we shall be concerned with their irreducibility. DEFINITION The polynomial f(x) = a0 + a1x + · · · + anxn, where the tzo, a1, a2, ... , an are integers is said to be primitive if the greatest common divisor of a0 , a1, ... , an is I. LEMMA 3.10.1 If f(x) and g(x) are primitive polynomials, then f(x)g(x) is a primitive polynomial. Proof. Letf(x) = a0 + a1x + · · · + anxn and g(x) = b0 + b1x + · · · + bmxm. Suppose that the lemma was false; then all the coefficients of f(x)g(x) would be divisible by some integer larger than 1, hence by some prime number p. Sincef (x) is primitive, p does not divide some coefficient \"\"\"\" ai. Let ai be the first coefficient off (x) which p does not divide. Similarly let bk be the first coefficient of g(x) which p does not divide. In f (x)~(x) the coefficient of xi+!, ci+k' is ci+k = aibk + (ai+ 1bk_ 1 + ai+ 2bk_ 2 + · · · + ai+kb0 ) + (aj-1bk+1 + ai_2bk+2 + · · · + aobj+k). (1) Now by our choice of bk, pI bk_ 1, bk_ 2, ... so that pI (ai+l bk- 1 + ai+ 2bk- 2 + · · · + ai+kb0 ). Similarly, by our choice of ai, pI ai_ 1, ai_ 2, ... so that PI (ai_ 1bk+ 1 + ai_ 2bk+2 + · · · + a0 bk+i). By assumption, pI cj+k· Thus by ( 1), p I a ibk, which is nonsense since p ,r a i and p ,r bk. This proves the lemma. DEFINITION The content of the polynomial f(x) = a0 + a1x + · · · + a~, where the a's are integers, is the greatest common divisor of the integers a0 , a1 , ••• , an. Clearly, given any polynomial p(x) with integer coefficients it can be written as p(x) = dq(x) where dis the content of p(x) and where q(x) is a primitive polynomial. 1 160 Ring Theory Ch. 3 THEOREM 3.10.1 (GAuss' LEMMA) If the primitive polynomial f(x) can be factored as the product of two polynomials having rational coefficients, it can be factored as the product of two polynomials having integer coefficients. Proof. Suppose that f (x) = u(x)v(x) where u(x) and v(x) have rational coefficients. By clearing of denominators and taking out common factors we can then write f(x) = (afb)l(x)Jl(x) where a and b are integers and where both l(x) and Jl(x) have integer coefficients and are primitive. Thus bf (x) = al(x) Jl(x). The content of the left-hand side is b, since f(x) is primitive; since both l(x) and Jl(x) are primitive, by Lemma 3.10.1 l(x) Jl(x) is primitive, so that the content of the right-hand side is a. There- fore a = b, (a/b) = 1, and f (x) = l(x)Jl(x) where l(x) and Jl(x) have integer coefficients. This is the assertion of the theorem. DEFINITION A polynomial is said to be integer monic if all its coefficients are integers and its highest coefficient is 1. Thus an integer monic polynomial is merely one of the form x\" + a1xn-i + · · · + an where the a's are integers. Clearly an integer monic polynomial is primitive. COROLLARY If an integer monic polynomial factors as the product of two non- constant polynomials having rational coefficients then it factors as the product of two integer monic polynomials. We leave the proof of the corollary as an exercise for the reader. The question of deciding whether a given polynomial is irreducible or not can be a difficult and laborious one. Few criteria exist which declare that a given polynomial is or is not irreducible. One of these few is the following result: THEOREM 3.10.2 (THE EISENSTEIN CRITERION) Letf(x) = a0 + a1x + a2x 2 + · · · + anxn be a polynomial with integer coefficients. Suppose that for some prime number p, p ,{'an, p I a 1 , pI a2 , ••. , p I a0 , p 2 ,{' a0 • Then f (x) is irreducible over the rationals. Proof. Without loss of generality we may assume thatf (x) is primitive, for taking out the greatest common factor of its coefficients does not disturb the hypotheses, since p ,{'an. Iff (x) factors as a product of two rational polynomials, by Gauss' lemma it factors as the product of two polynomials having integer coefficients. Thus if we assume that f (x) is reducible, then f(x) = (b0 + b1x + · · · + b,.xr)(c0 + c1x + · · · + c5x5 ), where the b's and c's are integers and where r > 0 and s > 0. Reading off Sec. 3.11 Polynomial Rings over Commutative Rings the coefficients we first get a0 = h0c0 . Since p I a0 , p must divide one of b0 or c0 . Since p2 -r a0 , p cannot divide both h0 and c0 . Suppose that p I b0 , p% c0 • Not all the coefficients h0 , ••. , b, can be divisible by p; otherwise all the coefficients off (x) would be divisible by p, which is manifestly false since p -r an. Let bk be the first b not divisible by p, k ::; r < n. Thus pI bk_ 1 and the earlier b's. But ak = bkco + bk_ 1c1 + bk_ 2c2 + · · · + h0ck, and p I ak, pI bk_ 1 , bk_ 2 , ••• , h0 , so that p I bkc0 . However, p -r c0 , p -r bk, which conflicts with pI bkc0 . This contradiction proves that we could not have factoredf (x) and so f (x) is indeed irreducible. Problems 1. Let D be a Euclidean ring, F its field of quotients. Prove the Gauss Lemma for polynomials with coefficients in D factored as products of polynomials with coefficients in F. 2. If p is a prime number, prove that the polynomial xn - p is irreducible over the rationals. 3. Prove that the polynomial 1 + x + · · · + xp- 1 , where p is a prime number, is irreducible over the field of rational numbers. (Hint: Con- sider the polynomial!+ (x + 1) + (x + 1) 2 + · · · + (x + l)P-1, and use the Eisenstein criterion.) 4. If m and n are relatively prime integers and if (x - ;} (ao + a1x + · · · + a,X), where the a's are integers, prove that m I a0 and n I a,. 5. If a is rational and x - a divides an integer monic polynomial, prove that a must be an integer. 3.11 Polynomial Rings over Commutative Rings In defining· the polynomial ring in one variable over a field F, no essential use was made of the fact that F was a field; all that was used was that F was a commutative ring. The field nature ofF only made itself felt in proving that F[x] was a Euclidean ring. Thus we can imitate what we did with fields for more general rings. While some properties may be lost, such as \"Euclideanism,\" we shall see that enough remain to lead us to interesting results. The subject could have been developed in this generality from the outset, and we could have obtained the particular results about F[x] by specializing the ring to be a field. However, we felt that it would be healthier to go from the concrete to the abstract rather than from the abstract to the concrete. The price we 161 162 Ring Theory Ch. 3 pay for this is repetition, but even that serves a purpose) namely, that of consolidating the ideas. Because of the experience gained in treating polynomials over fields, we can afford to be a little sketchier in the proofs here. Let R be a commutative ring with unit element. By the polynomial ring in x over R, R[x], we shall mean the set of formal symbols a0 + a1x+ · · · + a 111X 111, where a0 , a 1, .•. , a 111 are in R, and where equality, addition, and multiplication are defined exactly as they were in Section 3.9. As in that section, R[x] is a commutative ring with unit element. We now define the ring of polynomials in the n-variables x1 , ..• , xn over R, R[x1 , ••• , xn], as follows: Let R 1 = R[x1], R 2 = R 1 [x2 ], the polynomial ring in x2 over R 1, .•. , Rn = Rn_ 1 [xn]. Rn is called the ring of polynomials in x1 , ••• , xn over R. Its elements are of the form L:ai 1 i 2 ••• i\"x1 i 1x2 h · · · x/\", where equality and addition are defined coefficientwise and where multipli- cation is defined by use of the distributive law and the rule of exponents (x 1 i1x2 i 2 • • ·xni\")(x/ 1x/ 2 • • ·x/\") = x1it+itx2 i 2 +h · · ·x/\"+in. Of particular importance is the case in which R = F is a field; here we obtain the ring of polynomials in n-variables over a field. Of interest to us will be the influence of the structure of R on that of R[x1 , •• • , xnJ. The first result in this direction is LEMMA 3.11.1 If R is an integral domain, then so is R[x]. Proof. For 0 =If (x) = a0 + a1x + · · · + a 111x 111, where a 111 =I 0, in R[x], we define the degree off (x) to be m; thus deg f (x) is the index of the highest nonzero coefficient ofj(x). If R is an integral domain we leave it as an exercise to prove that deg (f (x)g(x)) = degf (x) + deg g(x). But then, for f (x) =I 0, g(x) =I 0, it is impossible to have f (x) g(x) = 0. That 1s, R[x] is an integral domain. Making successive use of the lemma immediately yields the COROLLARY If R is an integral domain, then so is R[x1 , .•. , xnJ. In particular, when Fis a field, F[x1, ••• , xn] must be an integral domain. As such, we can construct its field of quotients; we call this the field of rational functions in x1 , ••• , xn over F and denote it by F(xv ... , xn)· This field plays a vital role in algebraic geometry. For us it shall be of utmost im- portance in our discussion, in Chapter 5, of Galois theory. However, we want deeper interrelations between the structures of Rand of R[x1 , ••• , xn] than that expressed in Lemma 3.11.1. Our development now turns in that direction. Exactly in the same way as we did for Euclidean rings, we cari speak about divisibility, units, etc., in arbitrary integral domains, R, with unit element. Two elements a, b in Rare said to be associates if a = ub where u Sec. 3.11 Polynomial Rings over Commutative Rings is a unit in R. An element a which is not a unit in R will be called irreducible (or a prime element) if, whenever a = be with b, c both in R, then one of b or c must be a unit in R. An irreducible element is thus an element which cannot be factored in a \"nontrivial\" way. DEFINITION An integral domain, R, with unit element IS a unzque factorization domain if a. Any nonzero element in R is either a unit or can be written as the product of a finite number of irreducible elements of R. b. The decomposition in part (a) is unique up to the order and associates of the irreducible elements. Theorem 3. 7.2 asserts that a Euclidean ring is a unique factorization domain. The converse, however, is false; for example, the ring F[x 1 , x2 ], where F is a field, is not even a principal ideal ring (hence is certainly not Euclidean), but as we shall soon see it is a unique factorization domain. In general commutative rings we may speak about the greatest common divisors of elements; the main difficulty is that these, in general, might not exist. However, in unique factorization domains their existence is assured. This fact is not difficult to prove and we leave it as an exercise; equally easy are the other parts of LEMMA 3.11 .2 If R is a unique factorization domain and if a, b are in R, then a and b have a greatest common divisor (a, b) in R. Moreover, if a and b are relatively prime (i.e., (a, b) = 1), whenever a I be then a I c. ..,. COROLLARY If a E R is an irreducible element and a I be, then a I bora I c. We now wish to transfer the appropriate version of the Gauss lemma (Theorem 3.10.1), which we proved for polynomials with integer co- efficients, to the ring R[x], where R is a unique factorization domain. Given the polynomial J(x) = a0 + a1x + · · · + amxm in R[x], then the content off (x) is defined to be the greatest common divisor of a0 , a1, ••• , am. It is ..lnique within units of R. We shall denote the content off (x) by c(f). A polynomial in R[x] is said to be primitive if its content is 1 (that is, is a unit in R). Given any polynomial] (x) E R[x], we can write] (x) = aft (x) where a= c(f) and wheref1 (x) E R[x] is primitive. (Prove!) Except for multiplication by units of R this decomposition off (x), as an element of R by a primitive polynomial in R[x], is unique. (Prove!) The proof of Lemma 3.1 0.1 goes over completely to our present situation; the only change that must be made in the proof is to replace the prime number p by an irreducible element of R. Thus we have 163 164 Ring Theory Ch. 3 LEMMA 3.11.3 lf R is a unique factorization domain, then the product of two primitive polynomials in R[x] is again a primitive polynomial in R[x]. Given f (x), g(x) in R[x] we can write f (x) = af1 (x), g(x) = bg1 (x), where a = c(f), b = c(g) and where f 1 (x) and g1 (x) are primitive. Thus f(x)g(x) = abf1 (x)g1 (x). By Lemma 3.11.3,f1 (x)g1 (x) is primitive. Hence the content off (x)g(x) is ab, that is, it is c(f)c(g). We have proved the COR 0 LLA RY lf R is a unique factorization domain and iff ( x), g ( x) are in R[x], then c(fg) = c(f)c(g) (up to units). By a simple induction, the corollary extends to the product of a finite number of polynomials to read c(f1 f 2 · · · fk) = c(f1 )c(f2) · · · c(fk). Let R be a unique factorization domain. Being an integral domain, by Theorem 3.6.1, it has a field of quotients F. We can consider R[x] to be a subring of F[x]. Given any polynomialf (x) E F[x], thenf (x) = (f0 (x)Ja), where f 0 (x) E R[x] and where a E R. (Prove!) It is natural to ask for the relation, in terms of reducibility and irreducibility, of a polynomial in R[ x] considered as a polynomial in the larger ring F [ x] LEMMA 3.11.4 lf f(x) in R[x] is both primitive and irreducible as an element of R[x], then it is irreducible as an element of F[x]. Conversely, if the primitive element f ( x) in R[ x] is irreducible as an element ofF [ x], it is also irreducible as an element of R[x]. Proof. Suppose that the primitive elementf(x) in R[x] is irreducible in R[x] but is reducible in F[x]. Thusf (x) = g(x)h(x), where g(x), h(x) are in F[x] and are of positive degree. Now g(x) = (g0 (x)Ja), h(x) = (h0 (x)fb), where a, b E R and where g0 (x), h0 (x) E R[x]. Also g0 (x) = ag1 (x), h0 (x) = Ph 1 (x), where e< = c(g0 ), P = c(h0 ), and g1 (x), h1 (x) are primitive in R[x]. Thus f(x) = (apjab)g1 (x)h 1 (x), whence abf(x) = apg1 (x)h 1 (x). By Lemma 3.11.3, g1 (x)h 1 (x) is primitive, whence the content of the right- hand side is ap. Sincef (x) is primitive, the content of the left-hand side is ab; but then ab = ap; the implication of this is thatf(x) = g1 (x)h 1 (x), and we have obtained a nontrivial factorization off (x) in R[x], contrary to hypothesis. (Note: this factorization is nontrivial since each of g 1 (x), h1 (x) are of the same degree as g(x), h(x), so cannot be units in R[x] (see Problem 4).) We leave the converse halfofthe lemma as an exercise. L EM M A 3.11 . 5 lf R is a unique factorization domain and if p ( x) is a primitive polynomial in R[ x], then it can be factored in a unique way as the product of irreducible elements in R[ x]. ' Proof. When we consider p(x) as an element in F[x], by Lemma 3.9.5,. we can factor it as p(x) = p 1 (x) · · ·pk(x), where p1 (x),p 2 (x), ... ,pk(x) are Sec. 3.11 Polynomial Rings over Commutative Rings irreducible polynomials in F[x]. Each Pi(x) = (fi(x)fai), where fi(x) E R[x] and ai E R; moreover, !;,(x) = ciqi(x), where ci = c(fi) and where qi(x) is primitive in R[x]. Thus each Pi(x) = (ciqi(x)fai), where ai, ci E R and where qi(x) E R[ x] is primitive. Since Pi(x) is irreducible in F[ x], qi(x) must also be irreducible in F[x], hence by Lemma 3.11.4 it is irreducible in R[x]. Now whence a1 a2 • • • akp(x) = c1 c2 • • • ckq1 (x) · · · qk(x). Using the primitivity of p(x) and of q1 (x) · · · qk(x), we can read off the content of the left-hand side as a1a2 • • • ak and that of the right-hand side as c1c2 • • • ck. Thus a1a2 • • • ak = c1c2 • • ·ck, hence p(x) = q1 (x) · · · qk(x). We have factored p(x), in R[x], as a product of irreducible elements. Can we factor it in another way? If p(x) = r1 (x) · · · rk(x), where the ri(x) are irreducible in R[x], by the primitivity of p(x), each ri(x) must be primitive, hence irreducible in F[x] by Lemma 3.11.4. But by Lemma 3.9.5 we know unique factorization in F[x]; the net result of this is that the ri(x) and the qi(x) are equal (up to associates) in some order, hence p(x) has a unique factorization as a product ofirreducibles in R[x]. We now have all the necessary information to prove the principal theorem of this section. THEOREM 3.11.1 IJRisauniquefactorization domain, then so is R[x]. Proof. Letf(x) be an arbitrary element in R[x]. We can writef(x)\"in a unique way as f(x) = cf1 (x) where c = c(f) is in R and where f 1 (x), in R[x], is primitive. By Lemma 3.11.5 we can decomposef1 (x) in a unique way as the product of irreducible elements of R[x]. What about c? Suppose that c=a 1 (x)a 2 (x)···am(x) in R[x]; then O=degc= deg (a 1 (x)) + deg (a2 (x)) + · · · + deg (am(x)). Therefore, each ai(x) must be of degree 0, that is, it must be an element of R. In other words, the only ftctorizations of cas an element of R[x] are those it had as an element of R. In particular, an irreducible element in R is still irreducible in R[x]. Since R is a unique factorization domain, c has a unique factorization as a product of irreducible elements of R, hence of R[ x]. Putting together the unique factorization off (x) in the form cf1 (x) where ft (x) is primitive and where c E R with the unique factorizability of c and off1 (x) we have proved the theorem. Given R as a unique factorization domain, then R 1 = R[ x1] is also a unique factorization domain. Thus R 2 = R 1 [x2 ] = R[x1 , x2 ] is also a unique factorization domain. Continuing in this pattern we obtain 165 166 Ring Theory Ch. 3 COROLLARY 1 If R is a unique factorization domain then so is R[x1 , ••• , xn]. A special case of Corollary 1 but of independent interest and importance is COROLLARY 2 IfF is a field then F[x1 , ••• , xn] is a unique factorization domain. Problems 1. Prove that R[x] is a commutative ring with unit element whenever R is. 2. Prove that R[x1 , ••• , xn] = R[xi1 , ••• , xd, where (i1, ••• , in) is a permutation of (1, 2, ... , n). 3. If R is an integral domain, prove that for f(x), g(x) in R[x], deg (f(x)g(x)) = deg (f(x)) + deg (g(x)). 4. If R is an integral domain with unit element, prove that any unit in R[ x] must already be a unit in R. 5. Let R be a commutative ring with no nonzero nilpotent elements (that is, an= 0 implies a= 0). lfj(x) = a0 + a1x + · · · + amx\"' in R[x] is a zero-divisor, prove that there is an element b =I= 0 in R such that ba0 = ba1 = · · · = bam = 0. *6. Do Problem 5 dropping the assumption that R has no nonzero nilpotent elements. *7. If R is a commutative ring with unit element, prove that a0 + a1x + · · · + anxn in R[x] has an inverse in R[x] (i.e., is a unit in R[x]) if and only if a0 is a unit in R and a 1, ••• , an are nilpotent elements in R. 8. Prove that when F is a field, F[x1 , x2 ] is not a principal ideal ring. 9. Prove, completely, Lemma 3.11.2 and its corollary. 10. (a) If R is a unique factorization domain, prove that every f (x) E R[x] can be written as f (x) = aft (x), where a E R and where f 1 (x) is primitive. (b) Prove that the decomposition in part (a) is unique (up to associates). 11. If R is an integral domain, and ifF is its field of quotients, prove that any elementf(x) in F[x] can be written asf(x) = (f0 (x)fa), where fo(x) E R[x] and where a E R. 12. Prove the converse part of Lemma 3.11.4. 13. Prove Corollary 2 to Theorem 3.11.1. 14. Prove that a principal ideal ring is a unique factorization domain. 15. If J is the ring of integers, prove that J[x1 , ••• , xn] is a unique fac- torization domain. Sec. 3.11 Polynomial Rings over Commutative Rings ;Supplementary Problems I. Let R be a commutative ring; an ideal P of R is said to be a prime ideal of R if ab E P, a, b E R implies that a E P or b E P. Prove that P is a prime ideal of R if and only if RfP is an integral domain. 2. Let R be a commutative ring with unit element; prove that every maximal ideal of R is a prime ideal. 3. Give an example of a ring in which some prime ideal is not a maximal ideal. 4. If R is a finite commutative ring (i.e., has only a finite number of elements) with unit element, prove that every prime ideal of R is a maximal ideal of R. 5. IfF is a field, prove that F[x] is isomorphic to F[t]. 6. Find all the automorphisms (J of F[x] with the property that (J(j) = f for every f E F. 7. If R is a commutative ring, let N = {x E R I xn = 0 for some integer n }. Prove (a) N is an ideal of R. (b) In R = Rf N if xm = 0 for some m then x = 0. 8. Let R be a commutative ring and suppose that A 1s an ideal of R. Let N(A) = {x E R I xn E A for some n}. Prove (a) N(A) is an ideal of R which contains A. (b) N(N(A)) = N(A). N(A) is often called the radical of A. 9. If n is an integer, let Jn be the ring of integers mod n. Describ~ N (see Problem 7) for Jn in terms of n. 10. If A and B are ideals in a ring R such that A n B = (0), prove that for every a E A, b E B, ab = 0. 11. If R is a ring, let Z(R) = {x E R I xy = yx ally E R}. Prove that Z (R) is a subring of R. 12. If R is ·a division ring, prove that Z (R) is a field. 13. F\\nd a polynomial of degree 3 irreducible over the ring of integers, ] 3 , mod 3. Use it to construct a field having 27 elements. 14. Construct a field having 625 elements. 15. IfF is a field and p(x) E F[x], prove that in the ring R = F[x] (p(x))' N (see Problem 7) is (0) if an only if p(x) is not divisible by the square of any polynomial. 167 168 Ring Theory Ch. 3 16. Prove that the polynomial! (x) = 1 + x + x3 + x 4 is not irreducible over any field F. 17. Prove that the polynomial f (x) = x4 + 2x + 2 is irreducible over the field of rational numbers. 18. Prove that ifF is a finite field, its characteristic must be a prime number p and F contains pn elements for some integer. Prove further that if a E F then aPn = a. 19. Prove that any nonzero ideal in the Gaussian integers J[i] must contain some positive integer. 20. Prove that if R is a ring in which a 4 = a for every a E R then R must be commutative. 21. Let Rand R' be rings and 4> a mapping from R into R' satisfying (a) <f>(x + y) = <f>(x) + <f>(y) for every x,y E R. (b) <f>(xy) = <f>(x) <f>(y) or <f>(y) <f>(x). Prove that for all a, bE R, <f>(ab) = <f>(a)<f>(b) or that, for all a, bE R, ¢(a) = <f>(b)<f>(a). (Hint: If a E R, let wa = {x E R I ¢(ax) = <f>(a)<f>(x)} and ua = {x E R I ¢(ax) = <f>(x)<f>(a) }.) 22. Let R be a ring with a unit element, 1, in which (ab) 2 = a 2 b 2 for all a, b E R. Prove that R must be commutative. 23. Give an example of a noncommutative ring (of course, without 1) in which (ab) 2 = a 2 b 2 for all elements a and b. 24. (a) Let R be a ring with unit element 1 such that (ab) 2 = (ba) 2 for all a, bE R. If in R, 2x = 0 implies x = 0, prove that R must be commutative. (b) Show that the result of (a) may be false if 2x = 0 for some x =f. 0 in R. (c) Even if 2x = 0 implies x = 0 in R, show that the result of (a) may be false if R does not have a unit element. 25. Let R be a ring in which xn = 0 implies x = 0. If (ab) 2 = a 2b 2 for all a, b E R, prove that R is commutative. 26. Let R be a ring in which xn = 0 implies x = 0. If (ab) 2 = (ba) 2 for all a, bE R, prove that R must be commutative. 27. Let p1 , p2 , ••• , Pk be distinct primes, and let n = p1p2 • • • Pk· If R is the ring of integers modulo n, show that there are exactly 2k elements a in R such that a 2 = a. 28. Construct a polynomial q(x) =f. 0 with integer coefficients which has no rational roots but is such that for any prime p we can solve the congruence q(x) = 0 mod p in the integers. Sec. 3.11 Polynomial Rings over Commutative Rings ~Supplementary Reading ;,:f:. ~·· ;~ZAJUSKI, OscAR, and SAMUEL, PIERRE, Commutative Algebra, Vol. 1. Princeton, New ~~ Jersey: D. Van Nostrand Company, Inc., 1958. I ',·····M· .. ·• cCov, N.H., Rings and Ideals, Carus Monograph No.8. La Salle, Illinois: Open ~? Court Publishing Company, 1948. ~: ,£:Topic for Class Discussion ff,, t';)IIOTZKIN, T., \"The Euclidean algorithm,\" Bulletin of the American Mathematical Society, Vol. 55 (1949), pages 1142-1146. 169 4 Vector Spaces and Modules Up to this point we have been introduced to groups and to rings; the former has its motivation in the set of one-to-one mappings of a set onto itself, the latter, in the set of integers. The third algebraic model which we are about to consider-vector space-can, in large part, trace its origins to topics in geometry and physics. Its description will be reminiscent of those of groups and rings-in fact, part of its structure is that of an abelian group-but a vector space differs from these previous two structures in that one of the products defined on it uses elements outside of the set itself. These remarks will become clear when we make the definition of a vector space. Vector spaces owe their importance to the fact that so many models arising in the solutions of specific problems turn out to be vector spaces. For this reason the basic concepts introduced in them have a certain universality and are ones we encounter, and keep encountering, in so many diverse contexts. Among these fundamental notions are those of linear dependence, basis, and dimension which will be de- veloped in this chapter. These are potent and effective tools in all branches of mathematics; we shall make immediate and free use of these in many key places in Chapter 5 which treats the theory of fields. Intimately intertwined with vector spaces are the homomorphisms of one vector space into another (or into itself). These will make up the bulk of the subject matter to be considered in Chapter 6. In the last part of the present chapter we generalize from vector spaces 170 1 Sec. 4.1 Elementary Basic Concepts to modules; roughly speaking, a module is a vector space over a ring instead f over a field. For finitely generated modules over Euclidean rings we hall prove the fundamental basis theorem. This result allows us to give a omplete description and construction of all abelian groups which are enerated by a finite number of elements. Elementary Basic Concepts A nonempty set V is said to be a vector space over a field F V is an abelian group under an operation which we denote by +, and 'if for every a E F, v E V there is defined an element, written av, in V subject to 1. a ( V + W) = av + aw; 2. (a + f3)v = av + f3v; 3. a(f3v) = (af3)v; for all a, f3 E F, v, wE V (where the 1 represents the unit element of F multiplication). Note that in Axiom 1 above the + is that of V, whereas on the left-hand side of Axiom 2 it is that ofF and on the right-hand side, that of V. We shall consistently use the following notations: Lowercase Greek letters will be elements ofF; we shall often refer to elements ofF as scalars. Capital Latin letters will denote vector spaces over F. Lowercase Latin letters will denote elements of vector spaces. We shall often call elements of a vector space vectors. If we ignore the fact that V has two operations defined on it and view it for a moment merely as an abelian group under +, Axiom 1 states nothing more than the fact that multiplication of the elements of V by a fixed scalar rt defines a homomorphism of the abelian group V into itself. From Lemma 4.1.1 which is to follow, if a \"# 0 this homomorphism can be shown to be an isomorphism of V onto V. This suggests that many aspects of the theory of vector spaces (and of rings, too) could have been developed as a part of the theory of groups, had we generalized the notion of a group to that of a group with operators . . For students already familiar with a little abstract algebra, this is the pre- ferred point of view; since we assumed no familiarity on the reader's part with any abstract algebra, we felt that such an approach might lead to a 171 172 Vector Spaces and Modules Ch. 4 too sudden introduction to the ideas of the subject with no experience to act as a guide. Example 4.1 .1 Let F be a field and let K be a field which contains F as a subfield. We consider K as a vector space over F, using as the + of the vector space the addition of elements of K, and by defining, for r:t. E F, v E K, av to be the products of r:t. and v as elements in the field K. Axioms 1, 2, 3 for a vector space are then consequences of the right-distributive law, left-distributive law, and associative law, respectively, which hold for K as a ring. Example 4.1 .2 Let F be a field and let V be the totality of all ordered n-tuples, (r:t.1, ... , r:t.n) where the r:t.i E F. Two elements (r:t.1, ... , r:t.n) and ({31, .•• , f3n) of V are declared to be equal if and only if r:t.i = f3i for each i = 1, 2, ... , n. We now introduce the requisite operations in V to make of it a vector space by defining: 1. (r:t.1, · · ·' t:i.n) + ({31, · · ·' f3n) = (r:t.1 + /31, li.z + f3z, · · ·' t:i.n + f3n)· 2. '}'(r:t.1, ... , r:t.n) = (yr:t.1, ... , '}'r:t.n) for '}' E F. It is easy to verify that with these operations, V is a vector space over F. Since it will keep reappearing, we assign a symbol to it, namely F<n>. Example 4.1 .3 Let F be any field and let V = F [ x], the set of poly- nomials in x over F. We choose to ignore, at present, the fact that in F[x] we can multiply any two elements, and merely concentrate on the fact that two polynomials can be added and that a polynomial can always be multi- plied by an element of F. With these natural operations F[x] is a vector space over F. Example 4.1.4 In F[ X] let vn be the set of all polynomials of degree less than n. Using the natural operations for polynomials of addition and multiplication, Vn is a vector space over F. What is the relation of Example 4.1.4 to Example 4.1.2? Any element of vn is of the form t:i.o + t:i.1X + ... + t:i.n-1Xn-t, where t:i.i E F; if we map this element onto the element (a0 , r:t.1, ... , r:t.n_ 1) in F(n) we could reasonably expect, once homomorphism and isomorphism have been defined,, to find that vn and F(n) are isomorphic as vector spaces. DEFINITION If Vis a vector space over F and if W c V, then W is a subspace of V if under the operations of V, W, itself, forms a vect0r space over F. Equivalently, W is a subspace of V whenever w 1 , w2 E W, r:t., f3 E F implies that aw1 + {3w2 E W. Sec. 4.1 Elementary Basic Concepts Note that the vector space defined in Example 4.1.4 is a subspace of that defined in Example 4.1.3. Additional examples of vector spaces and subspaces can be found in the problems at the end of this section. If U and V are vector spaces over F then the mapping T U in to V is said to be a homomorphism if (u 1 + u2 ) T = u1 T + u2 T; (ocu1 ) T = oc(u1 T); for all uv u2 E U, and all oc E F. As in our previous models, a homomorphism is a mapping preserving all the algebraic structure of our system. If T, in addition, is one-to-one, we call it an isomorphism. The kernel of Tis defined as {u E U I uT = 0} where 0 is the identity element of the ::~·;.addition in V. It is an exercise that the kernel of T is a subspace of U and •{tiithat Tis an isomorphism if and only if its kernel is (0). Two vector spaces fJ:\\are said to be isomorphic if there is an isomorphism of one onto the other. ~···· The set of all homomorphisms of U into V will be written as Hom ( U, V) . . ;(('Of particular interest to us will be two special cases, Hom ( U, F) and i1'~;}Iom ( U, U). We shall study the first of these soon; the second, which can be l'i¥ahown to be a ring, is called the ring of linear transformations on U. A great ~;~eal of our time, later in this book, will be occupied with a detailed study ~~:or Hom ( U, U). ~ We begin the material proper with an operational lemma which, as i~ ~;~die case of rings, will allow us to carry out certain natural and simpfe t ~\\Computations in vector spaces. In the statement of the lemma, 0 represents lhe zero of the addition in V, o that of the addition in F, and - v the dditive inverse of the element v of V. ~;, ~ ~'-EMMA 4.1 .1 Ij V is a vector space over F then :r,,, . • ocO = 0 for oc E F. · ov =-Ofor v E V. • ( - OC) v = - ( ocv) for oc E F, v E V. If v =I= 0, then ocv = 0 implies that oc = o. The proof is very easy and follows the lines of the analogous ults proved for rings; for this reason we give it briefly and with few planations. . Since ocO = oc(O + 0) = ocO + ocO, we get ocO = 0. ~ Since ov = (o + o)v = ov + ov we get ov = 0. 173 174 Vector Spaces and Modules Ch. 4 3. Since 0 = (o: + ( -o:))v = o:v + ( -o:)v, ( -o:)v = - (o:v). 4. If o:v = 0 and o: i= o then 0 = 0:- 10 = 0:- 1 ( O:V) = ( 0:- 1 0:) V = 1 V = V. The lemma just proved shows that multiplication by the zero of V or of F always leads us to the zero of V. Thus there will be no danger of confusion in using the same symbol for both of these, and we henceforth will merely use the symbol 0 to represent both of them. Let V be a vector space over F and let W be a subspace of V. Considering these merely as abelian groups construct the quotient group VfW; its elements are the cosets v + W where v E V. The commutativity of the addition, from what we have developed in Chapter 2 on group theory, assures us that VfW is an abelian group. We intend to make of it a vector space. If 0: E F, v + wE v;w, define o:(v + W) = O:V + w. As is usual, we must first show that this product is well defined; that is, if v + W = v' + W then o:(v + W) = o:(v' + W). Now, because v + W = v' + W, v - v' is in W; since W is a subspace, o:(v - v') must also be in W. Using part 3 of Lemma 4.1.1 (see Problem 1) this says that o:v - o:v' E W and so o:v + W = o:v' + W. Thus o:(v + W) = o:v + W = o:v' + W = o:(v' + W); the product has been shown to be well defined. The verification of the vector-space axioms for Vf W is routine and we leave it as an exercise. We have shown LEMMA 4.1.2 If Vis a vector space over F and if W is a subspace of V, then v;w is a vector space over F, where, for v1 + w, Vz + wE v;w and 0: E F, 1. ( v1 + W) + ( v2 + W) = ( v1 + v2 ) + W. 2. o:(v1 + W) = o:v1 + W. v;w is called the quotient space of v by w. Without further ado we now state the first homomorphism theorem for vector spaces; we give no proofs but refer the reader back to the proof of Theorem 2. 7 .1. THEOREM 4.1.1 If Tis a homomorphism of U onto V with kernel W, then V is isomorphic to UfW. Conversely, if U is a vector space and W a subspace of U, then there is a homomorphism of U onto Uf W. The other homomorphism theorems will be found as exercises at the end of this section. DEFINITION Let V be a vector space over F and let U1 , ... , Un be subspaces of v. vis said to be the internal direct sum of u1, ... ' un if'every element v E V can be written in one and only one way as v = u1 + u2 + · · · + Un where Ui E Ui. Sec. 4.1 Elementary Basic Concepts Given any finite number of vector spaces over F, V1 , ... , Vm consider the set v of all ordered n-tuples (v1, ... ' vn) where viE vi. We declare two elements (v1 , ... , vn) and (v~, ... , v~) of V to be equal if and only if for ! each i, vi = v;. We add two such elements by defining (vv ... , vn) + ~:,(Wt, ... ' wn) to be (v1 + Wv Vz + Wz, ... ' vn + wn)· Finally, if IX E F ~and (v1, ... , vn) E V We define a(v1, ... , vn) to be (av 1, CWz, ... , avn). ~To check that the axioms for a vector space hold for V with its operations [~as defined above is straightforward. Thus V itself is a vector space over F. 'We call V the external direct sum of V1, . .. , Vn and denote it by writing 1 V=V1 Ee···ffiV. , n :THEOREM 4.1.2 lj V is the internal direct sum rif Uv ... , Um then V is isomorphic to the external direct sum rif u1, ... ' un. Proof. Given v E V, v can be written, by assumption, in one and only one way as V = U1 + Uz + · · · + Un where Ui E Ui; define the mapping T of V into U1 E9 · · · E9 U\" by vT = (uv ... , un)· Since v has a unique ;representation of this form, T is well defined. It clearly is onto, for the :arbitrary element (wv ... ' wn) E ul E9 .•. E9 un is wT where w = w1 + ', · · · + wn E V. We leave the proof of the fact that Tis one-to-one and a homomorphism to the reader. Because of the isomorphism proved in Theorem 4.1.2 we shall henceforth :~erely refer to a direct sum, not qualifying that it be internal or external. Problems 1. In a vector space show that a(v - w) = av - aw. 2. Prove that the vector spaces in Example 4.1.4 and Example 4.1.2 are isomorphic. 3. Prove that the kernel of a homomorphism is a subspace. 4. (a) IfF is a field of real numbers show that the set of real-valued, continuous functions on the closed interval [0, 1] forms a vector space over F. (6) Show that those functions in part (a) for which all nth derivatives exist for n = 1, 2, ... form a subspace. 5. (a) Let F be the field of all real numbers and let V be the set of all sequences (a 1, a2 , ••• , am ... ), ai E F, where equality, addition and scalar multiplication are defined componentwise. Prove that V is a vector space over F. (b) Let W = {(a1, ... , an, ... ) E V jlim an = 0}. Prove that W n-+oo is a subspace of V. 175 176 Vector Spaces and Modules Ch. 4 00 *(c) Let U = {(a 1, ... , a\"' ... ) E VI La/ is finite}. Prove that U is i= 1 a subspace of V and is contained in W. 6. If U and V are vector spaces over F, define an addition and a multipli- cation by scalars in Hom ( U, V) so as to make Hom ( U, V) into a vector space over F. *7. Using the result of Problem 6 prove that Hom (F<n>, F<m>) is isomorphic to F\"m as a vector space. 8. If n > m prove that there is a homomorphism of F<n> onto F(m) with a kernel W which is isomorphic to F(n-m>. 9. If v =f:. 0 E F(n) prove that there is an element T E Hom (F<n>, F) such that v T =P 0. 10. Prove that there exists an isomorphism of F<n> into Hom (Hom (F<n>, F), F). 11. If U and W are subspaces of V, prove that U + W = {v E VI v u + w, u E U, w E W} is a subspace of V. 12. Prove that the intersection of two subspaces of Vis a subspace of V. 13. If A and Bare subspaces of V prove that (A + B)/B is isomorphic to Af(A n B). 14. If Tis a homomorphism of U onto V with kernel W prove that there is a one-to-one correspondence between the subspaces of V and the subspaces of U which contain W. 15. Let V be a vector space over F and let V1, ••• , Vn be subspaces of V. Suppose that V = V1 + V2 + · · · + Vn (see Problem 11), and that Vi n (V1 + · · · + Vi_ 1 + Vi+ 1 + · · · + Vn) = (0) for every i = 1, 2, ... , n. Prove that Vis the internal direct sum of V1 , •.. , Vn. 16. Let V = V1 ffi · · · ffi Vn; prove that in V there are subspaces Vi isomorphic to vi such that vis the internal direct sum of the vi. 17. Let T be defined on F(2) by (xu x2 ) T = (ctx1 + {3x2 , yx1 + Dx2 ) where ct, {3, y, () are some fixed elements in F. (a) Prove that Tis a homomorphism of F(2) into itself. (b) Find necessary and sufficient conditions on ct, {3, y, () so that T is an isomorphism. 18. Let T be defined on F(3) by (xv x2 , x3 ) T = (ct11x1 + ct12x2 + ct13x3 , ct21 x1 + ct22x2 + ct23x3 , ct31x1 + ct32x2 + ct33x3 ). Show that T is a homomorphism of F(3) into itself and determine necessary and sufficient conditions on the ctii so that Tis an isomorphism. Sec. 4.2 linear Independence and Bases 177 19. Let T be a homomorphism of V into W. Using T, define a homomor- phism T* of Hom ( W, F) into Hom ( V, F). 20. (a) Prove that F(1) is not isomorphic to F<n> for n > I. (b) Prove that F<2 > is not isomorphic to F(3>. 21. If Vis a vector space over an irifinite field F, prove that V cannot be written as the set-theoretic union of a finite number of proper subspaces. 4.2 linear Independence and Bases If we look somewhat more closely at two of the examples described in the previous section, namely Example 4.1.4 and Example 4.1.3, we notice that although they do have many properties in common there is one striking difference between them. This difference lies in the fact that in the former we can find a finite number of elements, 1, x, x 2 , ••• , xn- 1 such that every element can be written as a combination of these with coefficients from F, whereas in the latter no such finite set of elements exists. We now intend to examine, in some detail, vector spaces which can be generated, as was the space in Example 4.1.4, by a finite set of elements. DEFINITION If Vis a vector space over F and if v11 ••• , vn E V then any element of the form oc1 v1 + oc2v2 + · · · + ocnvm where the oci E F, is a linear combination over F of v1 , ••• , vn. Since we usually are working with some fixed field F we shall often say linear combination rather than linear combination over F. Similarly it will be understood that when we say vector space we mean vector space over F . ..,. DEFINITION If Sis a nonempty subset of the vector space V, then L(S), the linear span of S, is the set of all linear combinations of finite sets of elements of S. We put, after all, into L(S) the elements required by the axioms of a Vector space,· so it is not surprising to find LEMMA 4.2.1 L(S) is a subspace of V. Proof. If v and w are in L(S), then v = A.1s1 + · · · + A.nsn and w = P.1 t1 + · · · + Jlmtm, where the A.'s and Jl's are in F and the si and ti are all in S. Thus, for oc, P E F, ocv + Pw = oc(A.1s1 + · · · + A.nsn) + P(J11t1 + · · · + Jlmtm) = (ocA.1 )s1 + · · · + (ocA.n)sn + (PJ11 )t1 + · · · + (PJlm)tm and so is again in L(S). L(S) has been shown to be a subspace of V. The proof of each part of the next lemma is straightforward and easy and we leave the proofs as exercises to the reader. 178 Vector Spaces and Modules Ch. 4 LEMMA 4.2.2 IJ S, Tare subsets of V, then 1. S c T implies L(S) c L( T). 2. L(S u T) = L(S) + L( T). 3. L(L(S)) = L(S). DEFINITION The vector space Vis said to be finite-dimensional (over F) if there is a finite subset S in V such that V = L(S). Note that F(n) is finite-dimensional over F, for if S consists of the n vectors (1, 0, ... , 0), (0, 1, 0, ... , 0), ... , (0, 0, ... , 0, 1), then V = L(S). Although we have defined what is meant by a finite-dimensional space we have not, as yet, defined what is meant by the dimension of a space. This will come shortly. DEFINITION If Vis a vector space and if v1, ... , vn are in V, we say that they are linearly dependent over F if there exist elements A1, ••• , An in F, not all of them 0, such that A1v1 + A2v2 + · · · + Anvn = 0. If the vectors v1, ••• , vn are not linearly dependent over F, they are said to be linearly independent over F. Here too we shall often contract the phrase \"linearly dependent over F\" to \"linearly dependent.\" Note that if v1, ••• , vn are linearly independent then none of them can be 0, for if v1 = 0, say, then cx:v1 + Ov2 + · · · + Ovn = 0 for any a i= 0 in F. In F(3) it is easy to verify that (1, 0, 0), (0, 1, 0), and (0, 0, 1) are linearly independent while (1, 1, 0), (3, 1, 3), and (5, 3, 3) are linearly dependent. We point out that linear dependence is a function not only of the vectors but also of the field. For instance, the field of complex numbers is a vector space over the field of real numbers and it is also a vector space over the field of complex numbers. The elements v1 = 1, v2 = i in it are linearly independent over the reals but are linearly dependent over the complexes, since iv 1 + ( -1)v 2 = 0. The concept of linear dependence is an absolutely basic and ultra- important one. We now look at some of its properties. LEMMA 4.2.3 IJ v1, ... , vn E V are linearly independent, then every element in their linear span has a unique representation in the form A1 v1 + · · · + Anvn with the Ai E F. Proof. By definition, every element in the linear span is of the form Atv1 + · · · + Anvn. To show uniqueness we must demonstrate that if At Vt + ... + AnVn =Ill vl + ... + JlnVn then At = Jll, A2 = J12, ... 'An = Jln· But if At v1 + · · · + Anvn = Ill v1 + · · · + JlnVn, then we certainly have. Sec. 4.2 Linear Independence Bases CA·t - flt)vt + (A.z - flz)Vz + · · · + (A.n - fln)vn = 0, which by the linear independence of Vv ... ' vn forces At - flt = 0, Az - flz = 0, ... ' An - fln = 0. The next theorem, although very easy and at first glance of a somewhat tec:nntie<U nature, has as consequences results which form the very foundations the subject. We shall list some of these as corollaries; the others will appear in the succession of lemmas and theorems that are to follow. lf v1, ••• , vn are in V then either they are linearly independ- ent or some vk is a linear combination rif the preceding ones, v1 , ••• , vk-t· Proof. If v1 , ••• , vn are linearly independent there is, of course, nothing to prove. Suppose then that a 1 v1 + · · · + anvn = 0 where not all the a's are 0. Let k be the largest integer for which ak =I= 0. Since ai = 0 for i > k, a 1 v1 + · · · + akvk = 0 which, since ak =I= 0, implies that vk = ak -t( -atvt - azvz - · · · - ak-tvk-1) = ( -ak - 1a1)v1 + · · · + ( -ak -lak_ 1)vk-t· Thus vk is a linear combination of its predecessors. 1 If v1 , ... , vn in V have W as linear span and if v1 , • •• , vk linearly independent, then we can find a subset rif v1 , ••• , vn rif the form v1 , V2 , • •• , vk, Vi1 , ••• , vir consisting rif linearly independent elements whose linear span is also W. Proof. If v1 , ••• , vn are linearly independent we are done. If not, weed out from this set the first vi, which is a linear combination of its predecessors. Since v1 , ... , vk are linearly independent, J > k. The subset so constructe~, v1 , ••• , vk, ... , vi_ 1, vi+t' ... , vn has n - 1 elements. Clearly its linear span is contained in W. However, we claim that it is actually equal to W; for, given w E W, w can be written as a linear combination of v1 , ••• , vn. But in this linear combination we can replace vi by a linear combination of v1 , ••• , vj-t· That is, w is a linear combination ofvv ... , vi_ 1, vi+ 1, ... ,vn. Continuing this weeding out process, we reach a subset v1 , ..• , vk, Vi1 , ••• , vir whose linear span is still W but in which no element is a linear combination of the preceding ones. By Theorem 4.2.1 the elements vl, . . ~ vk, vit, . .. , vir must be linearly independent. If V is a finite-dimensional vector space, then it contains a set v1 , ••• , vn rif linearly independent elements whose linear span is V. Proof Since V is finite-dimensional, it is the linear span of a finite number of elements uv .. . , um. By Corollary 1 we can find a subset of these, denoted by v1 , ••• , vn, consisting of linearly independent elements Whose linear span must also be V. 179 180 Vector Spaces and Modules Ch. 4 DEFINITION A subset Sofa vector space Vis called a basis of V if S consists of linearly independent elements (that is, any finite number of elements inS is linearly independent) and V = L(S). In this terminology we can rephrase Corollary 2 as COROLLARY 3 If V is a finite-dimensional vector space and if u1 , •.. , urn span V then some subset of u1 , • •• , urn forms a basis of V. Corollary 3 asserts that a finite-dimensional vector space has a basis containing a finite number of elements v1, ... , vn- Together with Lemma 4.2.3 this tells us that every element in V has a unique representation in the form cx1 v1 + · · · + cxnvn with cx1 , ... , cxn in F. Let us see some of the heuristic implications of these remarks. Suppose that Vis a finite-dimensional vector space over F; as we have seen above, V has a basis v1 , ••• , vn. Thus every element v E V has a unique repre- sentation in the form v = cx1 v1 + · · · + e<nvn. Let us map V into F(n) by defining the image of cx1v1 + · · · + cxnvn to be (cx1, ... , cxn)· By the unique- ness of representation in this form, the mapping is well defined, one-to-one, and onto; it can be shown to have all the requisite properties of an iso- morphism. Thus V is isomorphic to F<n> for some n, where in fact n is the number of elements in some basis of V over F. If some other basis of V should have m elements, by the same token V would be isomorphic to F<m>. Since both F(n) and F(m) would now be isomorphic to V, they would be isomorphic to each other. A natural question then arises! Under what conditions on n and m are F<n> and F(m) isomorphic? Our intuition suggests that this can only happen when n = m. Why? For one thing, if F should be a field with a finite number of elements-for instance, ifF = ]p the integers modulo the prime number p-then F(n) has pn elements whereas F(m) has pm elements. Iso- morphism would imply that they have the same number of elements, and so we would haven = m. From another point of view, ifF were the field of real numbers, then F<n> (in what may be a rather vague geometric way to the reader) represents real n-space, and our geometric feeling tells us that n-space is different from m-space for n =1 m. Thus we might expect that ifF is any field then F<n> is isomorphic to F(m) only if n = m. Equiv- alently, from our earlier discussion, we should expect that any two bases of V have the same number of elements. It is towards this goal that we prove the next lemma. LEMMA 4.2.4 If v1 , ••• , vn is a basis of V over F and if w 1 , ••• , U:m in V are linearly independent over F, then m :::;; n. Proof. Every vector in V, so in particular wm, is a linear combination of v1 , ••• , vn. Therefore the vectors wm, v1 , . .. , vn are linearly dependent. Sec. 4.2 linear Independence and Bases Moreover, they span V since v1 , .•• , vn already do so. Thus some proper subset of these wm, V; 1 , ••• , vik with k :::;; n - 1 forms a basis of V. We have \"traded off\" one w, in forming this new basis, for at least one vi. Repeat this procedure with the set wm_ 1 , wm, Vi 1 , ••• , vik· From this linearly dependent set, by Corollary 1 to Theorem 4.2.1, we can extract a basis of the form wm_ 1 , wm, vit, ... , vis' s :::;; n - 2. Keeping up this procedure we eventually get down to a basis of V of the form w 2 , •.• , wm_ 1 , wm, vr;., Vp ... ; since w 1 is not a linear combination of w2 , ••• , wm_ 1 , the above basis must actually include some v. To get to this basis we have introduced m - 1 w's, each such introduction having cost us at least one v, and yet there is a v left. Thus m - 1 :::;; n - 1 and so m :::;; n. This lemma has as consequences (which we list as corollaries) the basic results spelling out the nature of the dimension of a vector space. These corollaries are of the utmost importance in all that follows, not only in this chapter but in the rest of the book, in fact in all of mathematics. The corollaries are all theorems in their own rights. COROLLARY 1 If V is finite-dimensional over F then any two bases of V have the same number of elements. Proof. Let v1 , ••. , vn be one basis of V over F and let wv ... , wm be another. In particular, w1 , ••• , wm are linearly independent over F whence, by Lemma 4.2.4, m :::;; n. Now interchange the roles of the v's and w's and we obtain that n :::;; m. Together these say that n = m. COROLLARY 2 F(n) is isomorphic F(m) if and only if n = m. Proof F(n) has, as one basis, the set of n vectors, (1, 0, ... , 0), (0, 1, 0, ... , 0), ... , (0, 0, ... , 0, 1). Likewise F(m) has a basis containing m vectors. An isomorphism maps a basis onto a basis (Problem 4, end of this section), hence, by Corollary 1, m = n. Corollary 2 puts on a firm footing the heuristic remarks made earlier about the possible isomorphism of F(n) and F<m>. As we saw in those re- marks, Vis isomorphic to F(n) for some n. By Corollary 2, this n is unique, thus ..- COROLLARY 3 If V is finite-dimensional over F then Vis isomorphic to F(n) for a unique integer n ~· in fact, n is the number of elements in any basis of V over F. DEFINITION The integer n in Corollary 3 is called the dimension of V over F. The dimension of V over F is thus the number of elements in any basis of Vover F .. 181 82 Vector Spaces and Modules Ch. 4 We shall write the dimension of V over F as dim V, or, the occasional time in which we shall want to stress the role of the field F, as dimF V. COROLLARY 4 Any two finite-dimensional vector spaces over F of the same dimension are isomorphic. Proof. If this dimension is n, then each is isomorphic to p<n>, hence they are isomorphic to each other. How much freedom do we have in constructing bases of V? The next lemma asserts that starting with any linearly independent set of vectors we can \"blow it up\" to a basis of V. LEMMA 4.2.5 If V is finite-dimensional over F and if u1, ..• , um E V are linearly independent, then we can find vectors um + 1, ••• , um + r in V such that u1, ... ' um, um+1' ... ' um+r is a basis of v. Proof. Since V is finite-dimensional it has a basis; let v1, ••• , vn be a basis of V. Since these span V, the vectors u1, ••• , um, v1, ••• , vn also span V. By Corollary l to Theorem 4.2.1 there is a subset of these of the form u1, ••. , um, Vi 1 , • •• , vir which consists of linearly independent elements which span V. To prove the lemma merely put um+1 = Vi 1 , ••• , um+r = vir' What is the relation of the dimension of a homomorphic image of V to that of V? The answer is provided us by LEMMA 4.2.6 If V is finite-dimensional and if W is a subspace of V, then W is finite-dimensional, dim W ~ dim V and dim Vf W = dim V - dim W. Proof. By Lemma 4.2.4, if n = dim V then any n + 1 elements in V are linearly dependent; in particular, any n + 1 elements in Ware linearly dependent. Thus we can find a largest set of linearly independent elements in W, w 1, •.• , wm and m ~ n. If w E W then w 1, ..• , wm, w is a linearly dependent set, whence rxw + rx1 w1 + · · · + rxmwm = 0, and not all of the rx/s are 0. If rx = 0, by the linear independence of the wi we would get that each rxi = 0, a contradiction. Thus rx # 0, and so w = - rx- 1 ( rx1 w 1 + · · · + rxmwm)· Consequently, w1, •.• , wm span W; by this, W is finite- dimensional over F, and furthermore, it has a basis of m elements, where m ~ n. From the definition of dimension it then follows that dim W ~ dim V. Now, let w1, ••• , wm be a basis of W. By Lemma 4.2.5, we can fill this out to a basis, w1 , ... , wm, v1 , ... , vr of V, where m + r = dim V and m=dimW. Let Zi1 , ... , vr be the images, in V = Vf W, of v1 , ... , vr. Since any vector v E V is of the form v = rx1 w 1 + · · · + rxmwm + fJ1 V1 + · · · + flrvr, Sec. 4.2 linear Independence and Bases 183 then v, the image of v, is of the form v = /31 v1 + · · · + Prvr (since w1 = w2 = · · · = wm = 0). Thus v1, ... , vr span VfW. We claim that they are linearly independent, for if y1v1 + · · · + YrVr = 0 then y1v1 + · · · + YrVr E w, and so YtV1 + ... + YrVr = AtW1 + ... + Amwm, which, by the linear independence of the set wv ... , wm, v1, ... , vr forces y1 = · · · = Yr = A1 = · · · = Am = 0. We have shown that Vf W has a basis of r elements, and so, dim VfW = r =dim V- m = dim V- dim W. COROLLARY If A and B are finite-dimensional subspaces of a vector space V, then A + B is finite-dimensional and dim (A + B) = dim (A) + dim (B) dim (An B). Proof. By the result of Problem 13 at the end of Section 4.1, A+B A ~~ AnB' and since A and B are finite-dimensional, we get that dim (A + B) - dim B = dim (A ; B) = dim (A ~ B) = dim A - dim (A n B). Transposing yields the result stated in the lemma. Problems 1. Prove Lemma 4.2.2. 2. (a) IfF is the field of real numbers, prove that the vectors (1, I, 0, 0), (0, 1, -1, 0), and (0, 0, 0, 3) in F< 4> are linearly independent over F. (b) What conditions on the characteristic ofF would make the three vectors in (a) linearly dependent? 3. If V has a basis of n elements, give a detailed proof that Vis isomorphic to p<n>. ¥.If T is an isomorphism of V onto W, prove that T maps a basis of V onto a basis of W. 5. If Vis finite-dimensional and Tis an isomorphism of V into V, prove that T must map V onto V. 6. If V is finite-dimensional and T is a homomorphism of V onto V, prove that T must be one-to-one, and so an isomorphism. 7. If Vis of dimension n, show that any set of n linearly independent vectors in V forms a basis of V. 184 Vector Spaces and Modules Ch. 4 8. If Vis finite-dimensional and W is a subspace of V such that dim V = dim W, prove that V = W. 9. If V is finite-dimensional and T is a homomorphism of V into itself which is not onto, prove that there is some v # 0 in V such that vT = 0. 10. Let F be a field and let F [ x] be the polynomials in x over F. Prove that F[x] is not finite-dimensional over F. 11. Let vn = {p(x) E F[x] I deg p(x) < n}. Define T by (eto + et1x + ··· + Ctn_ 1xn- 1 )T = Cto + et1 (x + 1) + et2 (x + 1) 2 + · · · + etn_ 1 (x + 1)n- 1 . Prove that Tis an isomorphism of Vn onto itself. 12. Let W = {et0 + et1x + · · · + Ctn_ 1xn- 1 E F[x] I et0 + et1 + · · · + etn_ 1 = 0}. Show that W is a subspace of Vn and find a basis of W over F. 13. Let v1, .•• , vn be a basis of V and let Wv . .. , wn be any n elements in V. Define Ton V by (A,1 v1 + · · · + Anvn) T = A-1 w1 + · · · + AnWn. (a) Show that R is a homomorphism of V into itself. (b) When is T an isomorphism? 14. Show that any homomorphism of V into itself, when V is finite- dimensional, can be realized as in Problem 13 by choosing appropriate elements w1, ... ' wn. 15. Returning to Problem 13, since v1, ..• , vn is a basis of V, each wi = etil v1 + · · · + Ctinvm etii E F. Show that the n 2 elements etii of F determine the homomorphism T. *16. If dimp V = n prove that dimp (Hom (V,V)) = n 2 • 17. If V is finite-dimensional and W is a subspace of V prove that there is a subspace W1 of V such that V = W EB W1 • 4.3 Dual Spaces Given any two vector spaces, V and W, over a field F, we have defined Hom ( V, W) to be the set of all vector space homomorphisms of V into W. As yet Hom ( V, W) is merely a set with no structure imposed on it. We shall now proceed to introduce operations in it which will turn it into a vector space over F. Actually we have already indicated how to do so in the descriptions of some of the problems in the earlier sections. However we propose to treat the matter more formally here. Let S and T be any two elements of Hom ( V, W); this means that these are both vector space homomorphisms of V into W. Recalling the definitio~ I I Sec. 4.3 Dual Spaces 185 of such a homomorphism, we must have (vt + v2 )S = v1S + v2S and (cw1)S = a(vtS) for all vv v2 E V and all a E F. The same conditions also hold forT. We first want to introduce an addition for these elements S and Tin Hom (V, W). What is more natural than to define S + T by declaring v(S + T) = vS + vT for all v E V? We must, of course, verify that S + T is in Hom (V, W). By the very definition of S + T, if vt, v2 E V, then (vt + v2 ) (S + T) = (vt + v2 )S + (vt + v2 ) T; since (vt + v2 )S = vtS + v2S and (vt + v2 ) T = v1 T + v2 T and since addition in W is commutative, we get ( vt + v2 ) ( S + T) = vt S + vt T + v2S + v2 T. Once again invoking the definition of S + T, the right-hand side of this relation becomes vt (S + T) + v2 (S + T); we have shown that (vt + v2 ) (S + T) = ' vt (S + T) + v2 (S + T). A similar computation shows that (av) (S + T) = a(v(S + T)). Consequently S + T is in Hom (V, W). Let 0 be that homomorphism of V into W which sends every element of V onto the zero- element of W; for S E Hom (V, W) let -S be defined by v( -S) = - (vS). It is immediate that Hom ( V, W) is an abelian group under the addition defined above. Having succeeded in introducing the structure of an abelian group on Hom (V, W), we now turn our attention to defining .AS for .A E F and S E Hom (V, W), our ultimate goal being that of making Hom (V, W) into a vector space over F. For A E F and S E Hom (V, W) we define AS by v(.AS) = .A(vS) for all v E V. We leave it to the reader to show that .AS is in Hom ( V, W) and that under the operations we have defined, Hom (V, W) is a vector space over F. But we have no assurance that Hom ( V, W) has any elements other than the zero-homomorphism. Be that as it may, we have proved LEMMA 4.3.1 Hom (V, W) zs a vector space over F under the operations described above. A result such as that ofLemma 4.3.1 really gives us very little information; rather it confirms for us that the definitions we have made are reasonable. We would prefer some results about Hom (V, W) that have more of a bite to them. Such a result is provided us in THEOREM 4.3.1 If V and Ware of dimensions m and n, respectively, over F, then Hom ( V, W) is of dimension mn over F. Proof. We shall prove the theorem by explicitly exhibiting a basis of Hom ( V, W) over F consisting of mn elements. Let vt, ... , vm be a basis of V over F and wt, ... , wn one for W over F. If v E V then v = At vt + · · · + AmVm where At, ... , Am are uniquely de- 186 Vector Spaces and Modules Ch. 4 fined elements ofF; define Tii:V--+ W by vTii = A.iwi. From the point of view of the bases involved we are simply letting vk Tii = 0 for k =I= i and viTii = wi. It is an easy exercise to see that Tii is in Hom (V, W). Since i can be any of 1, 2, ... , m and j any of 1, 2, ... , n there are mn such Tii's. Our claim is that these mn elements constitute a basis of Hom ( V, W) over F. For, let S E Hom (V, W); since viSE W, and since any element in W is a linear combination over F of wi, ••• , Wm viS = oc11 wi + oc12w 2 + · · · + ocinwn, for some oc11 , oc12, ••• , oc1 n in F. In fact, viS= oci1wi + · · · + cxinwn for i = 1, 2, ... , m. Consider S0 = oc11 T11 + oc12 T 12 + · · · + OCtn Tin + oc21 T21 + ... + OC2n T2n + ... + oci1 Til + ... + cxin Tin + ... + ocmi Tmi + · · · + ocmn Tmn· Let us compute vkSo for the basis vector vk. Now vkSo = vk(cxu Tu + · · · + OCmi Tmi + · · · + OCmn Tmn) = OC11 (vk Tu) + cxu(vkTu) + ... + cxmi(vkTmi) + ... + CXmn(vkTmn)· Since vkTij = 0 for i =I= k and vkTki = wi, this sum reduces to vkSo = ock1wi + · · · + cxknwm which, we see, is nothing but VIP. Thus the homomorphisms S0 and S agree on a basis of V. We claim this forces S0 = S (see Problem 3, end of this section). However S0 is a linear combination of the Tii's, whence S must be the same linear combination. In short, we have shown that the mn elements T11 , T 12 , ••• , T 1 m ... , Tmi' ... , Tmn span Hom (V, W) over F. In order to prove that they form a basis of Hom ( V, W) over F there remains but to show their linear independence over F. Suppose that f3u Tu + fJ12 T12 + · · · + Pin Tin + · · · + f3i1 Til + · · · + Pin Tin + · · · + Pmi Tmi + · · · + PmnTmn = 0 with {3ij all in F. Applying this to vk we get 0 = vk(f3u Tu + · · · + f3iiTii + · · · + PmnTmn) = Pk1W1 + Pk2W2 + · · · + Pknwn since vk Tii = 0 for i =I= k and vk,Tki = wi. However, w 1, ..• , wn are linearly independent over F, forcing pki = 0 for all k and j. Thus the Tii are linearly independent over F, whence they indeed do form a basis of Hom ( V, W) over F. An immediate consequence of Theorem 4.3.1 is that whenever V =1= (0) and W =I= (0) are finite-dimensional vector spaces, then Hom ( V, W) does not just consist of the element 0, for its dimension over F is nm ;;::: 1. Some special cases of Theorem 4.3.1 are themselves of great interest and we list these as corollaries. COROLLARY 1 .lfdimp V = m then dimp Hom (V, V) = m 2. Proof. In the theorem put V = W, and so m = n, whence mn = m 2 • COROLLARY 2 .lfdimp V = m then dimp Hom (V, F) = m. Proof. As a vector space F is of dimension 1 over F. Applying the theorem yields dimp Hom ( V, F) = m. J Sec. 4.3 Dual Spaces 187 Corollary 2 has the interesting consequence that if Vis finite-dimensional over Fit is isomorphic to Hom (V, F), for, by the corollary, they are of the same dimension over F, whence by Corollary 4 to Lemma 4.2.4 they must be isomorphic. This isomorphism has many shortcomings! Let us explain. It depends heavily on the finite-dimensionality of V, for if V is not finite-dimensional no such isomorphism exists. There is no nice, formal construction of this isomorphism which holds universally for all vector spaces. It depends strongly on the specialities of the finite-dimensional situation. In a few pages we shall, however, show that a \"nice\" isomorphism does exist for any vector space V into Hom (Hom ( V, F), F). DEFINITION If Vis a vector space then its dual space is Hom (V, F). We shall use the notation V for the dual space of V. An element of V will be called a linear functional on V into F. If V is not finite-dimensional the V is usually too large and wild to be of interest. For such vector spaces we often have other additional structures, · such as a topology, imposed and then, as the dual space, one does not generally take all of our Vbut rather a properly restricted subspace. If Vis finite-dimen- sional its dual space Vis always defined, as we did it, as all of Hom (V, F). In the proof of Theorem 4.3.1 we constructed a basis of Hom ( V, W) using a particular basis of V and one of W. The construction depended crucially on the particular bases we had chosen for V and W, respectively. Had we chosen other bases we would have ended up with a different basis of Hom ( V, W). As a general principle, it is preferable to give proofs, whenever possible, which are basis-free. Such proofs are usually referred to as invariant ones. An invariant proof or construction has the advantage, other than the mere aesthetic one, over a proof or construction using a basis, in that one does not have to worry how finely everything depends on a particular choice of bases. The elements of V are functions defined on V and having their values in F. In keeping with the functional notation, we shall usually write elements of Vas J, g, etc. and denote the value on v E Vas f (v) (rather than as vf). Let V be a finite-dimensional vector space over F and let v1 , .•• , vn be a basis of V; let vi be the element of V defined by vi( vi) = 0 for i =I= j, vi(vi) = 1, and vi((X1Vl + ... + (XiVi + ... + (Xnvn) =(Xi. In fact the vi are nothing but the Tii introduced in the proof of Theorem 4.3.1, for here W = F is one-dimensional over F. Thus we know that v1 , •.• , fJn form a basis of V. We call this basis the dual basis of v1, ••. , vn. If v =I= 0 E V, by . Lemma 4.2.5 we can find a basis of the form v1 = v, v2, ... , vn and so there is an element in V, namely v1 , such that fJ1 (v1 ) = v1 (v) = I =/= 0. We have proved 188 Vector Spaces and Modules Ch. 4 LEMMA 4.3.2 If V is finite-dimensional and v =I= 0 E V, then there zs an elementf E V such thatf (v) =I= 0. In fact, Lemma 4.3.2 is true if V is infinite-dimensional, but as we have no need for the result, and since its proof would involve logical questions that are not relevant at this time, we omit the proof. Let v0 E V, where V is any vector space over F. As f varies over V, and v0 is kept fixed,j (v0 ) defines a functional on V into F; note that we are merely interchanging the role of function and variable. Let us denote this function by Tvo; in other words Tv 0 (j) =f(v0 ) for any jE V. What can we say about Tv/ To begin with, Tv 0 (j + g) = (j + g) (v0 ) = f (vo) + g(vo) = Tv0 (j) + Tv0 (g); furthermore, 'T__v 0 (Aj) = (A_j)(vo) = Aj(vol = ATvo(f). Thus Tvo is in the dual space of V! We write this space as V and refer to it as the second dual of V. :lt: Given any element v E V we can associate with it an element Tv in V. :lt: Define the mapping tjJ: V ~ V by vljl = Tv for every v E V. Is tjJ a homo- morphism of V into V? Indeed it is! For, Tv+w(f) = f (v + w) = f (v) + j(w) = Tv(f) + Tw(f) = (Tv + Tw)(f), and so Tv+w = Tv + Tw, that is, (v + w)t/J = vt/J + wt/J. Similarly for A E F, (A_v)t/J = A_(vt/J). Thus \"\"' tjJ defines a homomorphism of V into V. The construction of tjJ used no basis or special properties of V; it is an example of an invariant construction. When is tjJ an isomorphism? To answer this we must know when vljl = 0, or equivalently, when Tv = 0. But if Tv = 0, then 0 = Tv(f) = j (v) for all f E V. However as we pointed out, without proof, for a general vector space, given v =I= 0 there is an f E V with f (v) =I= 0. We actually proved this when V is finite-dimensional. Thus for V finite-dimensional (and, in fact, for arbitrary V) tjJ is an isomorphism. However, when Vis ~ finite-dimensional tjJ is an isomorphism onto V; when Vis infinite-dimen- sional tjJ is not onto. If Vis finite-dimensional, by the second corollary to Theorem 4.3.1, V and Vare of the same dimension; similarly, Vand Vare of the same dimen- sion; since ljJ is an isomorphism of V into V, the equality of the dimensions forces tjJ to be onto. We have proved LEMMA 4.3.3 If Vis finite-dimensional, then tjJ is an isomorphism of V onto V. We henceforth identify V and V, keeping in mind that this identification is being carried out by the isomorphism ljJ. DEFINITION If W is a subspace of V then the annihilator of W, A(W) = {jE V /J(w) = 0 all wE W}. We leave as an exercise to the reader the verification of the fact that . A(W) is a subspace of V. Clearly if U c W, then A(U) ~ A(W). Sec. 4.3 Dual Spaces 189 Let W be a subspace of V, where V is finite-dimensional. Iff E V let J be the restriction off to W; thus] is defined on W by j ( w) = f ( w) for every wE W. SincejE V, clearly] E W. Consider the mapping T: V-+ W defined by JT =]for f E V. It is immediate that (f + g) T = JT + gT and that (Aj) T = A(jT). Thus T is a homomorphism of V into W. What is the kernel of T? Iff is in the kernel of T then the restriction off to W must be 0; that is, f(w) = 0 for all wE W. Also, conversely, if f ( w) = 0 for all w E W then f is in the kernel of T. Therefore the kernel of Tis exactly A ( W). We now claim that the mapping Tis onto W. What we must show is that given any element h E W, then h is the restriction of some f E V, that is h = J By Lemma 4.2.5, if Wv .•. , wm is a basis of W then it can be expanded to a basis of V of the form w1, ••• , wm, v 1, ••• , vr where r + m = dim V. Let W1 be the subspace of V spanned by v1 , .•. , vr. Thus V = W ffi Wl. If hEW define jE V by: let V E V be written as V = W + w1 , wE w, wl E wl; thenf (v) = h(w). It is easy to see thatfis in Vand that ] = h. Thus h = JT and so T maps V onto W. Since the kernel of T is A(W) by Theorem 4.1.1, W is isomorphic to VJA(W). In particular they have the same dimension. Let m = dim W, n = dim V, and r = dim A(W). By Corollary 2 to Theorem 4.3.1, m =dim W and n =dim V. However, by Lemma 4.2.6 dim VJA(W) =dim V- dim A(W) = n- r, and so m = n - r. Transposing, r = n - m. We have proved THEOREM 4.3.2 If V is finite-dimensional and W is a subspace of V, then W is isomorphic to VJ A ( W) and dim A ( W) = dim V - dim W. COROLLARY A(A(W)) = W. Proof. Remember that in order :f'or the corollary even to make sense, since W c Vand A(A(W)) c V, we have identified V with V. Now W c A(A(W)), for if wE W then wt/J = Tw acts on V by Tw(f) =f(w) and so is 0 for all jE A(W). However, dim A(A(W)) =dim V- dim A(W) (applying the theorem to the vector space V and its subspace A( W)) so that dimA(A(W)) =dim V- dimA(W) =dim V- (dim V- dim W) = dinyW. Since W c A(A(W)) and they are of the same dimension, it follows that W = A(A(W)). Theorem 4.3.2 has application to the study of systems of linear homogeneous equations. Consider the system of m equations inn unknowns allxl + a12x2 + ... + alnxn = 0, a21X1 + a22X2 + · · · + a2nxn = 0, 190 Vector Spaces and Modules Ch. 4 where the aii are in F. \\Ve ask for the number of linearly independent solutions (x1 , ... , xn) there are in p<n> to this system. In p<n> let U be the subspace generated by them vectors (a11 , a12, . .. ,a1n), (a21 , a22 , • •• , a2 n), ... , (am1 , am2 , ••• , amn) and suppose that U is of dimension r. In that case we say the system of equations is of rank r. Let v1 = (1, 0, ... , 0), v2 = (0, 1, 0, ... , 0), ... , vn = (0, 0, ... , 0, 1) be used as a basis of p<n> and let z\\, v2 , • •• , vn be its dual basis in ft<n). Any fE p(n) is of the form f = xliJ1 + XzVz + ... + xnvm where the xi EF. When isfEA(U)? In that case, since (a11 , ... , a 1 n) E U, 0 =f(a11, a12, ... ' aln) =J(a11v1 + · · · + alnvn) = (xlz\\ + XzVz + ... + xnvn)(a11v1 + ... + alnvn) since iJi(rJi) = 0 fori # j and vi( vi) = 1. Similarly the other equations of the system are satisfied. Conversely, every solution (x1, ••• , xn) of the system of homogeneous equations yields an element, x1v1 + · · · + xnvm in A(U). Thereby we see that the number of linearly independent solutions of the system of equations is the dimension of A( U), which, by Theorem 4.3.2 is n - r. We have proved the following: THEOREM 4.3.3 If the system of homogeneous linear equations: a11xl + ... + alnxn = 0, a21x1 + ... + a2nxn = 0, amlxl + ... + amnxn = 0, where aii E F is of rank r, then there are n - r linearly independent solutions in p<n>. COROLLARY If n > m, that is, if the number of unknowns exceeds the number of equations, then there is a solution (x1 , ... , xn) where not all of x1 , ... , xn are 0. Proof. Since U is generated by m vectors, and m < n, r = dim U ~ m < n; applying Theorem 4.3.3 yields the corollary. Problems 1. Prove that A( W) is a subspace of V. 2. If S is a subset of V let A(S) = {fE V lf(s) = 0 all s E S}. Prove that A(S) = A(L(S)), where L(S) is the linear span of S. r r ; Sec. 4.4 Inner Product Spaces 3. If S, T E Hom ( V, W) and viS = vi T for all elements vi of a basis of V, prove that S = T. 4. Complete the proof, with all details, that Hom ( V, W) is a vector space over F. 5. If ljJ denotes the mapping used in the text of V into V, give a complete proof that ljJ is a vector space homomorphism of V into V. 6. If Vis finite-dimensional and v1 =/=- v2 are in V, prove that there is an f E V such thatf (v1 ) =I= f (v2 ). 7. If W1 and W2 are subspaces of V, which is finite-dimensional, describe A(W1 + W2 ) in terms of A(W1 ) and A(W2 ). 8. If vis a finite-dimensional and wl and w2 are subspaces of v, describe A(W1 n W2 ) in terms of A(W1 ) and A(W2 ). 9. IfF is the field of real numbers, find A(W) where (a) W is spanned by (1, 2, 3) and (0, 4, -1). (b) Wisspanned by (0, 0, 1, -1), (2, 1, 1, 0), and (2, 1, 1, -1). I 0. Find the ranks of the following systems of homogeneous linea~ equations over F, the field of real numbers, and find all the solutions. (a) x1 + 2x2 - 3x3 + 4x4 = 0, x1 + 3x2 - x3 = 0, 6x1 + x3 + 2x4 = 0. (b) x1 + 3x2 + x3 = 0, x1 + 4x2 + x3 = 0. (c) x1 + x2 + x3 + x4 + x 5 = 0, x1 + 2x2 = 0, 4x1 + 7x2 + x3 + x4 + x 5 = 0, x2 - x3 - x4 - x 5 = 0. II. Iff and g are in V such that f (v) = 0 implies g(v) = 0, prove that g = A_ffor some A E F. 4.4 Inner Product Spaces In our discussion of vector spaces the specific nature of F as a field, other thavthe fact that it is a field, has played virtually no role. In this section we no longer consider vector spaces V over arbitrary fields F; rather, we restrict F to be the field of real or complex numbers. In the first case V is called a real vector space, in the second, a complex vector space. We all have had some experience with real vector spaces-in fact both analytic geometry and the subject matter of vector analysis deal with these. What concepts used there can we carry over to a more abstract setting? To begin with, we had in these concrete examples the idea of length; secondly we had the idea of perpendicularity, or, more generally, that of 191 192 Vector Spaces and Modules Ch. 4 angle. These became special cases of the notion of a dot product ( oft~n called a scalar or inner product.) Let us recall some properties of dot product as it pertained to the special case of the three-dimensional real vectors. Given the vectors v = (x 1,x2 ,x3 ) and w = (YvY 2 ,y3 ), where the x's andy's are real numbers, the dot prod- uct of v and w, denoted by v · w, was defined as v · w = x1y 1 + x2y 2 + x3y 3 • Note that the length of v is given by .J~ and the angle (J between v and w is determined by v· w cos (J What formal properties does this dot product enjoy? We list a few: 1. v · v ~ 0 and v · v = 0 if and only if v = 0; 2.v·w=w·v; 3. u · (cw + Pw) = a(u · v) + P(u · w); for any vectors u, v, w and real numbers a, p. Everything that has been said can be carried over to complex vector spaces. However, to get geometrically reasonable definitions we must make some modifications. If we simply define v · w = x1y 1 + x2 y 2 + x 3y 3 for v = (x1 , x 2 , x 3 ) and w = (y 1,y2 ,y3 ), where the x's andy's are complex numbers, then it is quite possible that v · v = 0 with v =1=- 0; this is illus- trated by the vector v = ( 1, i, 0). In fact, v · v need not even be real. If, as in the real case, we should want v ~ v to represent somehow the length of v, we should like that this length be real and that a nonzero vector should not have zero length. We can achieve this much by altering the definition of dot product slightly. If iX denotes the complex conjugate of the complex number a, returning to the v and w of the paragraph above let us define v · w = x1 ji1 + x2 ji2 + x3 ji3 • For real vectors this new definition coincides with the old one; on the other hand, for arbitrary complex vectors v =1=- 0, not only is v · v real, it is in fact positive. Thus we have the possibility of intro- ducing, in a natural way, a nonnegative length. However, we do lose something; for instance it is no longer true that v · w = w · v. In fact the exact relationship between these is v · w = w · v. Let us list a few properties of this dot product: l.v·w=w·v; 2. v · v ~ 0, and v · v = 0 if and only if v 0· ' 3. (au + Pv) · w = a(u · w) + P(v · w); 4. u · (av + Pw) = a(u ·v ) + PCu · w); for all complex numbers a, p and all complex vectors u, v, w. We reiterate that in what follows F is either the field of real or complex numbers. ~ ! .. ; . ' ' Sec. 4.4 Inner Product Spaces 193 DEFINITION The vector space V over F is said to be an inner product space if there is defined for any two vectors u, v E V an element (u, v) in F such that 1. (u, v) = (V,U); 2. (u, u) ~ 0 and (u, u) = 0 if and only if u = 0; 3. (ocu + {3v, w) = oc(u, w) + {3(v, w); for any u, v, w E V and oc, {3 E F. A few observations about properties 1, 2, and 3 are in order. A function satisfying them is called an inner product. IfF is the field of complex numbers, property 1 implies that (u, u) is real, and so property 2 makes sense. Using 1 and 3, we see that (u, ocv + {3w) = (ocv + {3w, u) = oc(v, u) + f3(w, u) iX(V,U) + /1(w, u) = iX(u, v) + /1(u, w). We pause to look at some examples of inner product spaces. Example 4.4.1 In p<n> define, for u = (oc1 , •.• , ocn) and v = ({31 , •.• , fin), (u, v) = oc1/11 + oc2/12 + · · · + ocnf1n· This defines an inner product on p<n>. Example 4.4.2 In p(2> define for u = (oc1 , oc2 ) and v = ({31 , {32 ), (u, v) = 2a1/J1 + oc1/J2 + oc2/J1 + a2 /J2 • It is easy to verify that this defines an inner product on F< 2 >. Example 4.4.3 Let V be the set of all continuous complex-valued functions on the closed unit interval [0, 1]. Ifj(t), g(t) E V, define (f (t)' g(t)) = s: f (t) g(t) dt. We leave it to the reader to verify that this defines an inner product on V. For the ·remainder of this section V will denote an inner product space. D(FINITION If v E V then the length of v (or norm of v), written llvll, is defined by II vii = .J (v, v). LEMMA 4.4.1 If u, v E V and oc, {3 E F then (ocu + {3v, ocu + {3v) = aa(u, u) + oc/J(u, v) + iX{3(v, u) + {3/J(v, v). Proof. By property 3 defining an inner product space, ( ocu + {3v, au + {Jv) = oc(u, ocu + pv) + P(v, ocu + Pv); but (u, IXU + {3v) = a(u, u) + /1(u, v) and (v, ocu + {3v) = a(v, u) + /1(v, v). Substituting these in the expression for (au + pv, ocu + {3v) we get the desired resul~. 94 Vector Spaces and Modules Ch. 4 COROLLARY llcwll = lrxlllull. Proof. llrxuJJ 2 = (e<u, rxu) = £&(u, u) by Lemma 4.4.1 (with v = 0). Since e<~ = lacl 2 and (u, u) = llull 2 , taking square roots yields l!rxull = lrxl !lull. We digress for a moment, and prove a very elementary and familiar result about real quadratic equations. LEMMA 4.4.2 If a, b, c are real numbers such that a :> 0 and aA 2 + 2bA + c ~ 0 for all real numbers A, then b2 ~ ac. Proof. Completing the squares, al 2 + 2bl + c = ~(al + b) 2 + (c- b:} Since it is greater than or equal to 0 for all A, in particular this must be true for A = - bfa. Thus c - (b 2 fa) ~ 0, and since a > 0 we get b2 ~ ac. We now proceed to an extremely important inequality, usually known as the Schwarz inequality: THEOREM 4.4.1 If u, v E V then I (u, v) I ~ !lull I! vii. Proof. If u = 0 then both (u, v) = 0 and !lull llvll = 0, so that the result is true there. Suppose, for the moment, that (u, v) is real and u i= 0. By Lemma 4.4.1, for any real number A, 0 ~ (Au + v, AU + v) = A 2 (u, u) + 2(u, v) A + (v, v) Let a = (u, u), b = (u, v), and c = (v, v); for these the hypothesis of Lemma 4.4.2 is satisfied, so that b2 ~ ac. That is, (u, v) 2 ~ (u, u) (v, v); from this it is immediate that I (u,v) I ~ !lull II vii. If a = (u, v) is not real, then it certainly is not 0, so that ufe< is mean- ingful. Now, -' v = - ( u, v) = -- ( u, v) = 1' ( u ) 1 1 e< e< ( u, v) and so it is certainly real. By the case of the Schwarz inequality discussed in the paragraph above, since 11~11 1 = -IJuJJ, lrxl Sec. 4.4 Inner Product Spaces 195 we get 1 < !lull llvll - ltXI ' whence ltXI ::;; !lull llvll· Putting in that Cl = (u, v) we obtain l(u, v)l ::;; II u II II vII, the desired result. Specific cases of the Schwarz inequality are themselves of great interest. We point out two of them. 1. If V = p(n) with (u, v) = tX1/31 + · · · + Cln/3m where u = (tX1, ... , Cln) and v = (/31 , ... , f3n), then Theorem 4.4.1 implies that ltX1,B1 + ·\" + Cln/3n 1 2 ::;; (ltX1I 2 + \"' + ltXnl 2 )(1/311 2 + \"' + l/3nl 2 ). 2. If Vis the set of all continuous, complex-valued functions on [0, 1] with inner product defined by u (t), g(t)) = r f(t) g(t) dt, then Theorem 4.4.1 implies that 1r f(t) g(t) dt1 2 , r lf(t)l 2 dt r lg(t)1 2 dt. The concept of perpendicularity is an extremely useful and important one in geometry. We introduce its analog in general inner product spaces. DEFINITION If u, v E V then u is said to be orthogonal to v if (u, v) = 0. Note that if u is orthogonal to v then v is orthogonal to u, for (v, u) (u, v) = U = 0. DEFINITION If W is a subspace of V, the orthogonal complement of W, Wi, is defined by w.L = {x E Vl(x, w) = 0 for all wE W}. LEMMA 4.4.3 w.t is a subspace of V. \"Proof. If a, bE W.L then for all Cl, {3 E F and all wE W, (Cla + f3b, w) a(a, w) + f3(b, w) = 0 since a, bE W.L. Note that w () w.L = (0), for if wE w () w.L it must be self-orthogonal, that is (w, w) = 0. The defining properties of an inner product space rule out this possibility unless w = 0. One of our goals is to show that V = W + w.t. Once this is done, the remark made above will become of some interest, for it will imply that V is the direct sum of W and W .L. 196 Vector Spaces and Modules Ch. 4 DE FIN ITI 0 N The set of vectors { v J in Vis an orthonormal set if 1. Each vi is oflength 1 (i.e., (vi, vi) = 1). 2. For i =1- j, (vi, vi) = 0. LEMMA 4.4.4 If {vi} is an orthonormal set, then the vectors in {vJ are linearly independent. If w = a 1v1 + · · · + anvm then ai = (w, vi) for i = 1, 2, ... , n. Proof. Suppose that a 1 v1 + a2v2 + · · · + anvn = 0. Therefore 0 = (a1 v1 + · · · + anvm vi) = a 1 (v1, vi) + · · · + an(vm vi). Since (vi, vi) = 0 for j =1- i while (vi, vi) = 1, this equation reduces to ai = 0. Thus the v/s are linearly independent. If w = a 1v1 + · · · + anvn then computing as above yields (w, vi) = !Xi. Similar in spirit and in proof to Lemma 4.4.4 is LEMMA 4.4.5 If {v1 , ... , vn} is an orthonormal set in V and if wE V, then u = w - (w, v1)v1 - (w, v2 )v2 - • • • - (w, vi)vi - · · · - (w, vn)vn is orthogonal to each of v1, v2 , ••• , vn. Proof. Computing (u, vi) for any i :s; n, using the orthonormality of v1 , ••• , vn yields the result. The construction carried out in the proof of the next theorem is one which appears and reappears in many parts of mathematics. It is a basic pro- cedure and is known as the Gram-Schmidt orthogonalization process. Although we shall be working in a finite-dimensional inner product space, the Gram-Schmidt process works equally well in infinite-dimensional situations. THEOREM 4.4.2 Let V be a finite-dimensional inner product space; then V has an orthonormal set as a basis. Proof. Let V be of dimension n over F and let v1, ••• , vn be a basis of V. From this basis we shall construct an orthonormal set of n vectors; by Lemma 4.4.4 this set is linearly independent so must form a basis of V. We proceed with the construction. We seek n vectors w1, ••• , wn each of length 1 such that for i =1- j, (wi, wi) = 0. In fact we shall finally produce them in the following form: w1 will be a multiple of Vv w 2 will be in the linear span of w1 and v2 , w 3 in the linear span of w 1 , w 2 , and v3 , and more generally, wi in the linear span of w1, w2 , ••• , wi_ 1 , vi. Let w - v1 • 1 - 1lv1ll' then (w,, w,) = (u:: II' 11:: II) = llv: 112 (v,, v,) 1' Sec. 4.4 Inner Product Spaces 197 !WJ1erux II w 1 II = 1. We now ask: for what value of a is aw1 + v2 orthogonal w1? All we need is that (aw1 + v2 , w1) = 0, that is a(w1, w1) + '11 2 , w1) = 0. Since (w1, w1) = 1, a = - (v 2 , w1) will do the trick. Let ,z = - (v2, w1)w1 + v2 ; u2 is orthogonal to w1; since v1 and v2 are linearly · dependent, w 1 and v2 must be linearly independent, and so u2 =f:. 0. t w 2 = (u2 /llu2 ll); then {w1 , w2 } is an orthonormal set. We continue. et u3 = - (v3 , w1)w1 - (v3 , w2 )w2 + v3 ; a simple check verifies that '~ 3 , w1 ) = (u 3 , w2 ) = 0. Since w1 , w2 , and v3 are linearly independent ,(for w1 , w 2 are in the linear span of v1 and v2 ), u3 =f:. 0. Let w 3 = (u3 fllu 3 ll); ~tpen {w1 , w2 , w3 } is an orthonormal set. The road ahead is now clear. ~-uppose that we have constructed Wu w 2, ... , wi, in the linear span of f;, 1, •.. , vi, which form an orthonormal set. How do we construct the next 'pne, wi+l? Merely put ui+l = - (vi+l' w1)w1 - (vi+l' w 2)w2 - · · · - Xvi+1, wi)wi + vi+t· That ui+t =f:. 0 and that it is orthogonal to each of to1, ..• , wi we leave to the reader. Put wi+l = (ui+t/llui+tll)! *' In this way, given r linearly independent elements in V, we can construct an orthonormal set having r elements. If particular, when dim V = n, ''from any basis of V we can construct an orthonormal set having n elements. ;;This provides us with the required basis for V. We illustrate the construction used in the last proof in a concrete case. Let F be the real field and let V be the set of polynomials, in a variable x, over F of degree 2 or less. In V we define an inner product by: if p(x), q(x) E V, then (p(x), q(x)) = r/(x)q(x) dx. Let us start with the basis v1 = 1, v2 = x, v3 = x 2 of V. Following me construction used, u2 = - (v2, wl)wt + v2, whicfi after the computations reduces to u2 = x, and so finally, 198 Vector Spaces and Modules Ch. 4 and so We mentioned the next theorem earlier as one of our goals. We are now able to prove it. THEOREM 4.4.3 If Vis a finite-dimensional inner product space and if W is a subspace of V, then V = W + Wj_. More particularly, V is the direct sum rif wand wj_. Proof. Because of the highly geometric nature of the result, and because it is so basic, we give several proofs. The first will make use of Theorem 4.4.2 and some of the earlier lemmas. The second will be motivated geo- metrically. First Proof. As a subspace of the inner product space V, W is itself an inner product space (its inner product being that of V restricted to W). Thus we can find an orthonormal set Wv ... , wr in W which is a basis of W. If v E V, by Lemma 4.4.5, v0 = v - (v, w1)w1 - (v, w2 )w2 - • • • - (v, wr)wr is orthogonal to each of w1, ••• , wr and so is orthogonal to W. Thus v0 E Vfl\\ and since v = v0 + ((v, w1)w1 + · · · + (v, wr)wr), v E W + Wj_. Therefore V = W + Wj_. Since W n Wj_ = (0), this sum is direct. Second Proof. In this proof we shall assume that F is the field of real numbers. The proof works, in almost the same way, for the complex numbers; however, it entails a few extra details which might tend to obscure the essential ideas used. Let v E V; suppose that we could find a vector w0 E W such that llv - w0 ll ~ llv - wll for all wE W. We claim that then (v - w0 , w) = 0 for all WE W, that is, V - Wo E Wj_. If w E W, then w0 + w E W, in consequence of which (v - w0 , v - w0 ) ~ (v - (w0 + w), v - (w0 + w)). However, the right-hand side is (w, w) + (v - w0 , v - w0 ) - 2(v - w0 , w), leading to 2(v - w0 , w) ~ (w, w) for all wE W. If m is any positive integer, since wfm E W we have that - (v - w0 , w) = 2 v - w0 ,- ~ -,- = - (w, w), 2 ( w) (ww) 1 m m m m m 2 and so 2(v - w0 , w) ~ (1/m)(w, w) for any positive integer m. However, Sec. 4.4 Inner Product Spaces 199 ·{1/m)(w, w) ~ 0 as m ~ oo, whence 2(v- w0 , w) ~ 0. Similarly, -wE W, and so 0 ~ -2(v - w0 , w) = 2(v - w0 , -w) ~ 0, yielding (v - w0 , w) ;::: 0 for all w E w. Thus v - Wo E wl.; hence v E Wo + wl. c w + wl.. , To finish the second proof we must prove the existence of a w0 E W ~such that II v - w0 II ~ II v - w II for all w E W. We indicate sketchily two !1ways of proving the existence of such a w0 • ? Let uv ... , uk be a basis of W; thus any wE W is of the form w = '.,t1u1 + · · · + J..kuk. Let f3ii = (u;, uj) and let Yi = (v, ui) for v E V. Thus ~<v - w, v - w) = (v - A1u1 - • • • - J..kuk, v - A1w1 - • • • - Akwk) = (v, v) - L.J..)if3ii - 2L.AiYi· This quadratic function in the J..'s is nonnegative and so, by results from the calculus, has a minimum. The J..'s for this minimum, J..1 < 0 >, J..2 < 0 >, ... , Ak(O) give us the desired vector w0 = .A. 1 (O)u 1 + · · · + Ak (O)uk in W. A second way of exhibiting such a minimizing w is as follows. In V define a metric (by ((x,y) = llx - Yll; one shows that (is a proper metric on V, and V is now a metric space. ·Let S = {wE WI llv- wll ~ llvll}; in this metric S is a compact set (prove!) and so the continuous function f(w) = IJv - wll defined for wE S takes on a minimum at some point w0 E S. We leave it to the reader to verify that w0 is the desired vector satisfying llv - w0 11 ~ llv - wl/ for all wE W. COROLLARY If Vis a finite-dimensional inner product space and W is a subspace of V then (Wl.)l. = W. Proof. If wE W then for any u E Wl., (w, u) = 0, whence W c (W1.)1.. Now V = W + w1. and V = w1. + (Wl.)l.; from these we get, since the sums are direct, dim (W) =dim ((W1.)1.). Since W c (Wl.}.l. and is of the same dimension as (Wl.)l., it follows that W = (W1.)1.. Problems In all the problems Vis an inner product space over F. I. IfF is the real field and Vis p(3>, show that the Schwarz inequality i~lies that the cosine of an angle is of absolute value at most 1. 2. IfF is the real field, find all 4-tuples of real numbers (a, b, c, d) such that for u = (ctv ct2 ), v = ({31, {32 ) E F< 2>, (u, v) = act1/31 + bct2 {32 + cct1{32 + dct2 {31 defines an inner product on F< 2 >. 3. In V define the distance ((u, v) from u to v by ((u, v) = IJu - vii. Prove that (a) ((u, v) ~ 0 and ((u, v) = 0 if and only if u = v. (b) ((u, v) = '(v, u). (c) '(u, v) ~ '(u, w) + '(w, v) (triangle inequality). 200 Vector Spaces and Modules Ch. 4 4. If {w1, ... , wm} is an orthonormal set in V, prove that m L l(w;, v)l 2 ~ llvll 2 for any v E V. i= 1 (Bessel inequality) 5. If Vis finite-dimensional and if {w1, ••. , wm} is an orthonormal set in V such that m L l(w;, v)l2 = llvll2 i= 1 for every v E V, prove that {wv ... , wm} must be a basis of V. 6. If dim V = n and if {w1, ... , wm} is an orthonormal set in V, prove that there exist vectors wm+l' ... , wn such that {w1, ... , wm, wm+l' ... , wn} is an orthonormal set (and basis of V). 7. Use the result of Problem 6 to give another proof of Theorem 4.4.3. 8. In V prove the parallelogram law: Explain what this means geometrically in the special case V = p(3>, where F is the real field, and where the inner product is the usual dot product. 9. Let V be the real functions y = f (x) satisfying d 2yfdx 2 + 9y = 0. (a) Prove that Vis a two-dimensional real vector space. (b) In V define (y, z) = J: yz dx. Find an orthonormal basis in V. 10. Let V be the set of real functions y = f (x) satisfying d 3y d 2y dy - - 6 - + 11 - - 6y = 0. dx 3 dx 2 dx (a) Prove that Vis a three-dimensional real vector space. (b) In V define (u, v) = roo uv dx. Show that this defines an inner product on V and find an ortho- normal basis for V. 11. If W is a subspace of V and if v E V satisfies (v, w) + (w, v) ~ (w, w) for every wE W, prove that (v, w) = 0 for every wE W. 12. If V is a finite-dimensional inner product space and iff is a linear functional on V (i.e., fE V), prove that there is a u0 E V such that f (v) = (v, u0 ) for all v E V. Sec. 4.5 Modules 4.5 Modules The notion of a module will be a generalization of that of a vector space; instead of restricting the scalars to lie in a field we shall allow them to be elements of an arbitrary ring. This section has many definitions but only one main theorem. However the definitions are so close in spirit to ones already made for vector spaces that the main ideas to be developed here should not be buried in a sea of definitions. DEFINITION Let R be any ring; a nonempty set M is said to be an R-module (or, a module over R) if M is an abelian group under an operation + such that for every r E R and mE M there exists an element rm in M subject to I. r(a + b) = ra + rb; 2. r(sa) = (rs)a; 3. (r + s)a = ra + sa for all a, b E M and r, s E R. If R has a unit element, 1, and if lm = m for every element min M, then M is called a unital R-module. Note that if R is a field, a unital R-module is nothing more than a vector space over R. All our modules shall be unital ones. Properly speaking, we should call the object we have defined a left R- module for we allow multiplication by the elements of R from the left. Similarly we could define a right R-module. We shall make no such left-right distinction, it being understood that by the term R-module we mean a J.eft R-module. Example 4.5.1 Every abelian group G is a module over the ring of integers! For, write the operation of Gas + and let na, for a E G and nan integer, have the meaning it had in Chapter 2. The usual rules of exponents in abelian groups translate into the requisite properties needed to make of G a m9£1-ule over the integers. Note that it is a unital module. Example 4.5.2 Let R be any ring and let M be a left-ideal of R. For r E R, m E M, let rm be the product of these elements as elements in R. The definition of left-ideal implies that rm E M, while the axioms defining a ring insure us that M is an R-module. (In this example, by a ring we mean an associative ring, in order to make sure that r(sm) = (rs)m.) Example 4.5.3 The special case in which M = R; any ring R is an R-module over itself. 201-, 202 Vector Spaces and Modules Ch. 4 Example 4.5.4 Let R be any ring and let A be a left-ideal of R. Let M consist of all the cosets, a + A., where a E R, of A in R. In M define (a + A.) + (b + A.) = (a + b) + A and r(a + A.) = ra + X M can be shown to be an R-module. (See Problem 2, end of this section.) M is usually written as R - A (or, sometimes, as Rf A.) and is called the dijference (or quotient) module of R by A. An additive subgroup A of the R-module M is called a submodule of M if whenever r E R and a E A, then ra E A. Given an R-module M and a submodule A we could construct the quotient module MJA in a manner similar to the way we constructed quotient groups, quotient rings, and quotient spaces. One could also talk about homomorphisms of one R-module into another one, and prove the appro- priate homomorphism theorems. These occur in the problems at the end of this section. Our interest in modules is in a somewhat different direction; we shall attempt to find a nice decomposition for modules over certain rings. DEFINITION If M is an R-module and if M 1, ... , Ms are submodules of M, then M is said to be the direct sum of M 1, ••• , Ms if every element mE M can be written in a unique manner as m = m1 + m2 + · · · + ms where m1 E M 1, m2 E M 2 , ••. , ms E Ms. As in the case of vector spaces, if M is the direct sum of M 1., .•• , Ms then M will be isomorphic, as a module, to the set of all s-tuples, (m1, • •• , m5 ) where the ith component mi is any element of Mi, where addition is com- ponentwise, and where r(m 1 , • •• , m 5 ) = (rm 1, rm2 , • •• , rm5 ) for r E R. Thus, knowing the structure of each Mi would enable us to know the structure of M. Of particular interest and simplicity are modules generated by one element; such modules are called cyclic. To be precise: DEFINITION An R-module M is said to be cyclic if there is an element m0 E M such that every m E M is of the form m = rm0 where r E R. For R, the ring of integers, a cyclic R-module is nothing more than a cyclic group. We still need one more definition, namely, DEFINITION An R-module M is said to be .finitely generated if there exist elements a1 , ···,an EM such that every min M is of the form m = r1a1 + r2a2 + ... + rnan. Sec. 4.5 Modules With all the needed definitions finally made, we now come to the theorem which is the primary reason for which this section exists. It is often called the fundamental theorem on finitely generated modules over Euclidean rings. In it we shall restrict R to be a Euclidean ring (see Chapter 3, Section 3. 7); however the theorem holds in .the more general context in which R is any principal ideal domain. THEOREM 4.5.1 Let R be a Euclidean ring:~· then any finitely generated R- module, M, is the direct sum cif a finite number of cyclic submodules. Proof. Before becoming involved with the machinery of the proof, let us see what the theorem states. The assumption that M is finitely generated tells us that there is a set of elements av ... , an E M such that every ele- ment in M can be expressed in the form r1a1 + r2a2 + · · · + rnan, where the ri E R. The conclusion of the theorem states that when R is properly conditioned we can, in fact, find some other set of elements b1 , ... , bq in M such that every element mE M can be expressed in a unique fashion as m = s1b1 + · · · + sqbq with si E R. A remark about this uniqueness; it does not mean that the si are unique, in fact this may be false; it merely states that the elements sibi are. That is, if m = s1b1 + · · · + sqbq and m = s~b1 + · · · + s~bq we cannot draw the conclusion that s1 = s~, s2 = s;, . .. , sq = s~, but rather, we can infer from this that s1 b1 = s~bv ... , sqbq = s~bq. Another remark before we start with the technical argument. Although the theorem is stated for a general Euclidean ring, we shall give the proof in all its detail only for the special case of the ring of integers. At the end we shall indicate the slight modifications needed to make the proof go through for the more general setting. We have chosen this path to avoid cluttering up the essential ideas, which are the same in the general case, with some technical niceties which are of no importance. Thus we are simply assuming that M is an abelian group which has a finite-generating set. Let us call those generating sets having as few elements as possible minimal generating sets and the number of elements in such a minimal generating set the rank of M. /Our proof now proceeds by induction on the rank of M. If the rank of M is 1 then M is generated by a single element, hence it is cyclic; in this case the theorem is true. Suppose that the result is true for all abelian groups of rank q - 1, and that M is of rank q. Given any minimal generating set a1 , ... , aq of M, if any relation of the form n1a1 + n2 a2 + · · · + nqaq = 0 (n1 , ..• , nq integers) implies that n1 a1 = n2a2 = · · · = nqaq = 0, then M is the direct sum of M 1 , M 2 , ••• , Mq where each Mi is the cyclic module (i.e., subgroup) generated by ai, and so we would be done. Consequently, given any minimal generating set 20~ 204 Vector Spaces and Modules Ch. 4 b1 , . •• , bq of M, there must be integers ru . .. , rq such that r1 b1 + · · · + rqbq = 0 and in which not all of r1 b1 , r2b2 , • •• , riq are 0. Among all possible such relations for all minimal generating sets there is a smallest possible positive integer occurring as a coefficient. Let this integer be s1 and let the generating set for which it occurs be a 1, ••• , aq. Thus (1) We claim that if r1a 1 + · · · + rqaq = 0, then s1 I r1 ; for r1 = ms1 + t, 0 ~ t < s1 , and so multiplying Equation (1) by m and subtracting from r1a 1 + · · · + rqaq = 0 leads to ta 1 + (r2 - ms2 )a2 + · · · + (rq - msq)aq = 0; since t < s1 and s1 is the minimal possible positive integer in such a relation, we must have that t = 0. We now further claim that s1 I si for i = 2, ... , q. Suppose not; then s1 ,.V s2 , say, so s2 = m2s1 + t, 0 < t < s1 • Now a'1 = a 1 + m2a2 , a2 , .•• , aq also generate M, yet s1 a~ + ta2 + s3 q3 + · · · + sqaq = 0; thus t occurs as a coefficient in some relation among elements of a minimal generating set. But this forces, by the very choice of s1 , that either t = 0 or t ~ s1 . We are left with t = 0 and so s1 I s2 • Similarly for the other si. Let us write si = mis1 • Consider the elements ai = a1 + m2a2 + m3a3 + · · · + mqaq, a2 , ••• , aq. They generate M; moreover, s1ai = s1a 1 + m2s1a2 + · · · + mqs1aq = s1a1 + s2a2 + · · · + sqaq = 0. If r1ai + r2a2 + · · · + rqaq = 0, substitut- ing for ai, we get a relation between au ... , aq in which the coefficient of a1 is r1 ; thus s1 I r1 and so r1 ai = 0. If M 1 is the cyclic module generated by ai and if M 2 is the submodule of M generated by a2 , • •• , aq, we have just shown that M 1 n M 2 = (0). But M 1 + M 2 = M since ai, a2 , .•• , aq generate M. Thus M is the direct sum of M 1 and M 2 . Since M 2 is generated by a2 , ••. , aq, its rank is at most q - 1 (in fact, it is q - 1), so by the induction M 2 is the direct sum of cyclic modules. Putting the pieces together we have decomposed Minto a direct sum of cyclic modules. COROLLARY Any finite abelian group zs the direct product (sum) of cyclic groups. Proof. The finite abelian group G is certainly finitely generated; in fact it is generated by the finite set consisting of all its elements. Therefore applying Theorem 4.5.1 yields the corollary. This is, of course, the result proved in Theorem 2.14.1. Suppose that R is a Euclidean ring with Euclidean function d. We modify the proof given for the integers to one for R as follows: 1. Instead of choosing s1 as the smallest possible positive integer occurring in any relation among elements of a generating set, pick it as that element of R occurring in any relation whose d-value is minimal. Sec. 4.5 Modules 205 2. In the proof that s1 I r1 for any relation r1a1 + · · · + rqaq = 0, the only change needed is that r1 = ms1 + t where either t = 0 or d ( t) < d ( s 1) ; the rest goes through. Similarly for the proof that s1 I si. Thus with these minor changes the proof holds for general Euclidean rings, whereby Theorem 4.5.1 is completely proved. Problems 1. Verify that the statement made in Example 4.5.1 that every abelian group is a module over the ring of integers is true. 2. Verify that the set in Example 4.5.4 is an R-module. 3. Suppose that R is a ring with a unit element and that M is a module over R but is not unital. Prove that there exists an m =j:: 0 in M such that rm = 0 for all r E R. Given two R-modules M and N then the mapping T from M into N is called a homomorphism (orR-homomorphism or module homomorphism) if I. (m 1 + m2 ) T = m1 T + m2 T; 2. (rm 1 ) T = r(m1 T); for all m1 , m2 EM and all r E R. 4. If Tis a homomorphism of Minto N let K(T) = {x EM I xT = 0}. Prove that K(T) is a submodule of M and that l(T) = {xT I x EM} is a submodule of N. 5. The homomorphism Tis said to be an isomorphism if it is one-to-ORe. Prove that Tis an isomorphism if and only if K( T) = (0). 6. Let M, N, Q be three R-modules, and let T be a homomorphism of Minto Nand Sa homomorphism of N into Q. Define TS:M---+- Q by m( TS) = (m T)S for any mE M. Prove that TS is an R-homo- morphism of Minto Q and determine its kernel, K(TS). 7. If M is an R-module and A is a submodule of M, define the quotient module MfA (use the analogs in group, rings, and vector spaces as a /guide) so that it is an R-module and prove that there is an R-homo- morphism of M onto MfA. 8. If Tis a homomorphism of M onto N with K(T) = A, prove that N is isomorphic (as a module) to MfA. 9. If A and B are submodules of M prove (a) A n B is a submodule of M. (b) A + B = {a + b I a E A, bE B} is a submodule of Af. (c) (A + B)fB is isomorphic to Af(A n B). 206 Vector Spaces and Modules Ch. 4 10. An R-module M is said to be irreducible if its only submodules are (0) and M. Prove that any unital, irreducible R-module is cyclic. 11. If M is an irreducible R-module, prove that either M is cyclic or that for every m E M and r E R, rm = 0. *12. If M is an irreducible R-module such that rm # 0 for some r E R and mE M, prove that any R-homomorphism T of Minto M is either an isomorphism of M onto M or that m T = 0 for every m E M. 13. Let Mbe an R-module and let E(M) be the set of all R-homomorphisms of Minto M. Make appropriate definitions of addition and multi. plication of elements of E(M) so that E(M) becomes a ring. (Hint: imitate what has been done for Hom (V, V), V a vector space.) * 14. If M is an irreducible R-module such that rm # 0 for some r E R and mE M, prove that E(M) is a division ring. (This result is known as Schur's lemma.) 15. Give a complete proof of Theorem 4.5.1 for finitely generated modules over Euclidean rings. 16. Let M be an R-module; if mE M let A.(m) = {x E R I xm = 0}. Show that A.(m) is a left-ideal of R. It is called the order of m. 1 7. If A is a left-ideal of R and if M is an R-module, show that for m E M, A.m = {xm I x E A.} is a submodule of M. * 18. Let M be an irreducible R-module in which rm # 0 for some r E R and mE M. Let m0 # 0 E M and let A.(m0 ) = {x E R I xm0 = 0}. (a) Prove that A.(m0 ) is a maximal left-ideal of R (that is, if A is a left-ideal of R such that R ::) A ::) A.(m0 ), then A = R or A. = A.(mo)). (b) As R-modules, prove that M is isomorphic to R - A.(m0 ) (see Example 4.5.4). Supplementary Reading HALMos, PAuL R., Finite-Dimensional Vector Spaces, 2nd ed. Princeton, N.J.: D. Van Nostrand Company, Inc., 1958. 5 Fields In our discussion of rings we have already singled out a special class which we called fields. A field, let us recall, is a commutative ring with unit element in which every nonzero element has a multiplicative inverse. Put another way, a field is a commutative ring in which we can divide by any nonzero element. Fields play a central role in algebra. For one thing, results about them find important applications in the theory of numbers. For another, their theory encompasses the subject matter of the theor~.of equations which treats questions about the roots of polynomials. In our development we shall touch only lightly on the field of algebraic numbers. Instead, our greatest emphasis will be on aspects of field theory which impinge on the theory of equations. Although we shall not treat the material in its fullest or most general form, we shall go -far enough to introduce some of the beautiful ideas, due to the brilliant French mathematician Evariste Galois, which have _;;erved as a guiding inspiration for algebra as it is today. 5.1 Extension Fields In this section we shall be concerned with the relation of one field to another. Let F be a field; a field K is said to be an extension ofF if K contains F. Equivalently, K is an extension ofF ifF is a subfield of K. Throughout this chapter F will denote a given field and K an extension of F. As was pointed out earlier, in the chapter on vector spaces, if K is 207 208 Fields Ch. 5 an extension ofF, then, under the ordinary field operations inK, K is a vector space over F. As a vector space we may talk about linear dependence, dimension, bases, etc., inK relative to F. DEFINITION The degree of K over F is the dimension of K as a vector space over F. We shall always denote the degree of Kover F by [K:F]. Of particular interest to us is the case in which [ K :F] is finite, that is, when K is finite- dimensional as a vector space over F. This situation is described by saying that K is a finite extension of F. We start offwith a relatively simple but, at the same time, highly effective result about finite extensions, namely, THE 0 REM 5.1 .1 If L is a finite extension of K and if K is a finite extension of F, then Lis afinite extension of F. Moreover, [L:F] = [L:K][K:F]. Proof. The strategy we employ in the proof is to write down explicitly a basis of L over F. In this way not only do we show that L is a finite extension ofF, but we actually prove the sharper result and the one which is really the heart of the theorem, namely that [ L :F] = [ L :K] [ K :F]. Suppose, then, that [L:K] = m and that [K:F] = n. Let v1 , ..• , vm be a basis of L over K and let w 1, ••• , wn be a basis of Kover F. What could possibly be nicer or more natural than to have the elements viwi, where i = 1, 2, ... , m, J = 1, 2, ... , n, serve as a basis of L over F? Whatever else, they do at least provide us with the right number of elements. We now proceed to show that they do in fact form a basis of L over F. What do we need to establish this? First we must show that every element in L is a linear combination of them with coefficients in F, and then we must demonstrate that these mn elements are linearly independent over F. Lett be any element in L. Since every element in Lis a linear combination of v1, ••• , vm with coefficients in K, in particular, t must be of this form. Thus t = k1 v1 + · · · + kmvm, where the elements k1 , .•• , km are all in K. However, every element in K is a linear combination of w1 , ... , wn with coefficients in F. Thus k1 = f 11 w 1 + · · · + f 1nwm ... , ki = ./;,1w 1 + · · · + finwn, ... , km = fm 1w1 + · · · + fmnwn, where every jij is in F. Substituting these expressions for k1 , ... , km into t = k1 v1 + · · · + kmvm, we obtain t = (j11 w 1 + · · · + f 1 nwn)v1 + · · · + (fm 1w 1 + · · · + fmnwn)vm Multiplying this out, using the distributive and associative laws, we finally arrive at t = f 11 v1 w 1 + · · · + f 1 nv1wn + · · · + fijviwi + · · · + fmnvmWn· Since the fii are in F, we have realized t as a linear combination over F of the elements viwi. Therefore, the elements viwi do indeed span all of Lover F, and so they fulfill the first requisite property of a basis. Sec. 5.1 Extension Fields We still must show that the elements viwi are linearly independent over F. Suppose that f 11 v1w 1 + · · · + f 1 nv1wn + · · · + fiiviwi + · · · + fmnvmwn = 0, here the fii are in F. Our objective is to prove that each fii = 0. Re- ouping the above expression yields (f11w1 + · · · + f 1 nwn)v1 + · · · + fuwt + ·' · + finwn)vi + · · · + (fmtWt + · · · + fmnwn)vm = 0. Since the wi are in K, and since K :::J F, all the elements ki = .1;,1 w 1 + · · · finwn are in K. Now k 1 v1 + · · · + kmvm = 0 with k1, ... , km E K. But, y assumption, Vv ... , vm form a basis of L over K, so, in particular they ust be linearly independent over K. The net result of this is that k1 2 = · · · = km = 0. Using the explicit values of the ki, we get for i = 1, 2, ... , m. now we invoke the fact that the wi are linearly independent over F; '· 1this yields that each fii = 0. In other words, we have proved that the ;~,'fltWi are linearly independent over F. In this way they satisfy the other ~!¢:;requisite property for a basis. We have now succeeded in proving that the mn elements viwi form a asis of L over F. Thus [L:F] = mn; since m = [L:K] and n = [K:F] e have obtained the desired result [ L :F] = [ L :K] [ K :F]. Suppose that L, K, F are three fields in the relation L :::J K :::J F and, uppose further that [ L :F] is finite. Clearly, any elements in L linearly dependent over K are, all the more so, linearly independent over F. hus the assumption that [ L :F] is finite forces the conclusion that [ L :K] finite. Also, since K is a subspace of L, [ K :F] is finite. By the theorem, 1 L:F] = [L:K][K:F], whence [K:F] I [L:F]. We have proved the COROLLARY If L is a finite extension ofF and K is a subjield of L whieh ntains F, then [ K :F] I [ L :F]. Thus, for instance, if [ L :F] is a prime number, then there can be no elds properly between F and L. A little later, in Section 5.4, when we discuss the construction of certain geometric figures by straightedge and ompass, this corollary will be of great significance. EFitj~TION An element a E K is said to be algebraic over F if there exist lements cx0 , cx1 , ... , cxn in F, not all 0, such that cx0 an + cx1 an- 1 + · · · + If the polynomial q(x) E F[x], the ring of polynomials in x over F, and q(x) = fJ0 xm + {31xm- 1 + · · · + /3m, then for any element bE K, by q(b) e shall mean the element fJ0 bm + /J1 bm-t + · · · + Pm in K. In the ex- ression commonly used, q(b) is the value of the polynomial q(x) obtained Y substituting b for x. The element b is said to satisfy q(x) if q(b) = 0. 209 210 Fields Ch. 5 In these terms, a E K is algebraic over F if there is a nonzero polynomial p(x) E F[ x] which a satisfies, that is, for which p(a) = 0. Let K be an extension ofF and let a be in K. Let .A be the collection of all subfields of K which contain both F and a. .A is not empty, for K itself is an element of .A. Now, as is easily proved, the intersection of any number of subfields of K is again a subfield of K. Thus the intersection of all those subfields of K which are members of .A is a subfield of K. We denote this subfield by F(a). What are its properties? Certainly it contains both F and a, since this is true for every subfield of K which is a member of At. Moreover, by the very definition of intersection, every subfield of K in At contains F(a), yet F(a) itself is in .A. Thus F(a) is the smallest subfield of K containing both F and a. We call F (a) the subfield obtained by adJoining a to F. Our description of F(a), so far, has been purely an external one. We now give an alternative and more constructive description ofF (a). Consider all these elements inK which can be expressed in the form Po + P1 a+ · · · + f3sas; here the P's can range freely over F and s can be any nonnegative integer. As elements in K, one such element can be divided by another, provided the latter is not 0. Let U be the set of all such quotients. We leave it as an exercise to prove that U is a subfield of K. On one hand, U certainly contains F and a, whence U :J F(a). On the other hand, any subfield of K which contains both F and a, by virtue of closure under addition and multiplication, must contain all the elements Po + pia + ... + Psas where each piE F. Thus F(a) must contain all these elements; being a subfield of K, F (a) must also contain all quotients of such elements. Therefore, F(a) :J U. The two relations U c F(a), U :J F(a) of course imply that U = F(a). In this way we have obtained an internal construction of F(a), namely a,s U. We now intertwine the property that a E K is algebraic over F with macroscopic properties of the field F(a) itself. This is THEOREM 5.1.2 The element a E K is algebraic over F if and only if F(a) is a finite extension qf F. Proof. As is so very common with so many such \"if and only if\" pro~ positions, one-half of the proof will be quite straightforward and easy, whereas the other half will be deeper and more complicated. Suppose that F(a) is a finite extension of F and that [F(a) :F] = m. Consider the elements 1, a, a2 , ••• , am; they are all in F(a) and are m + 1 in number. By Lemma 4.2.4, these elements are linearly dependent over F. Therefore, there are elements ct0 , ct1, ... , ctm in F, not all 0, such that i ct0 1 + ct 1 a + ct2a2 + · · · + ctmam = 0. Hence a is algebraic over F and satisfies the nonzero polynomial p(x) = ct0 + ct1x + · · · + ctmxm ·in F[x] of degree at most m = [F(a) :F]. This proves the \"if\" part of the theorem. Now to the \"only if\" part. Suppose that a in K is algebraic over F. By Sec. 5.1 Extension Fields tion, a satisfies some nonzero polynomial in F[ x]; let p(x) be a in F[x] of smallest positive degree such that p(a) = 0. We that p(x) is irreducible over F. For, suppose that p(x) = f (x) g(x), f (x), g(x) E F[x]; then 0 = p(a) = f (a)g(a) (see Problem 1) and, f(a) and g(a) are elements of the field K, the fact that their product 0 forces f (a) = 0 or g(a) = 0. Since p(x) is of lowest positive degree p(a) = 0, we must conclude that one of deg f (x) ~ deg p(x) or g(x) ~ degp(x) must hold. But this proves the irreducibility of p(x). We define the mapping 1/J from F[x] into F(a) as follows. For any E F[x], h(x)i/J = h(a). We leave it to the reader to verify that 1/J is a homomorphism of the ring F[x] into the field F(a) (see Problem 1). t is V, the kernel of 1/J? By the very definition of 1/J, V = (x) E F[x] I h(a) = 0}. Also, p(x) is an element of lowest degree in the Vof F[x]. By the results of Section 3.9, every element in Vis a multiple (x), and since p(x) is irreducible, by Lemma 3.9.6, Vis a maximal ideal F[x]. By Theorem 3.5.1, F[x]fV is a field. Now by the general homo- \"··1~nn,rn•11<;11m theorem for rings (Theorem 3.4.1), F[x]/V is isomorphic to the of F[x] under 1/J. Summarizing, we have shown that the image of under 1/J is a subfield of F(a). This image contains xi/J = a and, for a E F, ai/J = a. Thus the image of F [ x J under 1/J is a subfield of which contains both F and a; by the very definition of F(a) we are to conclude that the image of F[x] under 1/J is all of F(a). Put more {~~tlllCCinctly, F [ x] / V is isomorphic to F (a). Now, V = (p(x)), the ideal generated by p(x); from this we claim that dimension of F[x]/V, as a vector space over F, is precisely equal to p(x) (see Problem 2). In view of the isomorphism between F[x]fV and (a) we obtain the fact that [F(a) :F] = deg p(x). Therefore, [F(a) :F] is :;;;,;;~~,~-rt·\"''\"' h, finite; this is the contention of the \"only if\" part of the theorem. that we have actually proved more, namely that [F(a) :F] is equal to degree of the polynomial of least degree satisfied by a over F. The proof we have just given has been somewhat long-winded, but erately so. - The route followed contains important ideas and ties in and concepts developed earlier with the current exposition. No part matj;lematics is an island unto itself. We now redo the \"only if\" part, working more on the inside of F(a). · reworking is, in fact, really identical with the proof already given; the \"tuent pieces are merely somewhat differently garbed. Again let p(x) be a polynomial over F of lowest positive degree satisfied a. Such a polynomial is called a minimal polynomial for a over F. We assume that its coefficient of the highest power of x is 1, that is, it is · ; in that case we can speak of the minimal polynomial for a over F any two minimal, monic polynomials for a over Fare equal. (Prove!) 211 212 Fields Ch. 5 Suppose that p(x) is of degree n; thus p(x) = xn + ct1xn- 1 + · · · + rx11 where the cti are in F. By assumption, an + ct 1 an- 1 + · · · + ctn = 0, whence an= -ct 1an-l- ct2an- 2 - • • ·- ctn. What about an+l? From the above, an+ 1 = -ct 1an - ct2an-l - · · ·- ana; if we substitute the expression for an into the right-hand side of this relation, we realize a\"+ 1 as a linear combination of the elements 1, a, ... , an- 1 over F. Con- tinuing this way, we get that an+\\ fork ~ 0, is a linear combination over F of 1, a, a 2 , .•. , an- 1 • Now consider T = {/30 + f31a + · · · + f3n_ 1an- 1 I /30 , f3u ... , f3n- 1 E F}. Clearly, T is closed under addition; in view of the remarks made in the paragraph above, it is also closed under multiplication. Whatever further it may be, T has at least been shown to be a ring. Moreover, T contains both F and a. We now wish to show that Tis more than just a ring, that it is, in fact, a field. Let 0 =I= u = {30 + f31a + · · · + f3n_ 1an- 1 be in T and let h(x) = /30 + f31x + · · · + f3n_ 1xn- 1 E F[x]. Since u =/= 0, and u = h(a), we have that h(a) =/= 0, whence p(x) ,f\" h(x). By the irreducibility of p(x), p(x) and h(x) must therefore be relatively prime. Hence we can find polynomials s(x) and t(x) in F[x] such that p(x)s(x) + h(x)t(x) = 1. But then 1 = p(a)s(a) + h(a)t(a) = h(a)t(a), since p(a) = 0; putting into this that u = h(a), we obtain ut(a) = 1. The inverse of u is thus t(a); in t(a) all powers of a higher than n - 1 can be replaced by linear combinations of 1, a, ... , an- 1 over F, whence t(a) E T. We have shown that every nonzero element of T has its inverse in T; consequently, T is a field. However, T c F(a), yet F and a are both contained in T, which results in T = F(a). We have identified F(a) as the set of all expressions {30 + f31a + · · · + f3n-lan-1. Now Tis spanned over F by the elements 1, a, ... , an- 1 in consequence of which [T:F] ::; n. However, the elements 1, a, a2 , ••• , an- 1 are linearly independent over F, for any relation of the form y0 + y1 a + · · · + Yn-lan- 1, with the elements Yi E F, leads to the conclusion that a satisfies the polynomial y0 + y1x + · · · + 'l'n- 1x»- 1 over F of degree less than n. This contradiction proves the linear independence of 1, a, ... , an- 1, and so these elements actually form a basis of T over F, whence, in fact, we now know that [T:F] = n. Since T = F(a), the result [F(a) :F] = n follows. DEFINITION The element a E K is said to be algebraic of degree n over F if it satisfies a nonzero polynomial over F of degree n but no nonzero polynomial of lower degree. In the course of proving Theorem 5.1.2 (in each proof we gave), we proved a somewhat sharper result than that stated in that theorem, namely, Sec. 5.1 Extension Fields THEOREM 5.1.3 If a E K is algebraic of degree n over F, then [F(a) :F] = n. This result adapts itself to many uses. We give now, as an immediate consequence thereof, the very interesting THEOREM 5.1.4 If a, b in K are algebraic over F then a ± b, ab, and afb (if b =f:. 0) are all algebraic over F. In other words, the elements in K which are algebraic over F form a subfield of K. Proof. Suppose that a is algebraic of degree m over F while b is algebraic of degree n over F. By Theorem 5.1.3 the subfield T = F(a) of K is of degree mover F. Now b is algebraic of degree n over F, a fortiori it is algebraic of degree at most n over T which contains F. Thus the subfield W = T (b) of K, again by Theorem 5.1.3, is of degree at most n overT. But [W:F] = [W: T][T:F] by Theorem 5.1.1; therefore, [W:F] ~ mn and so W is a finite extension of F. However, a and b are both in W, whence all of a ± b, ab, and ajb are in W. By Theorem 5.1.2, since [W:F] is finite, these elements must be algebraic over F, thereby proving the theorem. Here, too, we have proved somewhat more. Since [ W :F] :::;;; mn, every element in W satisfies a polynomial of degree at most mn over F, whence the COROLLARY If a and bin K are algebraic over F of degrees m and n, respectively, then a ± b, ab, and ajb (if b =f:. 0) are algebraic over F of degree at most mn. In the proof of the last theorem we made two extensions of the field F. The first we called T; it was merely the field P(a). The second we called W and it was T(b). Thus W = (F(a))(b); it is customary to write it\"as F(a, b). Similarly, we could speak about F(b, a); it is not too difficult to prove that F(a, b) = F(b, a). Continuing this pattern, we can define F(a 1 , a2 , • •• , an) for elements a 1 , ... , an inK. DEFINITION The extension K ofF is called an algebraic extension ofF if every element inK is algebraic over F. w~ prove one more result along the lines of the theorems we have proved so far. THEOREM 5.1.5 If Lis an algebraic extension of K and if K is an algebraic extension ofF, then L is an algebraic extension of F. Proof. Let u be any arbitrary element of L; our objective is to show that U satisfies some nontrivial polynomial with coefficients in F. What infor- mation do we have at present? We certainly do know that u satisfies some 213 214 Fields Ch. 5 polynomial xn + cr1x\"- 1 + · · · + cr\"' where cr1 , ... , ern are in K. But K is algebraic over F; therefore, by several uses of Theorem 5.1.3, M = F(cr1, ••• ,ern) is a finite extension of F. Since u satisfies the polynomial x\" + cr 1 x\"- 1 + · · · + cr n whose coefficients are in M, u is algebraic over M. Invoking Theorem 5.1.2 yields that M(u) is a finite extension of M. However, by Theorem 5.1.1, [M(u) :F] = [M(u) :M][M:F], whence M(u) is a finite extension of F. But this implies that u is algebraic over F, completing proof of the theorem. A quick description of Theorem 5.1.5: algebraic over algebraic is algebraic. The preceding results are of special interest in the particular case in which F is the field of rational numbers and K the field of complex numbers. DEFINITION A complex number is said to be an algebraic number if it is algebraic over the field of rational numbers. A complex number which is not algebraic is called transcendental. At the present stage we have no reason to suppose that there are any transcendental numbers. In the next section we shall prove that the familiar real number e is transcendental. This will, of course, establish the existence of trans- cendental numbers. In actual fact, they exist in great abundance; in a very well-defined way there are more of them than there are algebraic numbers. Theorem 5.1.4 applied to algebraic numbers proves the interesting fact that the algebraic numbers form afield; that is, the sum, products, and quotients of algebraic numbers are again algebraic numbers. Theorem 5.1.5 when used in conjunction with the so-called \"fundamental theorem of algebra,\" has the implication that the roots of a polynomial whose coefficients are algebraic numbers are themselves algebraic numbers. Problems 1. Prove that the mapping 1/J:F[x] ~ F(a) defined by h(x)l/J = h(a) is a homomorphism. 2. Let F be a field and let F[x] be the ring of polynomials in x over F. Let g(x), of degree n, be in F[x] and let V = (g(x)) be the ideal generated by g(x) in F[x]. Prove that F[x]JV is an n-dimensional vector space over F. 3. (a) If V is a finite-dimensional vector space over the field K, and if F is a subfield of K such that [ K :F] is finite, show that V is a finite-dimensional vector space over F and that moreover dimF (V) = (dimx (V))([K:F]). (b) Show that Theorem 5.1.1 is a special case of the result ofpart (a): Sec. 5.1 Extension Fields 4. (a) Let R be the field of real numbers and Q the field of rational numbers. In R, .J2 and .J3 are both algebraic over Q. Exhibit a polynomial of degree 4 over Q satisfied by .J2 + .J3. (b) What is the degree of .J2 + .J3 over Q? Prove your answer. (c) What is the degree of .J2 .J3 over Q? 5. With the same notation as in Problem 4, show that .J2 + Z/5 is algebraic over Q of degree 6. *6. (a) Find an element u E R such that Q( .J2, ~5) = Q(u). (b) In Q( .J2, Z/s) characterize all the elements w such that Q(w) =/; Q( .J2, Vs). 7. (a) Prove that F(a, h) = F(b, a). (b) If (i1 , i 2 , ..• , in) is any permutation of (1, 2, ... , n), prove that 8. If a, hE K are algebraic over F of degrees m and n, respectively, and if m and n are relatively prime, prove that F(a, h) is of degree mn over F. 9. Suppose that F is a field having a finite number of elements, q. (a) Prove that there is a prime number p such that a+ a+···+ a= 0 forallaEF. ~ (b) Prove that q = pn for some integer n. (c) If a E F, prove that aq = a. (d) If h E K is algebraic over F, prove hqm = h for some m > 0. An algebraic number a is said to be an algebraic integer if it satisfies .... an equation of the form am + rx1am-l + · · · + r:t.m = 0, where r:t.1 , .•. , r:t.m are integers. 10. If a is any algebraic number, prove that there is a positive integer n such that na is an algebraic integer. 11. If the rational number r is also an algebraic integer, prove that r must be an ordinary integer. 1~ If a is an algebraic integer and m is an ordinary integer, prove (a) a + m is an algebraic integer. (b) ma is an algebraic integer. 13. If rx is an algebraic integer satisfying rx3 + rx + 1 = 0 and p is an algebraic integer satisfying p 2 + p - 3 = 0, prove that both rx + P and rxP are algebraic integers. **14. (a) Prove that the sum of two algebraic integers is an algebraic integer. 215 216 Fields Ch. 5 (b) Prove that the product of two algebraic integers is an algebraic integer. 15. (a) Prove that sin 1° is an algebraic number. (b) From part (a) prove that sin m0 is an algebraic number for any integer m. 5.2 The Transcendence of e In defining algebraic and transcendental numbers we pointed out that it could be shown that transcendental numbers exist. One way of achieving this would be the demonstration that some specific number is transcendental. In 1851 Liouville gave a criterion that a complex number be algebraic; using this, he was able to write down a large collection of transcendental numbers. For instance, it follows from his work that the number .1 01001000000100 . . . 10 ... is transcendental; here the number of zeros between successive ones goes as 1 !, 2 !, ...... , n !, ... . This certainly settled the question of existence. However, the question whether some given, familiar numbers were transcendental still persisted. The first success in this direction was by Hermite, who in 1873 gave a proof that e is transcendental. His proof was greatly simplified by Hilbert. The proof that we shall give here is a variation, due to Hurwitz, of Hilbert's proof. The number n offered greater difficulties. These were finally overcome by Lindemann, who in 1882 produced a proof that n is transcendental. One immediate consequence of this is the fact that it is impossible, by straightedge and compass, to square the circle, for such a construction would lead to an algebraic number fJ such that 02 = n. But if fJ is algebraic then so is 02 ' in virtue of which n would be algebraic, in contradiction to Lindemann's result. In 1934, working independently, Gelfond and Schneider proved that if a and bare algebraic numbers and if b is irrational, then ab is transcendental. This answered in the affirmative the question raised by Hilbert whether 2..J2 was transcendental. For those interested in pursuing the subject of transcendental numbers further, we would strongly recommend the charming books by C. L. Siegel, entitled Transcendental Numbers, and by I. Niven, Irrational Numbers. To prove that e is irrational is easy; to prove that n is irrational is much more difficult. For a very clever and neat proof of the latter, see the paper by Niven entitled \"A simple proof that n is irrational,\" Bulletin of the American Mathematical Society, Vol. 53 ( 194 7), page 509. Now to the transcendence of e. Aside from its intrinsic interest,-its proof offers us a change ofpace. Up to this point all our arguments have been of an algebraic nature; now, for a short while, we return to the more familiar J --- Sec. 5.2 Transcendence of e 217 grounds of the calculus. The proof itself will use only elementary calculus; the deepest result needed, therefrom, will be the mean value theorem. THEOREM 5.2.1 The number e is transcendental. Proof. In the proof we shall use the standard notation j(i>(x) to denote the ith derivative off (x) with respect to x. Suppose that f (x) is a polynomial of degree r with real coefficients. Let F(x) = f(x) + j(l>(x) + j(2>(x) + · · · + j<r>(x). We compute (dfdx) (e-xF(x)); using the fact thatj<r+ 1>(x) = 0 (sincef (x) is of degree r) and the basic property of e, namely that (djdx)ex = ex, we obtain (dfdx)(e-xF(x)) = -e-xf(x). The mean value theorem asserts that if g(x) is a continuously differentiable, single-valued function on the closed interval [x 1 , x2 ] then where 0 < 0 < 1. We apply this to our function e-xF(x), which certainly satisfies all the required conditions for the mean value theorem on the closed interval [x1 , x2 ] where x1 = 0 and x2 = k, where k is any positive integer. We then obtain that e-kF(k) - F(O) = -e- 8k\"f(Okk)k, where Ok depends on k and is some real number between 0 and 1. Multiplying this relation through by I' yields F(k) - F(O)e!c = -e<l-Bk>\"J (Okk)k. We write this out explicitly: F(l) - eF(O) = -e(l-ot)f (Od = s1 , F(2) - e2F(O) = -2e 2< 1 - 82>j(202 ) = s2 , ~.c 1) Suppose now that e is an algebraic number; then it satisfies some relation of the form (2) whe;t c0 , c1 , • •• , en are integers and where c0 > 0. In the relations ( 1) let us multiply the first equation by c1 , the second by C2, and so on; adding these up we get c1F(l) + c2 F(2) + · · · + cnF(n) - F(O)(c1e + c2e 2 + · · · + en~) = c1s1 + c2 s2 + · · · + CnBn- In view of relation (2), c1e + c2e2 + · · · + cnen = -c 0 , whence the above equation simplifies to All this discussion has held for the F(x) constructed from an arbitrary 218 Fields Ch. 5 polynomial f (x). We now see what all this implies for a very specific polynomial, one first used by Hermite, namely, f (x) = 1 xP- 1 (1 - x)P(2 - x)P · · · (n - x)P. (P - 1)! Here p can be any prime number chosen so that p > n and p > c0 . For this polynomial we shall take a very close look at F(O), F(1), ... , F(n) and we shall carry out an estimate on the size of e1 , e2 , • •• , en. When expanded,J (x) is a polynomial of the form (n !)P p- 1 aoxP + a1 xP+ 1 --'--'-- X + + ... ' (p-1)! (p-1)! (p-1)! where a0 , a 1, ••• , are integers. When i ~ p we claim that J(i>(x) is a polynomial, with coefficients which are integers all of which are multiples of p. (Prove! See Problem 2.) Thus for any integer j, f < i) ( j), for i ~ p, is an integer and is a multiple of p. Now, from its very definition,J(x) has a root of multiplicity pat x = 1, 2, ... , n. Thus for j = 1, 2, ... , n,J (j) = O,J(l>(j) = 0, ... , J<P- 1>(j) = 0. However, F(j) = f (j) + J(l>(j) + · · · + J<P- 1>(j) + J<P>(j) + · · · + J<r>(j); by the discussion above, for j = 1, 2, ... , n, F(j) is an integer and is a multiple of p. What about F(O)? Since f (x) has a root of multiplicity p - 1 at x = 0, f (0) = J(l>(O) = · · · = J<P- 2 >(0) = 0. For i ~ p, J<i)(O) is an integer which is a multiple of p. But J<P- 1>(0) = (n!)P and since p > n and is a prime number, p ~ (n!)P so that J<P- 1>(0) is an integer not divisible by p. Since F(O) = f (0) + j(l>(O) + · · · + J<P- 2>(0) + J<P- 1>(0) + J<P>(O) + · · · + J<r>(O), we conclude that F(O) is an integer not divisible by p. Because c0 > 0 and p > c0 and because p ~ F(O) whereas pI F(l), pI F(2), ... , pI F(n), we can assert that c0 F(O) + c1F(l) + · · · + cnF(n) is an integer and is not divisible by p. However, by (3), c0F(O) + c1F(l) + · · · + cnF(n) = c1e1 + · · · + cnen. What can we say about ei? Let us recall that where 0 < (}i < 1. Thus Asp-+oo, Sec. 5.3 Roots of Polynomials 219 (Prove!) whence we can find a prime number larger than both c0 and n and large enough to force lc1e1 + · · · + cnenl < 1. But c1e1 + · · · + cnen = c0F(O) + · · · + c,!l(n), so must be an integer; since it is smaller than 1 in size our only possible conclusion is that c1 e1 + · · · + cnen = 0. Conse- quently, c0F(O) + · · · + cnF(n) = 0; this however is sheer nonsense, since we know that p ,r (c0 F(O) + · · · + cnF(n)), whereas pI 0. This contradic- tion, stemming from the assumption that e is algebraic, proves that e must be transcendental. Problems 1. Using the infinite series for e, e= 1 I 1 1 1+-+-+-+···+-+· .. 1! 2! 3! m! ' prove that e is irrational. 2. If g(x) is a polynomial with integer coefficients, prove that if pis a prime number then for i ~ p, di ( g(x) ) dx' (p - 1)! is a polynomial with integer coefficients each of which is divisible by p. 3. If a is any real number, prove that (amfm!) --+ 0 as m --+ 00. 4. If m > 0 and n are integers, prove that emln is transcendental. 6.3 Roots of Polynomials In Section 5.1 we discussed elements in a given extension K ofF which were algebraic over F, that is, elements which satisfied polynomials in F[x]. We now turn the problem around; given a polynomial p(x) in F[x] we wish to find a field K which is an extension ofF in which p(x) has a root. No longer is the field K available to us; in fact it is our prime objective to construct it.· Once it is constructed, we shall examine it more closely and see what consequences we can derive. / DEFINITION If p(x) E F[x], then an element a lying in some extension field ofF is called a root of p(x) if p(a) = 0. We begin with the familiar result known as the Remainder Theorem. LEMMA 5.3.1 If p(x) E F[x] and if K is an extension ofF, then for any ele- ment bE K,p(x) = (x - b)q(x) + p(b) where q(x) E K[x] and where deg q(x) deg p(x) - 1. 220 Fields Ch. 5 Proof. Since F c K, F[x] is contained in K[x], whence we can con- sider p(x) to be lying in K[x]. By the division algorithm for polynomials in K[x], p(x) = (x - b)q(x) + r, where q(x) E K[x] and where r = 0 or deg r < deg (x - b) = 1. Thus either r = 0 or deg r = 0; in either case r must be an element of K. But exactly what element of K is it? Since p(x) = (x - b)q(x) + r, p(b) = (b - b)q(b) + r = r. Therefore, p(x) = (x - b)q(x) + p(b). That the degree of q(x) is one less than that of p(x) is easy to verify and is left to the reader. COROLLARY If a E K is a root of p(x) E F[x], where F c K, then in K[x], (x - a) I p(x). Proof. From Lemma 5.3.1, in K[x], p(x) = (x - a)q(x) + p(a) (x - a)q(x) since p(a) = 0. Thus (x - a) I p(x) in K[x]. DEFINITION The element a E K is a root of p(x) E F[x] of multiplicity m if (x - a)m I p(x), whereas (x - a)m+ 1 _.r p(x). A reasonable question to ask is, How many roots can a polynomial have in a given field? Before answering we must decide how to count a root of multiplicity m. We shall always count it as m roots. Even with this convention we can prove LEMMA 5.3.2 A polynomial of degree n over a field can have at most n roots in any extension field. Proof. We proceed by induction on n, the degree of the polynomialp(x). If p(x) is of degree 1, then it must be of the form r:xx + f3 where r:x, f3 are in a field F and where r:x =I= 0. Any a such that p(a) = 0 must then imply that r:xa + f3 = 0, from which we conclude that a = ( --/3/r:x). That is, p(x) has the unique root - /3/r:x, whence the conclusion of the lemma certainly holds in this case. Assuming the result to be true in any field for all polynomials of degree less than n, let us suppose that p(x) is of degree n over F. Let K be any extension of F. If p(x) has no roots inK, then we are certainly done, for the number of roots in K, namely zero, is definitely at most n. So, suppose that p(x) has at least one root a E K and that a is a root of multiplicity m. Since (x - a)m I p(x), m :::;;; n follows. Now p(x) = (x - a)mq(x), where q(x) E K[x] is of degree n - m. From the fact that (x - a)m+ 1 ,V p(x), we get that (x- a) _.r q(x), whence, by the corollary to Lemma 5.3.1, a is not a root of q(x). If b =I= a is a root, in K, of p(x), then 0 = p(b) = (b ~ a)mq(b); however, since b - a =1= 0 and since we are in a field, we conclude that q(b) = 0. That is, any root of p(x), in K, other than a, must be a root of 1 Sec. 5.3 Roots of Polynomials 221 q(x). Since q(x) is of degree n - m < n, by our induction hypothesis, q(x) has at most n - m roots in K, which, together with the other root a, counted m times, tells us that p(x) has at most m + (n - m) = n roots in K. This completes the induction and proves the lemma. One should point out that commutativity is essential in Lemma 5.3.2. If we consider the ring of real quaternions, which falls short of being a field only in that it fails to be commutative, then the polynomial x 2 + 1 has at least 3 roots, i,j, k (in fact, it has an infinite number of roots). In a some- what different direction we need, even when the ring is commutative, that it be an integral domain, for if ab = 0 with a i= 0 and b i= 0 in the com- mutative ring R, then the polynomial ax of degree 1 over R has at least two distinct roots x = 0 and x = b in R. The previous two lemmas, while interesting, are of subsidiary interest. We now set ourselves to our prime task, that of providing ourselves with suitable extensions ofF in which a given polynomial has roots. Once this is done, we shall be able to analyze such extensions to a reasonable enough degree of accuracy to get results. The most important step in the cons-truction is accomplished for us in the next theorem. The argument used will be very reminiscent of some used in Section 5.1. THEOREM 5.3.1 If p(x) is a polynomial in F[x] of degree n ~ 1 and is irreducible over F, then there is an extension E ofF, such that [E:F] = n, in which p(x) has a root. Proof. Let F [ x] be the ring of polynomials in x over F and let V = (p(x)) be the ideal of F[x] generated by p(x). By Lemma 3.9.6, Vis a maximal ideal of F[x], whence by Theorem 3.5.1, E = F[x]fV is a field. This E will be shown to satisfy the conclusions of the theorem. First we want to show that E is an extension ofF; however, in fact, it is not! But let F be the image ofF in E; that is, F = {oc + VI oc E F}. We assert that F is a field isomorphic to F; in fact, if t/J is the mapping from F[x] into F[x]fV= E defined byf(x)t/J =f(x) + V, then the restriction of t/J to F induces an isomorphism ofF onto F. (Prove!) Using this iso- morphism, we identify F and F; in this way we can consider E to be an extension of/fK We claim that E is a finite extension ofF of degree n = deg p(x), for the elements!+ V,x+ V, (x+ V) 2 =x 2 + V, ... ,(x+ V)i=xi+ V, ... , (x + V)n- 1 = xn- 1 + V form a basis of E over F. (Prove!) For con- venience of notation let us denote the element xt/J = x + V in the field E as a. Given f (x) E F[x], what is f (x)t/J? We claim that it is merely ,f(a), for, since t/1 is a homomorphism, if f(x) =Po+ P1x + · · · + P0\\ then f (x)t/J = Pot/1 + (P 1 t/1) (xt/J) + · · · + (Pkt/1) (xt/J)\\ and using the identification indicated above of Pt/1 with p, we see that f(x)t/J =f(a). 222 Fields Ch. 5 In particular, since p(x) E V, p(x)t/J = 0; however, p(x)t/J = p(a). Thus the element a = xt/J in Eisa root of p(x). The field E has been shown to satisfy all the properties required in the conclusion of Theorem 5.3.1, and so this theorem is now proved. An immediate consequence of this theorem is the COROLLARY If f(x) E F[x], then there is a finite extension E ofF in which f(x) has a root. Moreover, [E:F] :$; degf(x). Proof. Let p(x) be an irreducible factor off (x); any root of p(x) Is a root ofj(x). By the theorem there is an extension E ofF with [E:F] = degp(x) :$; deg f(x) in whichp(x), and so,J(x) has a root. Although it is, in actuality, a corollary to the above corollary, the next theorem is of such great importance that we single it out as a theorem. THEOREM 5.3.2 Let f(x) E F[x] be of degree n ~ 1. Then there is an ex- tension E ofF of degree at most n! in whichf(x) has n roots (and so, a full com- plement of roots). Proof. In the statement of the theorem, a root of multiplicity m is, of course, counted as m roots. By the above corollary there is an extension E 0 ofF with [ E 0 :F] :$; n in whichf(x) has a root ex. Thus in E0 [x],J(x) factors asf(x) = (x- cx)q(x), where q(x) is of degree n - l. Using induction (or continuing the above process), there is an extension E of E 0 of degree at most (n - 1)! in which q(x) has n - 1 roots. Since any root ofj(x) is either ex or a root of q(x), we obtain in E all n roots ofj(x). Now, [E:F] = [E:E0][E0 :F] :$; (n- l)!n = n! All the pieces of the theorem are now established. Theorem 5.3.2 asserts the existence of a finite extension E in which the given polynomial f (x), of degree n, over F has n roots. Iff (x) = a0xn + a 1xn-l + · · · + am a0 =/= 0 and if the n roots in E are cx1 , ... , C<m making use of the corollary to Lemma 5.3.1,/(x) can be factored over E asf(x) = a0 (x - cx1)(x - cx2 ) • • • (x - cxn)· Thus f(x) splits up completely over E as a product of linear (first degree) factors. Since a finite extension ofF exists with this property, a finite extension ofF of minimal degree exists which also enjoys this property of decomposing/ (x) as a product of linear factors. For such a minimal extension, no proper subfield has the property that f (x) factors over it into the product of linear factors. This prompts the DEFINITION If f(x) E F[x], a finite extension E ofF is said- to be a splitting field over F for f(x) if over E (that is, in E[x]), but not over any proper subfield of E, f (x) can be factored as a product of linear factors. 1 Sec. 5.3 Roots of Polynomials 223 We reiterate: Theorem 5.3.2 guarantees for us the existence of splitting .fields. In fact, it says even more, for it assures that given a polynomial of degree n over F there is a splitting field of this polynomial which is an extension of F of degree at most n! over F. We shall see later that this upper bound of n! is actually taken on; that is, given n, we can find a field F and a poly- nomial of degree n in F[x] such that the splitting field off(x) over F has degree n!. Equivalent to the definition we gave of a splitting field for f (x) over F is the statement: E is a splitting field off (x) over F if E is a minimal extension ofF in whichf(x) has n roots, where n = deg f(x). An immediate question arises: given two splitting fields E 1 and E 2 of the same polynomial f(x) in F[x], what is their relation to each other? At first glance, we have no right to assume that they are at all related. Our next objective is to show that they are indeed intimately related; in fact, that they are isomorphic by an isomorphism leaving every element of F fixed. It is in this direction that we now turn. Let F and F' be two fields and let T be an isomorphism ofF onto F'. For convenience let us denote the image of any a E F under T by a'; that is, aT = a'. We shall maintain this notation for the next few pages. Can we make use ofT to set up an isomorphism between F[x] and F'[t], the respective polynomial rings over F and F'? Why not try the obvious? For an arbitrary polynomial f(x) = a0xn + a1xn- 1 + · · · + an E F[x] we define T* by f (x)T* = (ao~ + a1Xn- 1 + · · · + an)T* = a~tn + a~ tn- 1 + ... +a~. It is an easy and straightforward matter, which we leave to the reader, to verify. LEMMA 5.3.3 T* defines an isomorphism of F[x] onto F'[t] with the property that aT* = a' for every a E F. Iff(x) is in F[x] we shall writef(x)T* asf'(t). Lemma 5.3.3 immediately implies that factorizations of f(x) in F[x] result in like factorizations of f'(t) in F'[t], and vice versa. In particular, f(x) is irreducible in F[x] if and only iff'(t) is irreducible in F'[t]. )lowever, at the moment, we are not particularly interested in polynomial rings, but rather, in extensions of F. Let us recall that in the proof of Theorem 5.1.2 we employed quotient rings of polynomial rings to obtain suitable extensions of F. In consequence it should be natural for us to study the relationship between F[x]f(f(x)) and F'[t]f(f'(t)), where (f(x)) denotes the ideal generated by f(x) in F[x] and (f'(t)) that generated by f'(t) in F'[t]. The next lemma, which is relevant to this question, is actually part of a more general, purely ring-theoretic result, but we shall content ourselves with it as applied in our very special setting. 224 Fields Ch. 5 LEMMA 5.3.4 There is an isomorphism 1:** if F[x]j(J(x)) onto F'[t]j(j'(t)) withthepropertythatforeve~yt1.EF,rn** = 1:1.', (x + (j(x)))1:** = t + (f'(t)). Proof. Before starting with the proof proper, we should make clear what is meant by the last part of the statement of the lemma. As we have already done several times, we can consider F as imbedded in F[x]/(f(x)) by identifying the element 1:1. E F with the coset 1:1. + (.f(x)) in F[x]/(f(x)). Similarly, we can consider F' to be contained in F'[t]/(f'(t)). The isomorphism 1:** is then supposed to satisfy [1:1. + (j(x))]1:** = 1:1.' + (j'(t)). We seek an isomorphism 1:** of F[x]/(f(x)) onto F'[t]/(f'(t)). What could be simpler or more natural than to try the 1:** defined by [g(x) + (j(x))]1:** = g'(t) + (f'(t)) for every g(x) E F[x]? We leave it as an exercise to fill in the necessary details that the 7:* * so defined is well defined and is an isomorphism of F[x]/(f(x)) onto F'[t]/(J'(t)) with the properties needed to fulfill the statement of Lemma 5.3.4. For our purpose-that of proving the uniqueness of splitting fields- Lemma 5.3.4 provides us with the entering wedge, for we can now prove THEOREM 5.3.3 IJ p(x) is irreducible in F[x] and if vis a root if p(x), then F(v) is isomorphic to F'(w) where w is a root if p'(t); moreover, this isomorphism u can so be chosen that I. V(J = w. 2. 1:1.u = a' for every a E F. Proof. Let v be a root of the irreducible polynomial p(x) lying in some extension K of F. Let M = {f(x) E F[x] I f(v) = 0}. Trivially M is an ideal of F[x], and M =/= F[x]. Since p(x) EM and is an irreducible poly- nomial, we have that M = (p(x)). As in the proof of Theorem 5.1.2, map F[x] into F(v) c K by the mapping l/J defined by q(x)l/J = q(v) for every q(x) E F[x]. We saw earlier (in the proof of Theorem 5.1.2) that l/J maps F[x] onto F(v). The kernel of l/J is precisely M, so must be (p(x)). By the fundamental homomorphism theorem for rings there is an isomorphism tf;* of F[x]f(p(x)) onto F(v). Note further that atf;* = a for every 1:1. E F. Summing up: l/J* is an isomorphism of F[x]f(p(x)) onto F(v) leaving every element ofF fixed and with the property that v = [x + (p(x))Jl/J*. Since p(x) is irreducible in F[x], p'(t) is irreducible in F'[t] (by Lemma 5.3.3), and so there is an isomorphism()* of F'[t]f(p'(t)) onto F'(w) where w is a root of p' (t) such that ()* leaves every element ofF' fixed and such that [t + (p'(t)]()* = w. We now stitch the pieces together to prove Theorem 5.3.3. By· Lemma 5.3.4 there is an isomorphism 1:** of F[x]f(p(x)) onto F'[t]f(p'(t)) which coincides with 7: on F and which takes x + (p(x)) onto t + (p'(t)). Con- 1 Sec. 5.3 Roots of Polynomials 225 sider the mapping a = (t/1*) - 1r**8* (motivated by ( t/1*) - 1 F [X] t** F I [ t] 0* F(v)--)> -- ~ ----+ F'(w)) (p(x)) (p'(t)) of F(v) onto F'(w). It is an isomorphism of F(v) onto F'(w) since all the mapping t/J*, r**, and 8* are isomorphisms and onto. Moreover, since v = [x + (p(x))]t/1*, va = (v(t/J*)- 1)r**8* = ([x + (p(x)]r**)8* = [t + (p'(t))]8* = w. Also, for rxEF, rxa = (rx(t/J*)- 1)r**8* = (rxr**)8* = a,'()* = rx'. We have shown that a is an isomorphism satisfying all the requirements of the isomorphism in the statement of the theorem. Thus Theorem 5.3.3 has been proved. A special case, but itself of interest, is the COROLLARY If p(x) E F[x] is irreducible and if a, b are two roots of p(x), then F (a) is isomorphic to F (b) by an isomorphism which takes a onto b and which leaves every element ofF fixed. We now come to the theorem which is, as we indicated earlier, the foundation stone on which the whole Galois theory rests. For us it is the focal point of this whole section. THEOREM 5.3.4 Any splitting fields E and E' of the polynomials f(x) E F[x] and f'(t) E F'[t], respectively, are isomorphic by an isomorphism cp with the prop- erry that rxcp = rx' for every rx E F. (In particular, any two splitting .fields of the same polynomial over a given field F are isomorphic by an isomorphism leaving every element ofF fixed.) Proof. We should like to use an argument by induction; in order to do so, we need an integer-valued indicator of size which we can decrease by some technique or other. We shall use as our indicator the degree of some splitting field over the initial field. It may seem artificial (in fact, it may even be artificial), but we use it because, as we shall soon see, Theorem 5.3.3 provides us with the mechanism for decreasing it. If [E:F] = 1, then E = F, whencef(x) splits into a product of linear fac~rs over F itself. By Lemma 5.3.3f'(t) splits over F' into a product of linear factors, hence E' = F'. But then cp = r provides us with an iso- morphism of E onto E' coinciding with ron F. Assume the result to be true for any field F 0 and any polynomial f (x) E Fo[x] provided the degree of some splitting field E0 off(x) has degree less than n over F 0 , that is, [E0 :F0 ] < n. Suppose that [E:F] = n > 1, where E is a splitting field off( x) over F. Since n > 1, f(x) has an irreducible factor p(x) of degree r > 1. Let P'(t) be the corresponding irreducible factor off'(t). Since E splitsf(x), a 226 Fields Ch. 5 full complement of roots off (x), and so, a priori, of roots of p(x), are in E. Thus there is avE E such that p(v) = 0; by Theorem 5.1.3, [F(v) :F] = r. Similarly, there is a wEE' such that p'(w) = 0. By Theorem 5.3.4 there is an isomorphism u of F(v) onto F'(w) with the property that au = a' for every a E F. Since [F(v) :F] = r > 1, [E:F(v)] [E:F] [F(v) :F] n =- < n. r We claim that Eisa splitting field for f (x) considered as a polynomial over F0 = F(v), for no subfield of E, containing F0 and hence F, can splitf (x), since E is assumed to be a splitting field off (x) over F. Similarly E' is a splitting field forf'(t) over F~ = F'(w). By our induction hypothesis there is an isomorphism 4> of E onto E' such that acf> = au for all a E F0 • But for every a E F, au = a' hence for every a E F c F0 , acf> = au = a'. This completes the induction and proves the theorem. To see the truth of the \"(in particular ... )\" part, let F = F' and let 1: be the identity map at = a for every a E F. Suppose that E 1 and £ 2 are two splitting fields of f(x) E F[x]. Considering E 1 = E;:) F and £ 2 = E' ;:) F' = F, and applying the theorem just proved, yields that £ 1 and E2 are isomorphic by an isomorphism leaving every element ofF fixed. In view of the fact that any two splitting fields of the same polynomial over F are isomorphic and by an isomorphism leaving every element of F fixed, we are justified in speaking about the splitting field, rather than a splitting field, for it is essentially unique. Examples 1. Let F be any field and let p(x) = x 2 + ax + p, a, p E F, be in F[x]. If K is any extension ofF in which p(x) has a root, a, then the element b = -a - a also in K is also a root of p(x). If b = a it is easy to check that p(x) must then be p(x) = (x - a) 2 , and so both roots of p(x) are in K. If b =/= a then again both roots of p(x) are in K. Consequently, p(x) can be split by an extension of degree 2 of F. We could also get this result directly by invoking Theorem 5.3.2. 2. Let F be the field of rational numbers and letf (x) = x 3 - 2. In the field of complex numbers the three roots off (x) are '!../2, w'!../2, w 2 '!../2, where OJ = ( -1 + .J3 i)/2 and where '!../2 is a real cube root of 2. Now F('!../2) cannot split x 3 - 2, for, as a subfield of the real field-, it cannot contain the complex, but not real, number w'!../2. Without explicitly determining it, what can we say about E, the splitting field of x 3 - 2 over Sec. 5.3 Roots of Polynomials J F? By Theorem 5.3.2, [E:F] ~ 3! = 6; by the above remark, since x3 - 2 is irreducible over F and since [F(V2) :F] = 3, by the corollary to Theorem 5.1.1, 3 = [F(V2) :F] I [E:F]. Finally, [E:F] > [F(tf2) :F] = 3. The only way out is [E:F] = 6. We could, of course, get this result by making two extensions F 1 = F(V2) and E = F 1 (OJ) and showing that OJ satisfies an irreducible quadratic equation over F 1 . 3. Let F be the field of rational numbers and let f(x) = x4 + x 2 + 1 E F[x]. We claim that E = F(OJ), where OJ = ( -1 + )3 i)/2, is a splitting field off(x). Thus [E:F] = 2, far short of the maximum possible 4! = 24. Problems I. In the proof of Lemma 5.3.1, prove that the degree of q(x) is one less than that of p(x). 2. In the proof of Theorem 5.3.1, prove in all detail that the elements 1 + V, x + V, ... , xn- 1 + V form a basis of E over F. 3. Prove Lemma 5.3.3 in all detail. 4. Show that -r** in Lemma 5.3.4 is well defined and is an isomorphism of F[x]/(f(x)) onto F[t]/(f'(t)). 5. In Example 3 at the end of this section prove that F(OJ) is the splitting field of x 4 + x 2 + 1. 6. Let F be the field of rational numbers. Determine the degrees of the splitting fields of the following polynomials over F. (a) x4 + 1. (b) x 6 + 1. (c) x 4 - 2. (d) x 5 - 1. (e) x6 + x 3 + 1. 7. If p is a prime number, prove that the splitting field over F, the field of rational numbers, of the polynomial xP - 1 is of degree p - 1. •*8. If n > 1, prove that the splitting field of xn - 1 over the field of rational numbers is of degree <P(n) where <P is the Euler <P-function. 'I (This is a well-known theorem. I know of no easy solution, so don't be disappointed if you fail to get it. If you get an easy proof, I would like to see it. This problem occurs in an equivalent form as Problem 15, Section 5.6.) *9. If F is the field of rational numbers, find necessary and sufficient conditions on a and b so that the splitting field of x 3 + ax + b has degree exactly 3 over F. 10. Let p be a prime number and let F = JP, the field of integers mod p. (a) Prove that there is an irreducible polynomial of degree 2 over F. / 228 Fields Ch. 5 (b) Use this polynomial to construct a field with p 2 elements. * (c) Prove that any two irreducible polynomials of degree 2 over F lead to isomorphic fields with p2 elements. 11. If E is an extension ofF and iff (x) E F[x] and if 4> is an automor- phism of E leaving every element ofF fixed, prove that 4> must take a root off (x) lying in E into a root off (x) in E. 12. Prove that F(V2), where F is the field of rational numbers, has no automorphisms other than the identity automorphism. 13. Using the result of Problem 11, prove that if the complex number a is a root of the polynomial p(x) having real coefficients then ~' the complex conjugate of a, is also a root of p(x). 14. Using the result of Problem 11, prove that if m is an integer which is not a perfect square and if a + p..j-; (a, fJ rational) is the root of a polynomial p(x) having rational coefficients, then a - p.,J-; is also a root ofp(x). * 15. IfF is the field of real numbers, prove that if 4> is an automorphism ofF, then 4> leaves every element ofF fixed. 16 (a) Find all real quaternions t = a0 + a1i + a2 j + a3k satisfying t 2 = -1 *(b) For at as in part (a) prove we can find a real quaternion s such that sts- 1 = i. 5.4 Construction with Straightedge and Compass We pause in our general development to examine some implications of the results obtained so far in some familiar, geometric situations. A real number a is said to be a constructible number if by the use of straight- edge and compass alone we can construct a line segment of length a. We assume that we are given some fundamental unit length. Recall that from high-school geometry we can construct with a straightedge and compass a line perpendicular to and a line parallel to a given line through a given point. From this it is an easy exercise (see Problem 1) to prove that if a and fJ are constructible numbers then so are a ± {J, a{J, and when fJ # 0, af {J. Therefore, the set of constructible numbers form a subfield, W, of the field of real numbers. In particular, since 1 E W, W must contain F0 , the field of rational numbers. We wish to study the relation of W to the rational field. Since we shall have many occasions to use the phrase \"construct by straightedge and compass\" (and variants thereof) the words construct, con- structible, construction, will always mean by straightedge and compass. · If w E W, we can reach w from the rational field by a finite number of constructions. l Sec. 5.4 Construction with Straightedge and Compass 229 Let F be any subfield of the field of real numbers. Consider all the points (x,y) in the real Euclidean plane both of whose coordinates x andy are in F; we call the set of these points the plane of F. Any straight line joining two ts in the plane of F has an equation of the form ax + by + c = 0 a, b, c are all in F (see Problem 2). Moreover, any circle having as a point in the plane ofF and having as radius an element ofF has equation of the form x 2 + y 2 + ax + by + c = 0, where all of a, b, c in F (see Problem 3). We call such lines and circles lines and circles 'in F. Given two lines in F which intersect in the real plane, then their inter- section point is a point in the plane ofF (see Problem 4). On the other hand, the intersection of a line in F and a circle in F need not yield a point in the plane of F. But, using the fact that the equation of a line in F is of the form ax + by + c = 0 and that of a circle in F is of the form x 2 + y 2 + dx + ~ + f = 0, where a, b, c, d, e,J are all in F, we can show that when a line and circle ofF intersect in the real plane, they intersect either in a point in the plane ofF or in the plane ofF( -Jy) for some positive yin F (see Problem 5). Finally, the intersection of two circles in F can be realized as that of a line in F and a circle in F, for if these two circles are x 2 + y 2 + a1x + b1y + c1 = 0 and x 2 + y 2 + a2 x + b2 y + c2 = 0, then their intersection is the intersection of either of these with the line (a1 - a2 )x + (b1 - b2 ) y + {c1 - c2 ) = 0, so also yields a point either in the plane ofF or ofF( -Jy) for some positive yin F. Thus lines and circles ofF lead us to points either in For in quadratic extensions of F. If we now are in F( -Jy1 ) for some quadratic extension of F, then lines and circles in F( -Jy1) intersect in points in the plan,e of F( -Jy1 , -Jy2 ) where y2 is a positive number in F( -Jy1). A point is con- structible from F if we can find real numbers A1, ••• , Am such that A1 2 E F, A22 E F(A 1), A3 2 E F(A 1 , A2 ), ••• , An2 E F(A1 , ... , An_ 1 ), such that the point is in the plane of F(A1, ..• , An)· Conversely, if y E F is such that -Jy is real then we can realize y as an intersection of lines and circles in F (see Problem 6). Thus a point is constructible from F if and only if we can find a finite number of real numbers A1 , ... , Am such that I. [F(A1 ) :F] = 1 or 2; /2. [F(A1 , ... , Ai) :F(A1 , ..• , Ai_ 1 )] = 1 or 2 fori = 1, 2, ... , n; and such that our point lies in the plane of F(A1 , ... , An)· We have defined a real number a to be constructible if by use of straight- edge and compass we can construct a line segment of length a. But this translates, in terms of the discussion above, into: a is constructible if starting from the plane of the rational numbers, F0 , we can imbed a in a field obtained from F 0 by a finite number of quadratic extensions. This is 230 Fields Ch. 5 THEOREM 5.4.1 The real number ex is constructible if and only if we can find afinite number ofreal numbers A-1 , ... , An such that 1. A-1 2 E F 0 , 2. A./ E F0 (A.1, ... , A.i_1) fori = 1, 2, ... , n, such that ex E F0 (A.1, •.. , An)· However, we can compute the degree of F 0 (A.1, ... , An) over F0 , for by Theorem 5.1.1 [Fo(A.l, · · ·' An) :Fo] = [Fo(Au · · ·' An) :Fo(A-1, · · ·' An-1)] · · · X [Fo(A.l, · · ·, A.i) :Fo(A-1, · · ·, Ai-l)] · · · x [F0 (A.1 ) :F0]. Since each term in the product is either 1 or 2, we get that and thus the COROLLARY 1 If ex is constructible then ex lies in some extension of the rationals of degree a power of 2. If ex is constructible, by Corollary 1 above, there is a subfield K of the real field such that ex E K and such that [K:F0 ] = 2r. However, F0 (ex) c K, whence by the corollary to Theorem 5.1.1 [F0 (ex) :F0] I [K:F0 ] = 2r; thereby [F0 (ex) :F0] is also a power of 2. However, if ex satisfies an irreducible polynomial of degree k over F0 , we have proved in Theorem 5.1.3 that [F0 (ex) :F0 ] = k. Thus we get the important criterion for nonconstructibility COROLLARY 2 If the real number ex satisfies an irreducible polynomial over the field of rational numbers of degree k, and if k is not a power of 2, then ex is not constructible. This last corollary enables us to settle the ancient problem of trisecting an angle by straightedge and compass, for we prove THEOREM 5.4.2 It is impossible, by straightedge and compass alone, to trisect 60°. Proof. If we could trisect 60° by straightedge and compass, then the length ex = cos 20° would be constructible. At this point, let us recall the identity cos 3(} = 4 cos 3 (} - 3 cos(}. Putting (} = 20° and remembering that cos 60° = t, we obtain 4ex 3 - 3ex = t, whence 8ex 3 - 6ex - 1 = 0. Thus ex is a root of the polynomial 8x 3 - 6x - 1 over the rational field. l Sec. 5.4 Construction with Straightedge and Compass 231 ~ ..... ,ATP1JP.r, this polynomial is irreducible over the rational field (Problem and since its degree is 3, which certainly is not a power of 2, by 2 to Theorem 5.4.1, ex is not constructible. Thus 60° cannot be n.:;{;'~'-'\"·~ by straightedge and compass. Another ancient problem is that of duplicating the cube, that is, of a cube whose volume is twice that of a given cube. If the cube is the unit cube, this entails constructing a length ex such that = 2. Since the polynomial x 3 - 2 is irreducible over the rationals blem 7(b)), by Corollary 2 to Theorem 5.4.1, ex is not constructible. By straightedge and compass it is impossible to duplicate the We wish to exhibit yet another geometric figure which cannot be con- by straightedge and compass, namely, the regular septagon. To out such a construction would require the constructibility of ex = cos (2n/7). However, we claim that ex satisfies x 3 + x 2 - 2x - 1 rr<>011em 8) and that this polynomial is irreducible over the field of rational ••. u ..... ...,..._ .• ., (Problem 7(c)). Thus again using Corollary 2 to Theorem 5.4.1 5.4.4 It is impossible to construct a regular septagon by straightedge Prove that if ex, {3 are constructible, then so are ex ± {3, ex/3, and exj {3 (when {3 =f. 0). Prove that a line in F has an equation of the form ax + by + c = 0 with a, b, c in F. 3. Prove that a circle in F has an equation of the form x 2 + y 2 + ax + by + c = 0, rwith a, b, c in F. 4. Prove that two lines in F, which intersect in the real plane, intersect at a point in the plane of F. 5. Prove that a line in F and a circle in F which intersect in the real plane do so at a point either in the plane ofF or in the plane ofF( .Jy) where y is a positive number in F. 6. If y E F is positive, prove that .Jy is realizable as an intersection of lines and circles in F. 232 Fields Ch. 5 7. Prove that the following polynomials are irreducible over the field of rational numbers. (a) 8x 3 - 6x - 1. (b) x 3 - 2. (c) x 3 + x 2 - 2x - 1. 8. Prove that 2 cos (2n/7) satisfies x 3 + x 2 - 2x - 1. (Hint: Use 2 cos (2n/7) = e21r.i/? + e- 2 1r.if1.) 9. Prove that the regular pentagon is constructible. 10. Prove that the regular hexagon is constructible. 11. Prove that the regular 15-gon is constructible. 12. Prove that it is possible to trisect 72°. 13. Prove that a regular 9-gon is not constructible. * 14. Prove a regular 1 7 -gon is constructible. 5.5 More about Roots We return to the general exposition. Let F be any field and, as usual, let F[x] be the ring of polynomials in x over F. DEFINITION If f(x) = a0xn + a1~- 1 + · · · + aixn-i + · · · + an_ 1x + an in F[x], then the derivative ofj(x), written asf'(x), is the polynomial f' (x) = na0 xn- 1 + (n -1 )a1~- 2 + · · · + (n - i)aixn- i- 1 + · · · + C(n-l in F[x]. To make this definition or to prove the basic formal properties of the derivatives, as applied to polynomials, does not require the concept of a limit. However, since the field F is arbitrary, we might expect some strange things to happen. At the end of Section 5.2, we defined what is meant by the characteristic of a field. Let us recall it now. A field F is said to be of characteristic 0 if ma i= 0 for a i= 0 in F and m > 0, an integer. Ifma = 0 for some m > 0 and some a i= 0 E F, then F is said to be of finite characteristic. In this second case, the characteristic ofF is defined to be the smallest positive integer p such that pa = 0 for all a E F. It turned out that ifF is of finite characteristic then its characteristic pis a prime number. We return to the question of the derivative. Let F be a field of character- istic p i= 0. In this case, the derivative of the polynomial xP is pxP- 1 = 0. Thus the usual result from the calculus that a polynomial whose derivative is 0 must be a constant no longer need hold true. However, if the charac- teristic ofF is 0 and if f'(x) = 0 for f(x) E F[x], it is indeed true that J (x) = a E F (see Problem 1). Even when the characteristic of F is ~~ p =1= 0, we can still describe the polynomials with zero derivative; if f' (x) = 0, thenf (x) is a polynomial in xP (see Problem 2). ·. Sec. 5.5 More About Roots 233 We now prove the analogs of the formal rules of differentiation that we so well. For any f (x), g(x) E F[x] and any ex E F, (J(x) + g(x))' =f'(x) + g'(x). ( cxf ( x))' = cxj' ( x) . (f(x)g(x))' = f'(x)g(x) + f(x)g'(x). Proof. The proofs of parts 1 and 2 are extremely easy and are left as <:exercises. To prove part 3, note that from parts 1 and 2 it is enough to ':prove it in the highly special case f (x) = xi and g(x) = xi where both i and j are positive. But then f(x)g(x) = xi+i, whence (f(x)g(x))' = (i +j)xi+i- 1 ; however, J'(x)g(x) = ixi- 1xi = ixi+i- 1 and f(x)g'(x) = jx1xi- 1 = jxi+ i- 1 ; consequently,!' (x) g(x) + f (x) g' (x) = (i + j)xi+ i- 1 = (f(x)g(x))'. Recall that in elementary calculus the equivalence is shown between the existence of a multiple root of a function and the simultaneous vanishing of the function and its derivative at a given point. Even in our setting, where F is an arbitrary field, such an interrelation exists. LEMMA 5.5.2 The polynomial f (x) E F[x] has a multiple root if and only if f(x) andf'(x) have a nontrivial (that is, of positive degree) common factor. Proof. Before proving the lemma proper, a related remark is in order, namely, iff (x) and g(x) in F[x] have a nontrivial common factor in K[x], for K an extension ofF, then they have a nontrivial common factor in F [ x]. For, were they relatively prime as elements in F [ x], then we would be able to find two polynomials a(x) and b(x) in F[x] such that a(x)f(x) + b(x) g(x) = 1. Since this relation also holds for those elements viewed as elements of K[x], in K[x] they would have to be relatively prime. Now to the lemma itself. From the remark just made, we may assume, without loss of generality, that the roots off (x) all lie in F (otherwise ex- tend F to K, ·the splitting field ofj(x)). lfj(x) has a multiple root ex, then f(x) = (x - cx)mq(x), where m > I. However, as is easily computed, ((x 1- cx)m)' = m(x - cx)m- 1 whence, by Lemma 5.5.1, f'(x) = (x - cx)mq'(x) + m(x - cx)m- 1q(x) = (x - cx)r(x), since m > I. But this thatj(x) andf'(x) have the common factor x- ex, thereby proving the lemma in one direction. On the other hand, if f (x) has no multiple root then f (x) = (x - cx1) (x - cx2 ) • • • (x - an) where the cx;'s are all distinct (we are supposingf(x) to be monic). But then f'(x) 234 Fields Ch. 5 where the A denotes the term is omitted. We claim no root off (x) Is a root ofj'(x), for since the roots are all distinct. However, ifj (x) andf' (x) have a nontrivial common factor, they have a common root, namely, any root of this common factor. The net result is thatf(x) andf'(x) have no nontrivial common factor, and so the lemma has been proved in the other direction. COROLLARY 1 lff(x) E F[x] is irreducible, then 1. If the characteristic ofF is O,j (x) has no multiple roots. 2. If the characteristic ofF is p =1- 0, f (x) has a multiple root only if it is of the formf (x) = g(xP). Proof. Sincef(x) is irreducible, its only factors in F[x] are 1 andf(x). Ifj (x) has a multiple root, thenf (x) and.f' (x) have a nontrivial common factor by the lemma, hence f (x) If' (x). However, since the degree off' (x) is less than that ofj(x), the only possible way that this can happen is for f'(x) to be 0. In characteristic 0 this implies thatf(x) is a constant, which has no roots; in characteristic p =1- 0, this forces f (x) = g(xP). We shall return in a moment to discuss the implications of Corollary 1 more fully. But first, for later use in Chapter 7 in our treatment of finite fields, we prove the rather special COROLLARY 2 If F is a field of characteristic p =1- 0, then the polynomial xP\" - x E F[x], for n ~ 1, has distinct roots. Proof. The derivative of xP\" - x is p\"xP\"-l - 1 = -1, since F is of characteristic p. Therefore, xP\" - x and its derivative are certainly rela- tively prime, which, by the lemma, implies that xP\" - x has no multiple roots. Corollary 1 does not rule out the possibility that in characteristic p =I= 0 an irreducible polynomial might have multiple roots. To clinch matters, we exhibit an example where this actually happens. Let F0 be a field of characteristic 2 and let F = F 0 (x) be the field of rational functions in x over F0 • We claim that the polynomial t 2 - x in F[t] is irreducible over F and that its roots are equal. To prove irreducibility we must show that there is no rational function in F 0 (x) whose square is x; this is the content of Problem 4. To see that t 2 - x has a multiple root, notice that its deriv- ative (the derivative is with respect to t; for x, being in F, is considered as a constant) is 2t = 0. Of course, the analogous example works for any prime characteristic. Sec. 5.5 More About Roots 235 Now that the possibility has been seen to be an actuality, it points out sharp difference between the case of characteristic 0 and that of charac- ,,,., ... n·!~r·tc p. The presence of irreducible polynomials with multiple roots in latter case leads to many interesting, but at the same time complicating, These require a more elaborate and sophisticated treatment we prefer to avoid at this stage of the game. Therifore, we make the assumption for the rest of this chapter that all fields occurring in the text material are fields of characteristic 0. The extension K of F is a simple extension of F if K = F (a) ·· for some a in K. In characteristic 0 (or in properly conditioned extensions in characteristic p =1= 0; see Problem 14) all finite extensions are realizable as simple ex- tensions. This result is THEOREM 5.5.1 IfF is of characteristic 0 and if a, b, are algebraic over F, •• ·! ·then there exists an element c E F (a, b) such that F (a, b) = F (c). Proof. Letf(x) and g(x), of degrees m and n, be the irreducible poly- .. \\ nomials over F satisfied by a and b, respectively. Let K be an extension :w pf Fin which bothf (x) and g(x) split completely. Since the characteristic \\'ofF is 0, all the roots off (x) are distinct, as are all those of g(x). Let the ' roots off (x) be a = a1, a2 , • •• , am and those of g(x), b = b1, b2 , • •• , bn. If j =I= I, then bi =I= b1 = b, hence the equation a; + Abi = a 1 + Ab1 = + Ab has only one solution A in K, namely, a.- a A=-'-- b - bj F is of characteristic 0 it has an infinite number of elements, so we can find an element y E F such that ai + yb i =/= a + yb for all i and for j =/= 1. Let c = a + yb; our contention is that F (c) = F (a, b). Since . E F(a, b), w·e certainly do have that F(c) c F(a, b). We will now show t both a and bare in F(c) from which it will follow that F(a, b) c F(c). · Nclw b satisfies the polynomial g(x) over F, hence satisfies g(x) considered a polynomial over K = F(c). Moreover, if h(x) = f(c - yx) then E K[x] and h(b) = f (c - yb) = f (a) = 0, since a = c - yb. Thus in extension of K, h(x) and g(x) have x - b as a common factor. We that x - b is in fact their greatest common divisor. For, if b i =/= b another root of g(x), then h(bi) =f(c- ybi) =/= 0, since by our choice y, c- ybiforj =/=I avoidsallrootsaioff(x). Also, since (x- b) 2 .{' g(x), - b) 2 cannot divide the greatest common divisor of h(x) and g(x). Thus - b is the greatest common divisor of h(x) and g(x) over some extension 236 Fields Ch. 5 of K. But then they have a nontrivial greatest common divisor over K, which must be a divisor of x - b. Since the degree of x - b is 1, we see that the greatest common divisor of g(x) and h(x) in K[x] is exactly x- b. Thus x - bE K[x], whence bE K; remembering that K = F(c), we obtain that bE F(c). Since a = c - yb, and since b, c E F(c), y E F c F(c), we get that a E F(c), whence F(a, b) c F(c). The two opposite containing relations combine to yield F(a, b) = F(c). A simple induction argument extends the result from 2 elements to any finite number, that is, if a 1 , .•. , an are algebraic over F, then there is an element c E F(a 1, ... , an) such that F(c) = F(a 1, .•. , an)· Thus the COROLLARY Any finite extension of afield of characteristic 0 is a simple extension. Problems 1. IfF is of characteristic 0 and f(x) E F[x] is such that f'(x) = 0, provethatf(x) = aEF. 2. If F is of characteristic p # 0 and if f(x) E F[x] is such that f'(x) = 0, prove thatf(x) = g(xP) for some polynomialg(x) E F[x]. 3. Prove that (f(x) + g(x))' =f'(x) + g'(x) and that (af(x))' = af'(x) for f (x), g(x) E F[x] and a E F. 4. Prove that there is no rational function in F(x) such that its square is x. 5. Complete the induction needed to establish the corollary to Theorem 5.5.1. An element a in an extension K ofF is called separable over F if it satisfies a polynomial over F having no multiple roots. An extension K ofF is called separable over F if all its elements are separable over F. A field F is called perfect if all finite extensions ofF are separable. 6. Show that any field of characteristic 0 is perfect. 7. (a) IfF is of characteristic p # 0 show that for a, bE F, (a + b)Pm == aPm + bPm. (b) If F is of characteristic p # 0 and if K is an extension of F let T = {a E K I aP\" E Ffor some n}. Prove that Tis a subfield of K. 8. If K, T, Fare as in Problem 7 (b) show that any automorphism of K leaving every element ofF fixed also leaves every element of T fixed. *9. Show that a field F of characteristic p # 0 is perfect if and only if for every a E F we can find a b E F such that bP = a. 10. Using the result of Problem 9, prove that any finite field is perfect. Sec. 5.6 Elements of Galois Theory 237 If K is an extension ofF prove that the set of elements in K which are separable over F forms a subfield of K. 12. If F is of characteristic p =f. 0 and if K is a finite extension ofF, prove that given a E K either aP\" E F for some n or we can find an integer m such that aPm ¢ F and is separable over F. 13. If K and Fare as in Problem 12, and if no element which is in K but not in F is separable over F, prove that given a E K we can find an integer n, depending on a, such that aP\" E F. 14. If K is a finite, separable extension of F prove that K is a simple extension of F. 15. If one of a or b is separable over F, prove that F(a, b) is a simple extension of F. 5.6 The Elements of Galois Theory , Given a polynomial p(x) in F[x], the polynomial ring in x over F, we shall ;[associate with p(x) a group, called the Galois group of p(x). There is a very ,~~~dose relationship between the roots of a polynomial and its Galois group; i!i~~:ln fact, the Galois group will turn out to be a certain permutation group ·c the roots of the polynomial. We shall make a study of these ideas in this, in the next, section. The means of introducing this group will be through the splitting field p(x) over F, the Galois group of p(x) being defined as a certain group of tomorphisms of this splitting field. This accounts for our concern, in so of the theorems to come, with the automorphisms of a field. A ~?'t>eautiful duality, expressed in the fundamental theorem of the Galois the~ry ··. (Theorem 5.6.6), exists between the subgroups of the Galois group and the of the splitting field. From this we shall eventually derive a \"IJ4:ortC1Ition for the solvability by means of radicals of the roots of a polynomial terms of the algebraic structure of its Galois group. From this will follow the classical result of Abel that the general polynomial of degree 5 is not ble by radicals. Along the way we shall also derive, as side results, me:or·errts of great interest in their own right. One such will be the funda- theorem on symmetric functions. Our approach to the subject is ,--•££\"'''-''\"'\" on the treatment given it by Artin. Recall that we are assuming that all our fields are of characteristic 0, we can (and shall) make free use ofTheorem 5.5.1 and its corollary. By an automorphism qf the field K we shall mean, as usual, a mapping u K onto itself such that u(a + b) = u(a) + u(b) and u(ab) = u(a)u(b) all a, b E K. Two automorphisms a and -r of K are said to be distinct u(a) =f. -r(a) for some element a inK. We begin the material with 238 Fields Ch. 5 THEOREM 5.6.1 If K is afield and if a 1 , ... , an are distinct automorphisms of K, then it is impossible to find elements a 1, ..• , am not all 0, in K such that a1a 1 (u) + a2 a2 (u) + · · · + anan(u) = Ofor all u E K. Proof. Suppose we could find a set of elements a1, ••• , an in K, not all 0, such that a 1a 1 (u) + ··· + anan(u) = 0 for all uEK. Then we could find such a relation having as few nonzero terms as possible; on renumbering we can assume that this minimal relation is (1) where a1, •.. , am are all different from 0. If m were equal to 1 then a 1 a 1 ( u) = 0 for all u E K, leading to a1 = 0, contrary to assumption. Thus we may assume that m > 1. Since the auto- morphisms are distinct there is an element c E K such that a 1 (c) =f:. am( c). Since cu E K for all u E K, relation (1) must also hold for cu, that is, a 1a 1 (cu) + a2 a2 (cu) + · · · + amam(cu) = 0 for all u E K. Using the hypo- thesis that the a's are automorphisms of K, this relation becomes Multiplying relation (1) by u 1 (c) and subtracting the result from (2) yields If we put bi = ai(ai(c) - a 1 (c)) fori= 2, ... , m, then the bi are inK, bm = am(am(c) - a 1 (c)) =f:. 0, since am =f:. 0, and am(c) - a 1 (c) =f:. 0 yet b2a2 (u) + · · · + bmam(u) = 0 for all u E K. This produces a shorter rela- tion, contrary to the choice made; thus the theorem is proved. DEFINITION If G is a group of automorphisms of K, then the fixed field of G is the set of all elements a E K such that a( a) = a for all a E G. Note that this definition makes perfectly good sense even if G is not a group but is merely a set of automorphisms of K. However, the fixed field of a set of automorphisms and that of the group of automorphisms generated by this set (in the group of all automorphisms of K) are equal (Problem 1 ), hence we lose nothing by defining the concept just for groups of auto- morphisms. Besides, we shall only be interested in the fixed fields of groups of automorphisms. Having called the set, in the definition above, the fixed field of G, it would be nice if this terminology were accurate. That it is we. see in LEMMA 5.6.1 Thefixedfield ofG is a subfield of K. l Sec. 5.6 Elements of Galois Theory 239 Proof. Let a, b be in the fixed field of G. Thus for all (J E G, (J(a) = a (J(b) = b. But then (J(a ± b) = (J(a) ± (J(b) = a ± b and (J(ab) = u(b) = ab; hence a ± b and ab are again in the fixed field of G. If 0, then (J(b- 1 ) = (J(b) - 1 = b- 1, hence b- 1 also falls in the fixed of G. Thus we have verified that the fixed field of G is indeed a sub- We shall be concerned with the automorphisms of a field which behave a prescribed manner on a given subfield. Let K be a field and let F be a subfield of K. Then the of automorphisms of K relative to F, written G (K, F), is the set of all il'>~tut:on1or·pn1snns of K leaving every element of F fixed; that is, the auto- ,,,,.>,::o.\"\"n·rnt11c::1TI (J of K is in G (K, F) if and only if (J(e<) = oc for every oc E F. G (K, F) is a subgroup of the group of all automorphisms of K. We leave the proof of this lemma to the reader. One remark: K contains field of rational numbers F0 , since K is of characteristic 0, and it is easy see that the fixed field of any group of automorphisms of K, being a field, contain F0 • Hence, every rational number is left fixed by every · automorphism of K. We pause to examine a few examples of the concepts just introduced. Example 5.6.1 Let K be the field of complex numbers and let F be~the field of real numbers. We compute G(K, F). If (J is any automorphism of since i 2 = -1, (J(i) 2 = (J(i 2 ) = (J(-1) = -1, hence (J(i) = ±i. If, addition, (J leaves every real number fixed, then for any a + bi where b are real, (J(a + bi) = (J(a) + (J(b)(J(i) = a ± bi. Each of these possi- ties, namely the mapping (J 1 (a + bi) = a + bi and (J 2 (a + bi) = a - bi ;;~~aeltmc~s an automorphism of K, (]'1 being the identity automorphism and 2 complex-conjugation. Thus G (K, F) is a group of order 2. W,bat is the fixed field of G (K, F)? It certainly must contain F, but does contain more? If a + bi is in the fixed field of G (K, F) then a + bi = (a + bi) = a - hi, whence b = 0 and a = a + bi E F. In this case see that the fixed field of G (K, F) is precisely F itself. Example 5.6.2 Let F0 be the field of rational numbers and let K = (;.}2) where ;.}2 is the real cube root of 2. Every element in K is of the oc0 + oc1Z/2 + oc2 (Z/2) 2 , where oc0 , oc1 , oc2 are rational numbers. If 240 Fields Ch. 5 a is an automorphism of K, then a(~2) 3 = a( (~2) 3) = a(2) = 2, hence a(~2) must also be a cube root of 2 lying in K. However, there is only one real cube root of 2, and since K is a subfield of the real field, we must have that a(~2) = ~2. But then a(exo + r:t.1~2 + a2(~2) 2 ) = a0 + a1~2 + a2(~2)2 , that is, a is the identity automorphism of K. We thus see that G (K, F0 ) consists only of the identity map, and in this case the fixed field of G (K, F0 ) is not F0 but is, in fact, larger, being all of K. Example 5.6.3 Let F0 be the field of rational numbers and let w = e2 nif 5 ; thus w 5 = 1 and w satisfies the polynomial x4 + x 3 + x 2 + x + 1 over F0 • By the Eisenstein criterion one can show that x4 + x 3 + x 2 + x + 1 is irreducible over F0 (see Problem 3). Thus K = F0 (w) is of degree 4 over F0 and every element in K is of the form CXo + r:t.1 w + a2w 2 + a3w 3 where all of r:t.0, r:t.1 , r:t.2, and r:t.3 are in F0. Now, for any automorphism a of K, a(w) =1= 1, since a(1) = 1, and a(w) 5 = a(w 5) = a(1) = 1, whence a(ro) is also a 5th root of unity. In consequence, a(ro) can only be one of w, w 2 , ro3, or ro4 • We claim that each of these possibilities actually occurs, for let us define the four mappings a 1, a2 , a3, and a 4 by 2 3 . . 2 . 3 ai(r:t.o + a1w + a2w + r:t.3ro ) = CXo + r:t.1(w') + r:t.z(w') + a3(w') ' for i = 1, 2, 3, and 4. Each of these defines an automorphism of K (Problem 4). Therefore, since a E G (K, F0) is completely determined by a(w), G (K, F0 ) is a group of order 4, with a 1 as its unit element. In light of a 2 2 = a 4 , a 2 3 = a 3, a 2 4 = a 1, G(K, F0 ) is a cyclic group of order 4. One can easily prove that the fixed field of G (K, F0 ) is F0 itself (Problem 5). The subgroup A = {a1 , a 4 } of G(K, F0 ) has as its fixed field the set of all elements a0 + a 2 (w 2 + ro 3), which is an extension of F0 of degree 2. The examples, although illustrative, are still too special, for note that in each of them G (K, F) turned out to be a cyclic group. This is highly atypical for, in general, G (K, F) need not even be abelian (see Theorem 5.6.3). However, despite their speciality, they do bring certain important things to light. For one thing they show that we must study the effect of the automorphisms on the roots of polynomials and, for another, they point out that F need not be equal to all of the fixed field of G (K, F). The cases in which this does happen are highly desirable ones and are situations with which we shall soon spend much time and effort. We now compute an important bound on the size of G (K, F). THEOREM 5.6.2 If K is afinite extension ofF, then G(K, F) is afinite group and its order, o(G(K, F)) satisfies o(G(K, F)) ~ [K:F]. - Proof. Let [K:F] = n and suppose that u1, ••• , un is a basis of Kover F. Suppose we can find n + 1 distinct automorphisms u 1 , u 2 , ••• , an+t Sec. 5.6 Elements of Galois Theory 241 in G(K, F). By the corollary to Theorem 4.3.3 the system of n homogeneous linear equations in the n + 1 unknowns x 1, ... , xn+ 1 : 0\"1(u1)x1 + O\"z(u1)xz + · · · + O\"n+1(u1)xn+1 = 0 0\"1 (un)x1 + O\"z(un)Xz + · · · + 0\" n+1 (un)xn+1 = 0 bas a nontrivial solution (not all 0) x1 = a 1 , ... , xn+ 1 = an+ 1 inK. Thus (1) fori = 1, 2, ... , n. Since every element in F is left fixed by each ui and since an arbitrary element t in K is of the form t = a 1 u1 + · · · + rxnun with rxv ... , rxn in F, then from the system of equations (1) we get a1u1 (t) + · · · + an+ 1 u n + 1 ( t) = 0 for all t E K. But this contradicts the result of Theorem 5.6.1. Thus Theorem 5.6.2 has been proved. ,, Theorem 5.6.2 is of central importance in the Galois theory. However, '' aside from its key role there, it serves us well in proving a classic result ''.concerned with symmetric rational functions. This result on symmetric functions in its turn will play an important part in the Galois theory. , First a few remarks on the field of rational functions in n-variables over a , field F. Let us recall that in Section 3.11 we defined the ring of polynomials ~:in the n-variab1es x1, ... , xn over F and from this defined the field of !~~;'rational functions in x1, ... , Xm F(x1, ... , xn), over F as the ring of all ·~;f, quotients of such polynomials. . Let Sn be the symmetric group of degree n considered to be acting on tiie Set [1, 2, ... , n]; for 0\" E Sn and i an integer with 1 ::::;; i ::::;; n, let u(i) be image of i under u. We can make Sn act on F(xv ... , xn) in the following natural way: for (j E sn and r(x1, ... ' xn) E F(x1, ... ' xn), define the mapping which takes r(x1, ... , x~) onto r(xu( 1), ... , xu(n)). We shall .Write this mapping of F(xv ... , xn) onto itself also as u. It is obvious these mappings define automorphisms of F(xv ... , xn)· What is fixed field of F(x1, ••• , xn) with respect to Sn? It consists of all ·-~4'-\".0.0£1..0 functions r(x1, ... ' xn) such that r(x1, ... ' xn) = r(xu(1)' ... ' xu(n)) all (j E sn. But these are precisely those elements in F(x1, ... 'xn) are known as the symmetric rational functions. Being the fixed field Sn they form a subfield of F(x1, ... , xn), called the field of symmetric · functions which we shall denote by S. We shall be concerned • What is [F(x1, ... , xn) :SJ? What is G(F(x1, ... , xn), S)? Can we describe S in terms of some particularly easy extension of F? 242 Fields Ch. 5 We shall answer these three questions simultaneously. We can explicitly produce in S some particularly simple functions con- structed from x1, •.. , xn known as the elementary symmetric functions in x1, ... , xn. These are defined as follows: a3 L xixjxk i<j<k That these are symmetric functions is left as an exercise. For n 4 we write them out explicitly below. n = 2 n 3 n 4 al = xl + x2 + x3. a2 = xlx2 + xlx3 + x2x3. al = Xt + x2 + x3 + x4. a2 = x 1 x2 + x 1 x 3 + x 1 x 4 + x2x3 + x2x4 + x 3x 4 . a 3 = x1x2x3 + x1x2x4 + x 1x 3x 4 + x2x3x4 • 2, 3 and Note that when n = 2, x1 and x2 are the roots of the polynomial t 2 - a 1 t + a2, that when n = 3, x1, x2, and x3 are roots of t 3 - a 1 t2 + a2t - a3 and that when n = 4, x 1, x2 , x 3 , and x 4 are all roots of t 4 - a 1 t 3 + a2t2 - a3 t + a 4 • Since a 1, ... , an are all in S, the field F(a 1 , • •• , an) obtained by ad- joining a 1 , •.. , an to F must lie in S. Our objective is now twofold, namely, to prove 1. [F(x 1, ••• , xn) :S] n!. 2. S = F(a 1 , ••• , an)· Since the group sn IS a group of automorphisms of F(xl, ... ' Xn) leaving S fixed, Sn c G(F(x 1 , ••• , xn), S). Thus, by Theorem 5.6.2, Sec. 5.6 Elements of Galois Theory 243 [F(x1, ... , xn) :S] ~ o(G(F(x\"' ... , xn), S)) ~ o(Sn) = n!. If we could show that [F(x1, ... ,xn):F(a1, ... ,an)]::;n!, well then, since F(a1, ... ,an) is a subfield of S, we would have n! ~ [F(x1, ... , xn) :F(a1 , •.. , an)] = [F(x1 , •.. , xn) :S][S:F(a1, ... , an)] ~ n!. But then we would get that [F(x1, ... , xn) :S] = n !, [S:F(a 1 , .•. , an)] = 1 and so S =F(a 1, . .. , an), finally, Sn = G (F(x1, ... , xn), S) (this latter from the second sen- of this paragraph). These are precisely the conclusions we seek. Thus we merely must prove that [F(x1, ... ,xn):F(av···,an)J ::;n!. o see how this settles the whole affair, note that the polynomial p(t) = a1tn- 1 +a 2tn-l, · · + (-l)na\"' which has coefficients in F(a1, ... ,an), factors over F(x1, ... , xn) as p(t) = (t- x1 )(t- x2) · · • (t- xn)· (This is in fact the origin of the elementary symmetric functions.) Thus p(t), of degree n over F(a 1 , ••• , an), splits as a product of linear factors over F(x1, ... , xn)· It cannot split over a proper subfield of F(x1, ... , xn) which contains F ( av ... , an) for this subfield would then have to contain both F and each of the roots of p(t), namely, x1 , x2, ... , xn; but then this subfield would be all of F(xv ... , xn). Thus we see that F(x1, . .. , xn) is the splitting field of the polynomial p(t) = tn - a1tn-t + · · · + ( -l)\"an over F(a 1 , ••• , an)· Since p(t) is of degree n, by Theorem 5.3.2 we get ... , x n) :F ( av ... , an)] ::; n!. Thus all our claims are established. summarize the whole discussion in the basic and important result 5.6.3 Let F he afield and let F(x1, ... , xn) he thefield of rational Suppose that S is the field of symmetric rational [F(x1, ... , xn) :S] = n!. G (F(x1, ... , xn), S) = S\"' the symmetric group of degree n. If a1 , • •• , an are the elementary symmetric functions in x1 , ••• , Xm then S = F(a 1, a2, ... , an)· F(xu ... , xn) is the splitting field over F(av . .. , an) = S of the polynomial t\" - a1tn-1 + a2tn-2 ... + ( -l)nan. We mentioned earlier that given any integer nit is possible to construct field and a polynomial of degree n over this field whose splitting field is of · possible degree, n !, over this field. Theorem 5.6.3 explicitly us with such an example for if we put S = F(a 1, ... , an), the function field in n variables a1, ••• , an and consider the splitting of the polynomial tn- a1tn- 1 + a2tn-l ... + (-l)nan overS then is of degree n! over S. Part 3 of Theorem 5.6.3 is a very classical theorem. It asserts that a sym- . rational function in n variables is a rational function in the elementary symmetric of these variables. This result can even be sharpened to: A symmetric in n variables is a polynomial in their elementary symmetric 244 Fields Ch. 5 functions (see Problem 7). This result is known as the theorem on symmetric polynomials. In the examples we discussed of groups of automorphisms of fields and of fixed fields under such groups, we saw that it might very well happen that F is actually smaller than the whole fixed field of G (K, F). Certainly F is always contained in this field but need not fill it out. Thus to impose the condition on an extension K ofF that F be precisely the fixed field of G (K, F) is a genuine limitation on the type of extension ofF that we are considering. It is in this kind of extension that we shall be most interested. DEFINITION K is a normal extension ofF if K is a finite extension of F such that F is the fixed field of G(K, F). Another way of saying the same thing: If K is a normal extension ofF, then every element in K which is outside F is moved by some element in G (K, F). In the examples discussed, Examples 5.6.1 and 5.6.3 were normal extensions whereas Example 5.6.2 was not. An immediate consequence of the assumption of normality is that it allows us to calculate with great accuracy the size of the fixed field of any subgroup of G (K, F) and, in particular, to sharpen Theorem 5.6.2 from an inequality to an equality. THEOREM 5.6.4 Let K be a normal extension ofF and let H be a subgroup of G (K, F); let KH = {x E K I a(x) = xfor all a E H} be the fixed field of H. Then 1. [K:KH] = o(H). 2. H = G(K, KH)· (In particular, when H = G(K, F), [K:F] = o(G(K, F)).) Proof. Since very element in H leaves KH elementwise fixed, certainly H c G(K, KH). By Theorem 5.6.2 we know that [K:KH) ;;::: o(G(K, K8 )); and since o(G(K, KH)) ;;::: o(H) we have the inequalities [K:KH] :2:: o(G(K, KH)) ;;::: o(H). If we could show that [K:KH] = o(H), it would immediately follow that o(H) = o(G(K, KH)) and as a subgroup of G (K, KH) having order that of G (K, KH), we would obtain that H = G(K, KH). So we must merely show that [K:KH] = o(H) to prove every· thing. By Theorem 5.5.1 there exists an a E K such that K = KH(a); this a must therefore satisfy an irreducible polynomial over KH of degree m = [K:KH] and no nontrivial polynomial of lower degree (Theorem 5.1.3). Let the elements of H be a 1 , a 2, . .. , ah, where a 1 is the identity of G (K, F) Sec. 5.6 Elements of Galois Theory 245 and where h = o(H). Consider the elementary symmetric functions of a = u1 (a), u2 (a), ... : uh(a), namely, h cx1 = u1 (a) + u2 (a) + · · · + uh(a) = L ui(a) i = 1 cx2 = L ui(a)ui(a) i<j Each cxi is invariant under every u E H. (Prove!) Thus, by the definition of KH, cx1 , cx2 , • •• , cxh are all elements of KH. However, a (as well as u2 (a), . .. , uh(a)) is a root of the polynomial p(x) = (x- u1 (a)) (x- u2 (a))· · · (x - uh(a)) = xh - cx1J'- 1 + cx2xh- 2 + · · · + ( -1 )hcxh having coefficients in KH. By the nature of a, this forces h ~ m = [K:KII], whence o(H) ~ [K:KH]. Since we already know that o(H) ::;; [K:KH] we obtain o(H) = [K :KH], the desired conclusion. When H = G(K, F), by the normality of Kover F, KH = F; consequently for this particular case we read off the result [K:F] = o(G(K, F)). We are rapidly nearing the central theorem of the Galois theory. What we still lack is the relationship between splitting fields and normal extensions. This gap is filled by THEOREM 5.6.5 K is a normal extension ofF if and only if K is the splitting field of some polynomial over F. Proof. In one direction the proof will be highly reminiscent of that of Theorem 5.6.4. ~· Suppose that K is a normal extension ofF; by Theorem 5.5.1, K = F(a). Consider the polynomial p(x) = (x - u1 (a)) (x - u2 (a)) · · · (x - u n(a)) over K, where u1 , u2 , ••• , u 11 are all the elements of G(K, F). Expanding P(x) we see that p(x) = ~ - cx1x\"- 1 + cx2xn- 2 + · · · + ( -1 ) 110: 11 where cx1 , ••• , cx 11 are the elementary symmetric functions in a = u 1 (a), u2 (a), ... , U 11(a), But then cx1, ••• , CX 11 are each invariant with respect to every u E G (K, F), whence by the normality of K over F, must all be in F. Ther<)fore, K splits the polynomial p(x) E F[x] into a product of linear factors. Since a is a root of p(x) and since a generates Kover F, a can be in no proper subfield of K which contains F. Thus K is the splitting field of P(x) over F. Now for the other direction; it is a little more complicated. We separate off one piece of its proof in LEMMA 5.6.3 Let K be the splitting field of f(x) in F[x] and let p(x) be an ~6 Fields Ch. 5 irreducible factor off (x) in F[x]. lf the roots of p(x) are a 1, ... , a,, then for each i there exists an automorphism ui in G(K, F) such that ui(a 1 ) = ai. Proof. Since every root of p(x) is a root off (x), it must lie in K. Let a 1, ai be any two roots of p(x). By Theorem 5.3.3, there is an isomorphism -r of F 1 = F(a 1) onto F{ = F(ai) taking a1 onto ai and leaving every element ofF fixed. Now K is the splitting field off (x) considered as a polynomial over F1; likewise, K is the splitting field off (x) considered as a polynomial over F;. By Theorem 5.3.4 there is an isomorphism ui of K onto K (thus an automorphism of K) coinciding with -r on F 1. But then ui(a1) = -r(a1) = ai and ui leaves every element ofF fixed. This is, of course, exactly what Lemma 5.6.3 claims. We return to the completion of the proof of Theorem 5.6.5. Assume that K is the splitting field of the polynomial f (x) in F[x]. We want to show that K is normal over F. We proceed by induction on [K:F], assuming that for any pair of fields K 1, F 1 of degree less than [K :F] that whenever K 1 is the splitting field over F 1 of a polynomial in F 1 [x], then K 1 is normal over F 1 . Iff (x) E F[x] splits into linear factors over F, then K = F, which is certainly a normal extension of F. So, assume that f (x) has an irreducible factor p(x) E F[x] of degree r > 1. The r distinct roots a 1, a2 , ••• , a, of p(x) all lie in K and K is the splitting field off (x) considered as a poly- nomial over F(a 1). Since [K:F] [F(a 1) :F] n =- < n, r by our induction hypothesis K is a normal extension ofF ( a 1). Let e E K be left fixed by every automorphism (J E G (K, F); we would like to show that 8 is in F. Now, any automorphism in G ( K, F ( a 1)) certainly leaves F fixed, hence leaves 8 fixed; by the normality of K over F(a1), this implies that 8 is in F(a1). Thus e = Ao + .Alai + .A2al 2 + ... + .A,-lal'- 1 where Ao, ... ' .A,-1 E F. (1) By Lemma 5.6.3 there is an automorphism ui of K, ui E G(K, F), such that ui(a 1 ) = ai; since this ui leaves 8 and each .Ai fixed, applying it to ( 1) we obtain 8 = .A0 + .A1ai + .A2a/ + · · · + .A,_ 1a{- 1 for i = 1, 2, ... , r. (2) Thus the polynomial q(x) = .A,_ 1x'- 1 + .A,_ 2x'- 2 + · · · + .A1x + (.A0 :_ 8) in K[x], of degree at most r - I, has the r distinct roots a1 , a2 , ..• , a,. Sec. 5.6 Elements of Galois Theory 247 This can only happen if all its coefficients are 0; in particular, 20 - e = 0 whence 8 = 20 so is in F. This completes the induction and proves that K is a normal extension of F. Theorem 5.6.5 is now completely proved. DEFINITION Let f(x) be a polynomial in F[x] and let K be its splitting field over F. The Galois group off (x) is the group G (K, F) of all the auto- morphisms of K, leaving every element ofF fixed. Note that the Galois group off (x) can be considered as a group of permutations of its roots, for if a is a root off (x) and if a E G (K, F), then a( a) is also a root off (x). We now come to the result known as the fundamental theorem cif Galois theory. It sets up a one-to-one correspondence between the subfields of the splitting field off (x) and the subgroups of its Galois group. Moreover, it gives a criterion that a subfield .of a normal extension itself be a normal extension of F. This fundamental theorem will be used in the next section to derive conditions for the solvability by radicals of the roots of a poly- nomial. THEOREM 5.6.6 Let f(x) be a polynomial in F[x], Kits splitting field over F, and G (K, F) its Galois group. For any subfield T cif K which contains F let G(K, T) = {a E G(K, F) l a(t) = tfor every t E T} and for any subgroup H cif G (K, F) let KH = {x E K l a(x) = x for every a E H}. Then the asso- ciation cif T with G ( K, T) sets up a one-to-one correspondence cif the set cif sub fields of K which contain F onto the set cif subgroups cif G (K, F) such that I. T = KG(K,T)• 2. H = G(K, KH). 3. [K:T] = o(G(K, T)), [T:F] = index qfG(K, T) in G(K, F). 4. T is a normal extension cif F if and only if G (K, T) is a normal subgroup cif G(K, F). 5. When T is a normal extension ofF, then G ( T, F) is isomorphic to G(K, F)JG(K, T). Prpof. . Since K is the splitting fiel.d off (x)_ over Fit is also the splitting field of f(x) over any subfield T which contams F, therefore, by Theorem 5.6.5, K is a normal extension of T. Thus, by the definition of normality, Tis the fixed field of G (K, T), that is, T = KG(K,T)' proving part 1. Since K is a normal extension ofF, by Theorem 5.6.4, given a subgroup H of G(K, F), then H = G(K, KH), which is the assertion of part 2. More- over, this shows that any subgroup of G (K, F) arises in the form G (K, T), whence the association of T with G (K, T) maps the set of all subfields of K containing F onto the set of all subgroups of G(K, F). That it is one-to-one 248 Fields Ch. 5 is clear, for, if G (K, T 1) = G (K, T 2 ) then, by part 1, T 1 = KG(K,T 1 ) = KG(K,Tz) = T2. Since K is normal over T, again using Theorem 5.6.4, [K: T] = o(G(K, T)); but then we have o(G(K,F)) = [K:F] = [K:T][T:F] = o(G (K, T))[T:F], whence [T:F] = o(G(K, F)) =index ofG(K, T) o(G(K, T)) in G (K, F). This is part 3. The only parts which remain to be proved are those which pertain to normality. We first make the following observation. Tis a normal extension ofF if and only if for every a E G(K, F), a(T) c T. Why? We know by Theorem 5.5.1 that T = F(a); thus if a(T) c T, then a(a) E T for all a E G (K, F). But, as we saw in the proof of Theorem 5.6.5, this implies that Tis the splitting field of p(x) II (x - a(a)) aE G(K,F) which has coefficients in F. As a splitting field, T, by Theorem 5.6.5, is a normal extension of F. Conversely, if Tis a normal extension ofF, then T = F(a), where the minimal polynomial of a, p(x), over F has all its roots in T (Theorem 5.6.5). However, for any (J E G(K, F), (J(a) is also a root of p(x), whence (J(a) must be in T. Since Tis generated by a over F, we get that (J(T) c Tfor every (J E G(K, F). Thus T is a normal extension of F if and only if for any (J E G ( K, F), r:EG(K, T) and tE T, (J(t) E T and so r:((J(t)) = (J(t); that is, if and only if (J- 1r:(J(t) = t. But this says that Tis normal over F if and only if (J- 1G(K, T)(J c G(K, T) for every (J E G(K, F). This last condition being precisely that which defines G (K, T) as a normal subgroup of G (K, F), we see that part 4 is proved. Finally, if T is normal over F, given (J E G(K, F), since (J(T) c T, u induces an automorphism u* of T defined by u*(t) = u(t) for every t E T. Because (J* leaves every element ofF fixed, u* must be in G(T, F). Also, as is evident, for any u, 1/J E G(K, F), (u!/J)* = u*t/J* whence the mapping of G(K, F) into G(T, F) defined by u ~ u* is a homomorphism of G(K, F) into G(T, F). What is the kernel of this homomorphism? It consists of all elements u in G (K, F) such that u * is the identity map on T. That is, the kernel is the set of all u E G (K, F) such that t = (J * (t) = u(t); by the very definition, we get that the kernel is exactly G (K, T). The image of G(K, F) in G(T, F), by Theorem 2.7.1_ is isomorphic to G(K, F)JG(K, T), whose order is o(G(K, F))fo(G(K, T)) = [T:F] (by part 3) = o(G(T, F)) (by Theorem 5.6.4). Thus the image of G(K, F) in G ( T, F) is all of G ( T, F) and so we have G ( T, F) isomorphic to Sec. 5.6 Elements of Galois Theory 249 G (K, F) fG (K, T). This finishes the proof of part 5 and thereby completes the proof of Theorem 5.6.6. Problems 1. If K is a field and S a set of automorphisms of K, prove that the fixed field of Sand that of S (the subgroup of the group of all automorphisms of K generated by S) are identical. 2. Prove Lemma 5.6.2. 3. Using the Eisenstein criterion, prove that x 4 + x 3 + x 2 + x + 1 is irreducible over the field of rational numbers. 4. In Example 5.6.3, prove that each mapping ui defined is an auto- morphism ofF 0 ( w). 5. In Example 5.6.3, prove that the fixed field of F0 (w) under Uv u2 , u3 , u 4 is precisely F0 • 6. Prove directly that any automorphism of K must leave every rational number fixed. *7. Prove that a symmetric polynomial in x1, ... , xn is a polynomial in the elementary symmetric functions in x1, ... , xn. 8. Express the following as polynomials in the elementary symmetric functions in x1, x2, x3: (a) x/ + x22 + x/. (b) x13 + x23 + x33· (c) (x1 - x2)2(x1 - x3)2(x2- x3)2. 9. If a1 , a2 , cc3 are the roots of the cubic polynomial x 3 + 7x 2 8x + 3, find the cubic polynomial whose roots are (b) _!_' _!_' _!_, cc 1 cc2 cc3 * 10. Prove Newton's identities, namely, if cc1, cc2, ... , ccn are the roots of f (x) = xn + a1xn- 1 + a2~- 2 + · · · + an and if sk = cc/ + cc/ + · · · + cc/ then (a) sk + a1sk-t + a2sk_ 2 + · · · + ak_ 1s1 + kak = 0 if k = 1, 2, ... ,n. / (b) sk + a1sk-l + · · · + ansk-n = 0 fork > n. (c) For n = 5, apply part (a) to determine s2, s3, s4 , and s5. 11. Prove that the elementary symmetric functions in x1, ... , xn are indeed symmetric functions in XV ••• , X n• 12. If p(x) = xn - 1 prove that the Galois group of p(x) over the field of rational numbers is abelian. The complex number w is a primitive nth root rif unity if wn = 1 but wm =f:. for 0 < m < n. F 0 will denote the field of rational numbers. 250 Fields Ch. 5 13. (a) Prove that there are t/J(n) pnm1t1ve nth roots of unity where t/J(n) is the Euler t/J-function. (b) If w is a primitive nth root of unity prove that F0 ( w) is the splitting field of x\" - 1 over F0 (and so is a normal extension of F0 ). (c) If w1, ... , Wq,(n) are the t/J(n) primitive nth roots of unity, prove that any automorphism of F0 (w1) takes w1 into some wi. (d) Prove that [F0 (w1 ) :F0 ] ~ t/J(n). 14. The notation is as in Problem 13. *(a) Prove that there is an automorphism ui of F 0 (w1 ) which takes co1 into wi. (b) Prove the polynomial Pn(x) = (x - w1)(x - w2) · · · (x - Wq,(n)) has rational coefficients. (The polynomial Pn(x) is called the nth cyclotomic po(ynomial.) * (c) Prove that, in fact, the coefficients of Pn(x) are integers. **15. Use the results of Problems 13 and 14 to prove thatpn(x) is irreducible over F0 for all n ~ 1. (See Problem 8, Section 3.) 16. For n = 3, 4, 6, and 8, calculate Pn(x) explicitly, show that it has integer coefficients and prove directly that it is irreducible over F0 • 17. (a) Prove that the Galois group of x 3 - 2 over F 0 is isomorphic to s3, the symmetric group of degree 3. (b) Find the splitting field, K, of x 3 - 2 over F0 • (c) For every subgroup H of S3 find KH and check the correspondence given in Theorem 5.6.6. (d) Find a normal extension inK of degree 2 over F0 . 18. If the field F contains a primitive nth root of unity, prove that the Galois group of x\" - a, for a E F, is abelian. 5. 7 Solvability by Radicals Given the specific polynomial x 2 + 3x + 4 over the field of rational numbers F 0 , from the quadratic formula for its roots we know that its roots are (- 3 ± J- 7) /2; thus the field F 0 ( J7 i) is the splitting field of x 2 + 3x + 4 over F 0 • Consequently there is an element y = -7 in F'o such that the extension field F 0 (w) where w 2 = y is such that it contains all the roots of x 2 + 3x + 4. From a slightly different point of view, given the general quadratic poly- nomial p(x) = x 2 + a1x + a2 over F, we can consider it as a particular polynomial over the field F(a 1, a2) of rational functions .in the two variables a 1 and a2 over F; in the extension obtained by adjoining w to F(a 1 , a2) where w 2 = a 1 2 - 4a2 E F(a 1 , a2 ), we find all the roots of p(x). There is Sec. 5.7 Solvability by Radicals 251 a formula which expresses the roots of p(x) in terms of a1 , a2 and square roots of rational functions of these. For a cubic equation the situation is very similar; given the general cubic equation p(x) = x 3 + a1x 2 + a2 x + a 3 an explicit formula can be given, involving combinations of square roots and cube roots of rational functions in a1 , a2 , a3 • While somewhat messy, they are explicitly given by Cardan' s formulas: Let p = a2 - (a 1 2 /3) and 2al 3 ataz q = V- 3 + a3 and let and (with cube roots chosen properly); then the roots are P + Q- (a 1f3), roP + ro 2 Q- (a1/3), and w 2P + wQ- (a1/3), where w =/:. 1 is a cube root of 1. The above formulas only serve to illustrate for us that by adjoining a certain square root and then a cube root to F(a 1 , a2 , a 3 ) we reach a field in which p(x) has its roots. For fourth-degree polynomials, which we shall not give explicitly, by using rational operations and square roots, we can reduce the problem to that of solving a certain cubic, so here too a formula can be given expressing the roots in terms of combinations of radicals (surds) of rational functions of the coefficients. For polynomials of degree five and higher, no such universal radical formula can be given, for we shall prove that it is impossible to express their roots, in general, in this way. Given a field F and a polynomial p(x) E F[x], we say that p(x) is solvable by radicals over F if we can find a finite sentence of fields F1 = F(w 1 ), Fz = F1 (w 2 ), ••• ,Fk = Fk_ 1 (wk) such that w 1r 1 EF, w2 r 2 EF1 , ••• , OJkrk E Fk-l such that the roots of p(x) all lie in Fk. If/K is ,the splitting field of p(x) over F, then p(x) is solvable by radicals over F if we can find a sequence of fields as above such that K c Fk. An important remark, and one we shall use later, in the proof of Theorem 5.7.2, is that if such an Fk can be found, we can, without loss of generality, assume it to be a normal extension of F; we leave its proof as a problem (Problem 1). By the general polynomial of degree n over F, p(x) = xn + a1xn-l +···+am We mean the following: Let F(a 1 , .•• , an) be the field of rational functions, 252 Fields Ch. 5 in the n variables a1, .•. , an over F, and consider the particular polynomial p(x) = xn + a1xn- 1 + · · · + an over the field F(a 1 , ... , an)· We say that it is solvable by radicals if it is solvable by radicals over F ( a1, ••. , an). This really expresses the intuitive idea of \"finding a for- mula\" for the roots of p(x) involving combinations of mth roots, for various m's, of rational functions in a 1, a2 , ••• , an- For n = 2, 3, and 4, we pointed out that this can always be done. For n ~ 5, Abel proved that this cannot be done. However, this does not exclude the possibility that a given poly- nomial over F may be solvable by radicals. In fact, we shall give a criterion for this in terms of the Galois group of the polynomial. But first we must develop a few purely group-theoretical results. Some of these occurred as problems at the end of Chapter 2, but we nevertheless do them now officially. DEFINITION A group G is said to be solvable if we can find a finite chain of subgroups G = N0 :::) N 1 :::) N 2 :::) • • • :::) Nk = (e), where each Ni is a normal subgroup of Ni-l and such that every factor group Ni_ 1 fNi is abelian. Every abelian group is solvable, for merely take N0 = G and N1 = (e) to satisfy the above definition. The symmetric group of degree 3, S3 , is solvable for take N1 = {e, (1, 2, 3), (1, 3, 2)}; N1 is a normal subgroup of S3 and S3/N1 and N 1f(e) are both abelian being of orders 2 and 3, respec- tively. It can be shown that S4 is solvable (Problem 3). For n ~ 5 we show in Theorem 5.7.1 below that Sn is not solvable. We seek an alternative description for solvability. Given the group G and elements a, binG, then the commutator of a and b is the element a- 1b- 1ab. The commutator subgroup, G', of G is the subgroup of G generated by all the commutators in G. (It is not necessarily true that the set of commutators itself forms a subgroup of G.) It was an exercise before that G' is a normal subgroup of G. Moreover, the group GJG' is abelian, for, given any two elements in it, aG', bG', with a, bEG, then (aG')(bG') = abC' = ba(a- 1b- 1ab)G' = (since a- 1b- 1ab E G') baG' = (bG') (aG'). On the other hand, if M is a normal subgroup of G such that G f M is abelian, then M:::) G', for, given a, bEG, then (aM)(bM) = (bM)(aM), from which we deduce abM = baM whence a- 1b- 1abM = M and so a- 1b- 1ab EM. Since M contains all commutators, it contains the group these generate, namely G'. G' is a group in its own right, so we can speak of its commuta~or subgroup c<2 > = (G')'. This is the subgroup of G generated by all elements (a')- 1 (b')- 1a'b' where a', b' E G'. It is easy to prove that not only is G( 2 ) a normal subgroup of G' but it is also a normal subgroup of G (Problem 4). Sec. 5.7 Solvability by Radicals 253 We continue this way and define the higher commutator subgroups G(m) by c<m> = (G(m- 1>)'. Each c<m> is a normal subgroup of G (Problem 4) and G(m- 1> jc<m> is an abelian group. In terms of these higher commutator subgroups of G, we have a very succinct criterion for solvability, namely, LEMMA 5. 7.1 G is solvable if and only if G (k) = (e) for some integer k. Proof. If c<k> = (e) let N 0 = G, N1 = G', N 2 = c< 2 >, ... , Nk = G(k) = (e). We have G = N 0 ~ N1 ~ N2 ~ • • • ~ Nk = (e); each Ni being normal in G is certainly normal in Ni_ 1. Finally, Ni-1 c(i-1) c(i-1) N. = (i<i} = (G(i-1))' hence is abelian. Thus by the definition of solvability G is a sO'lvable group. Conversely, if G is a solvable group, there is a chain G = N 0 ~ N 1 ~ N2 ~ • • • ~ Nk = (e) where each Ni is normal in Ni_ 1 and where Ni_ 1fNi is abelian. But then the commutator subgroup Nf _1 of Ni _1 must be contained in Ni. Thus N1 ~ Nb = G', N2 ~ N~ ~ (G')' = G<2 >, N 3 ~ N~ ~ (G(2>)' = c< 3 >, ... , Ni ~ G(i), (e) = Nk ~ c<k>. We therefore obtain that G(k) = (e). COROLLARY If G is a solvable group and if G is a homomorphic image of G, then G is solvable. ..,. Proof. Since Cis a homomorphic image of G it is immediate that (G)(k) is the image of c<k>. Since c<k> = (e) for some k, (C)<k> = (e) for the same k, whence by the lemma G is solvable. The next lemma is the key step in proving that the infinite family of groups sm with n ~ 5, is not solvable; here sn is the symmetric group of degree n. LEI)AMA 5.7.2 Let G = Sm where n ~ 5; then c<k> for k = 1, 2, ... , contains every 3-cycle of sn. Proof. We first remark that for an arbitrary group G, if N is a normal subgroup of G, then N' must also be a normal subgroup of G (Problem 5). We claim that if N is a normal subgroup of G = Sm where n ~ 5, which contains every 3-cycle in Sn, then N' must also contain every 3-cycle. For suppose a = (1, 2, 3), b = (1, 4, 5) are in N (we are using here that n ~ 5); then a- 1b- 1ab = (3, 2, 1)(5, 4, 1)(1, 2, 3)(1, 4, 5) = (1, 4, 2), as a commutator of elements of N must be in N'. Since N' is a normal 254 Fields Ch. 5 subgroup of G, for any n E sn, n- 1(1, 4, 2)n must also be in N'. Choose a n in sn such that n(1) = il, n(4) = iz, and n(2) = i3, where il, iz, i3 are any three distinct integers in the range from 1 to n; then n- 1 (1, 4, 2)n = (i1 , i2 , i 3 ) is in N'. Thus N' contains all 3-cycles. Letting N = G, which is certainly normal in G and contains all 3-cycles, we get that G' contains all 3-cycles; since G' is normal in G, G<2> contains all 3-cycles; since G< 2 > is normal in G, G< 3 > contains all 3-cycles. Con- tinuing this way we obtain that G(k) contains all 3-cycles for arbitrary k. A direct consequence of this lemma is the interesting group-theoretic result. THEOREM 5.7.1 Sn is not solvable for n ~ 5. Proof. If G = S\"' by Lemma 5.7.2, c<k> contains all 3-cycles in Sn for every k. Therefore, G(k) =/:- (e) for any k, whence by Lemma 5.7.1, G cannot be solvable. We now interrelate the solvability by radicals of p(x) with the solvability, as a group, of the Galois group of p(x). The very terminology is highly suggestive that such a relation exists. But first we need a result about the Galois group of a certain type of polynomial. LEMMA 5.7.3 Suppose that the field F has all nth roots of unity (for some particular n) and suppose that a =/:- 0 is in F. Let x\" - a E F[x] and let K be its splitting field over F. Then 1. K = F(u) where u is any root of x\" - a. 2. The Galois group of x\" - a over F is abelian. Proof. Since F contains all nth roots of unity, it contains ~ = e 2ni/n; note that ~n = 1 but ~m =/:- 1 for 0 < m < n. If u E K is any root of x\" - a, then u, ~u, ~2u, ... , ~n-lu are all the roots of x\" - a. That they are roots is clear; that they are distinct follows from: ~iu = ~iu with 0 ~ i < j < n, then since u =/:- 0, and (~i - ~i)u = 0, we must have ~i = ~i, which is impossible since ~i-i = 1, with 0 <j- i < n. Since ~ E F, all of u, ~u, ... , ~n-lu are in F(u), thus F(u) splits x\" - a; since no proper subfield of F(u) which contains F also contains u, no proper subfield of F(u) can split x\" - a. Thus F(u) is the splitting field of x\" - a, and we have proved that K = F(u). If a, 't' are any two elements in the Galois group of x\" - a, that is, if a, 't' are automorphisms of K = F(u) leaving every element ofF fixed, then since both a(u) and 't'(u) are roots of x\" - a, a(u) = ~iu and 't'(u) = ~iu for some i and j. Thus a't'(u) = a(~iu) = ~ia(u) (since ~i E F) = ~i~iu == ~i+ iu; similarly, 't'a(u) = ~i+ iu. Therefore, a't' and 't'a agree on u and on Sec. 5.7 Solvability by Radicals 255 F hence on all of K = F(u). But thena-r = -ra, whence the Galois group is abelian. Note that the lemma says that when F has all nth roots of unity, then adjoining one root of xn - a to F, where a E F, gives us the whole splitting field of xn - a; thus this must be a normal extension of F. We assume for the rest of the section that F is a field which contains all nth roots of unity for every integer n. We have . THEOREM 5.7.2 If p(x) E F[x] is solvable by radicals over F, then the Galois group over F of p(x) is a solvable group. Proof. Let K be the splitting field of p(x) over F; the Galois group of p(x) over F is G(K, F). Since p(x) is solvable by radicals, there exists a ~equence of fields F c: F 1 = F(w 1 ) c: F2 = F1 (w2 ) c: · · · c: Fk = Fk- 1 (wk), where w{ 1 E F, w2r 2 E F1 , ••• , wkrk E Fk_ 1 and where K c: Fk. As we pointed out, without loss of generality we may assume that Fk is a normal extension of F. As a normal extension of F, Fk is also a normal extension of any intermediate field, hence Fk is a normal extension of each Fi. By Lemma 5. 7.3 each Fi is a normal extension of Fi_ 1 and since Fk is normal over Fi_ 1 , by Theorem 5.6.6, G(Fk, Fi) is a normal subgroup in G(Fk, Fi_ 1 ). Consider the chain As we just remarked, each subgroup in this chain is a normal subgroup in the one preceding it. Since Fi is a normal extension of Fi_ 1 , by the fundamental theorem of Galois theory (Theorem 5.6.6) the group of Fi over Fi_ 1 , G(Fi, Fi_ 1 ) is isomorphic to G(Fk, Fi_ 1 )/G(Fk, Fi). However, by Lemma 5. 7.3, G (Fi, Fi_ 1 ) is an abelian group. Thus e~ch quotient group G(Fk, Fi_ 1 )/G(Fk, Fi) of the chain (I) is abelian. Thus the group G (Fk, F) is solvable! Since K c: Fk and is a normal extension of F (being a splitting field), by Theorem 5.6.6, G(Fk, K) is ~ normal subgroup of G (Fk, F) and G (K, F) is isomorphic to G(Fk, F)fG(Fk, K). Thus G(K, F) is a homomorphic image of G(Fk, F), a ble group; by the corollary to Lemma 5.7.1, G(K, F) itself must then he a solvable group. Since G (K, F) is the Galois group of p(x) over F the theorem has been proved. We make two remarks without proof. The converse of Theorem 5. 7.2 is also true; that is, if the Galois group of p(x) over F is solvable then p(x) is solvable by radicals over F. .&.v-6 Fields Ch. 5 2. Theorem 5. 7.2 and its converse are true even if F does not contain roots of unity. Recalling what is meant by the general polynomial of degree n over F, p(x) = x\" + a 1x\"- 1 + · · · + am and what is meant by solvable by radicals, we close with the great, classic theorem of Abel: THEOREM 5.7.3 The general polynomial of degree n ~ 5 is not solvable by radicals. Proof. In Theorem 5.6.3 we saw that if F(a 1, ••• , an) is the field of rational functions in the n variables a1, •.. , am then the Galois group of the polynomial p(t) = tn + a1tn- 1 + · · · + an over F(a 1, ••• , an) was Sn, the symmetric group of degree n. By Theorem 5. 7.1, Sn is not a solvable group when n ~ 5, thus by Theorem 5. 7.2, p(t) is not solvable by radicals over F(a 1, ••• , an) when n ~ 5. Problems * 1. If p(x) is solvable by radicals over F, prove that we can find a sequence of fields F c F 1 = F(m 1) c F2 = F 1 (m2 ) c · · · c Fk = Fk_ 1 (mk), where m1' 1 E F, m2' 2 E F 1, •.. , m{k E Fk_ 1, Fk containing all the roots of p(x), such that Fk is normal over F. 2. Prove that a subgroup of a solvable group is solvable. 3. Prove that S4 is a solvable group. 4. If G is a group, prove that all G(k) are normal subgroups of G. 5. If N is a normal subgroup of G prove that N' must also be a normal subgroup of G. 6. Prove that the alternating group (the group of even permutations in Sn) An has no nontrivial normal subgroups for n ~ 5. 5.8 Galois Groups over the Rationals In Theorem 5.3.2 we saw that, given a field F and a polynomial p(x), of degree n, in F[x], then the splitting field of p(x) over F has degree at most n! over F. In the preceding section we saw that this upper limit of n! is, indeed, taken on for some choice ofF and some polynomial p(x) of degree n over F. In fact, if F0 is any field and ifF is the field of rational functions in the variables av ... , an over F0 , it was shown that the spli'tting field, K, of the polynomial p(x) = x\" + a1xn- 1 + · · · + an over F has degree exactly n! over F. Moreover, it was shown that the Galois group of Kover Sec. 5.8 Galois Groups over the Rationals 257 F is Sn, the symmetric group of degree n. This turned out to be the basis for the fact that the general polynomial of degree n, with n ~ 5, is not solvable by radicals. However, it would be nice to know that the phenomenon described above can take place with fields which are more familiar to us than the field of rational functions in n variables. What we shall do will show that for any prime number p, at least, we can find polynomials of degree p over the field of rational numbers whose splitting fields have degree p! over the rationals. This way we will have polynomials with rational coefficients whose Galois group over the rationals is SP. In light of Theorem 5. 7.2, we will conclude from this that the roots of these polynomials cannot be ex- pressed in combinations of radicals involving rational numbers. Although in proving Theorem 5. 7.2 we used that roots of unity were in the field, and roots of unity do not lie in the rationals, we make use of remark 2 following the proof of Theorem 5. 7.2 here, namely that Theorem 5. 7.2 remains valid 'even in the absence of roots of unity. We shall make use of the fact that polynomials with rational coefficients have all their roots in the complex field. We now prove THEOREM 5.8.1 Let q(x) be an irreducible polynomial of degree p, p a prime, over the field Q of rational numbers. Suppose that q(x) has exactly two nonreal roots in the field of complex numbers. Then the Galois group of q ( x) over Q is S P' the symmetric group of degree p. Thus the splitting field of q(x) over Q has degree p! over Q. Proof. Let K be the splitting field of the polynomial q(x) over (i. If a is a root of q(x) in K, then, since q(x) is irreducible over Q, by Theorem 5.1.3, [Q(a) :Q] = p. Since K ~ Q(cx) ~ Q and, according to Theorem 5.1.1, [K:Q] = [K:Q(cx)][Q(cx) :Q] = [K:Q(cx)]p, we have that PI [K:Q]. If G is the Galois group of K over Q, by Theorem 5.6.4, o(G) = [K:F]. Thus pI o(G). Hence, by Cauchy's theorem (Theorem 2.11.3), G has an element a of order p. To this point we have not used our hypothesis that q(x) has exactly two noreal roots. We use it now. If cx1, cx2 are these nonreal roots, then IX1 = ~2 , cx2 = ~ 1 (see Problem 13, Section 5.3), where the bar denotes ~ the complex conjugate. If cx3 , ••• , cxP are the other roots, then, since they are real, ~i = cxi for i ~ 3. Thus the complex conjugate mapping takes K into itself, is an automorphism 't of Kover Q, and interchanges cx1 and a2 , leaving the other roots of q(x) fixed. Now, the elements of G take roots of q(x) into roots of q(x), so induce permutations of cx1 , ••• , cxP. In this way we imbed G in SP. The auto- morphism -r described above is the transposition (1, 2) since -r(a1 ) = cx2, 258 Fields Ch. 5 -r(~X2 ) = ~X 1 , and -r(~Xi) = !Xi for i ~ 3. What about the element u E G, which we mentioned above, which has order p? As an element of SP, u has order p. But the only elements of order p in SP are p-cycles. Thus u must be a p-cycle. Therefore G, as a subgroup of SP, contains a transposition and a p-cycle. It is a relatively easy exercise (see Problem 4) to prove that any transposition and any p-cycle in SP generate SP. Thus u and -r generate SP. But since they are in G, the group generated by u and 't' must be in G. The net result of this is that G = SP. In other words, the Galois group of q(x) over Q is indeed SP. This proves the theorem. The theorem gives us a fairly general criterion to get SPas a Galois group over Q. Now we must produce polynomials of degree p over the rationals which are irreducible over Q and have exactly two nonreal roots. To pro- duce irreducible polynomials, we use the Eisenstein criterion (Theorem 3.1 0.2). To get all but two real roots one can play around with the co- efficients, but always staying in a context where the Eisenstein criterion is in force. We do it explicitly for p = 5. Let q(x) = 2x 5 - lOx + 5. By the Eisenstein criterion, q(x) is irreducible over Q. We graph y = q(x) = 2x 5 - lOx + 5. By elementary calculus it has a maximum at x = -1 and a minimum at x = 1 (see Figure 5.8.1). As the graph clearly indicates, y X (1, - 3) Figure 5.8.1 y = q(x) = 2x 5 - lOx + 5 crosses the x-axis exactly three times, so q(x) has exactly three roots which are real. Hence the other two roots must be complex, nonreal numbers. Therefore q(x) satisfies the hypothesis of Theorem 5.8.1, in consequence of which the Galois group of q(x) over Q is S5 • Using Theorem 5. 7.2, we know that it is not possible to express the roots of q(x) in a combination of radicals of rational numbers. 1 ~ I·. Sec. 5.8 Galois Groups over the Rationals 259 Problems 1. In S5 show that (1 2) and (1 2 3 4 5) generate S5 • 2. In S5 show that ( 1 2) and ( 1 3 2 4 5) generate S5 • 3. If p > 2 is a prime, show that (1 2) and (1 2 ... p - 1 p) generate sp. 4. Prove that any transposition and p-cycle in SP, p a prime, generate SP. 5. Show that the following polynomials over Q are irreducible and have exactly two nonreal roots. (a) p ( x) = x 3 - 3x - 3, (b) p( X) = X 5 - 6x + 3, (c) p(x) = x 5 + 5x 4 + 10x 3 + 10x 2 - x - 2. 6. What are the Galois groups over Q of the polynomials in Problem 5? 7. Construct a polynomial of degreee 7 with rational coefficients whose Galois group over Q is S7 • Supplementary Reading ARTIN, E., Galois Theory, 2nd ed. Notre Dame Mathematical Lectures Number 2. Notre Dame, Ind.: Notre Dame Press, 1966. KAPLANSKY, IRVING, Fields and Rings, 2nd ed. Chicago: University of Chicago Press, 1972. PoLLARD, H., Theory of Algebraic Numbers, Carus Monograph, Number 9. New York: John Wiley & Sons, 1950. VAN DER WAERDEN, B. L., Modern Algebra, Vol. I. New York: Ungar Publishing Company, 1949. WEISNER, L., Theory of Equations. New York: The Macmillan Company, 19381!'·. SmGAL, C. L., Transcendental Numbers, Annals of Mathematics Studies Number 16. Princeton, N.J.: Princeton University Press, 1949. Milwood, N.Y.: Kraus Reprint Company, 1949. NIVEN, 1., Irrational Numbers, Carus Monograph Number 11. New York: John Wiley & Sons, 1956. Topics for Class Discussion NIVEN_.yl., \"A simple proof of the irrationality of n,\" Bulletin of the American Math-ematical Society, Vol. 53 (1947), page 509. 6 Linear TransforDlations In Chapter 4 we defined, for any two vector spaces V and W over the same field F, the set Hom ( V, W) of all vector space homomorphisms of V into W. In fact, we introduced into Hom ( V, W) the operations of addition and of multiplication by scalars (elements of F) in such a way that Hom (V, W) itself became a vector space over F. Of much greater interest is the special case V = W, for here, in addition to the vector space operations, we can introduce a multi- plication for any two elements under which Hom ( V, V) becomes a ring. Blessed with this twin nature-that of a vector space and of a ring-Hom ( V, V) acquires an extremely rich structure. It is this structure and its consequences that impart so much life and sparkle to the subject and which justify most fully the creation of the abstract concept of a vector space. Our main concern shall be concentrated on Hom (V, V) where V will not be an arbitrary vector space but rather will be restricted to be a finite-dimensional vector space over a field F. The finite- dimensionality of V imposes on Hom (V, V) the consequence that each of its elements satisfies a polynomial over F. This fact, perhaps more than any other, gives us a ready entry into Hom (V, V) and allows us to probe both deeply and effectively into its structure. The subject matter to be considered often goes under the name of linear algebra. It encompasses the isomorphic theory of matrices. The statement that its results are in constant everyday use in every aspect ofmathematics (and elsewhere) is not in the least exaggerated. 260 Sec. 6.1 Algebra of Linear Transformations 261 A popular myth is that mathematicians revel in the inapplicability of their discipline and are disappointed when one of their results is \"soiled\" by use in the outside world. This is sheer nonsense! It is true that a mathe- tician does not depend for his value judgments on the applicability of a result outside of mathematics proper but relies, rather, on some · c, and at times intangible, mathematical criteria. However, it is true that the converse is false-the utility of a result has never its mathematical value. A perfect case in point is the subject of algebra; it is real mathematics, interesting and exciting on its own, it is probably that part of mathematics which finds the widest applica- , •• ,.,,,... __ 1,... physics, chemistry, economics, in fact in almost every science and The Algebra of Linear Transformations V be a vector space over a field F and let Hom ( V, V), as before, be ·the set of all vector-space-homomorphisms of V into itself. In Section 4.3 ·we showed that Hom (V, V) forms a vector space over F, where, for T 1, T 2 E Hom (V, V), T 1 + T2 is defined by v(T1 + T2) = vT1 + vT2 for all v E V and where, for ex E F, exT1 is defined by v(exT1 ) = ex(vT1 ). For T 1 , T 2 E Hom (V, V), since vT1 E V for any v E V, (vT1 ) T2 makes sense. As we have done for mappings of any set into itself, we define T1 T 2 by v( T 1 T 2) = (vT1 ) T2 for any v E V. We now claim that T1 T2 E (V, V). To prove this, we must show that for all ex, {1 E F and all u, v E V, (exu + {1v)(T1 T 2) = ex(u(T1 T 2 )) + {1(v(T1 T2 )). We compute ((exu + {1v) T 1 ) T 2 (ex(uT1 ) + {1(vT1 )) T2 = ex(uT1 ) T 2 + {1(vT1 ) T 2 = ex(u(T1 T2 )) + {J(v(T1 T 2 )). an exercise the following properties of this product in ( T 1 + T2) T 3 = T 1 T3 + T2 T3; T3(Tt + T2) = T3 Tt + T3 T2; Tt(T2 T3) = (Tt T2) T3; ex(T1 T2) = (exT1) T2 = T 1 (exT2); all T 1, T 2 , T 3 E Hom (V, V) and all ex E F. Note that properties I, 2, 3, above, are exactly what are required to of Hom ( V, V) an associative ring. Property 4 intertwines the .:uarac:ter of Hom (V, V), as a vector space over F, with its character as a 262 Linear Transformations Ch. 6 Note further that there is an element, I, in Hom (V, V), defined by vi = v for all v E V, with the property that TI = IT = T for every T E Hom (V, V). Thereby, Hom (V, V) is a ring with a unit element. More- over, if in property 4 above we put T 2 = I, we obtain rxT1 = T 1 (rx/). Since (rx/) T 1 = rx(IT1) = rxT1 , we see that (rxl) T1 = T 1 (rxl) for all T 1 E Hom (V, V), and so rxl commutes with every element of Hom (V, V). We shall always write, in the future, rxl merely as rx. DEFINITION An associative ring A is called an algebra over F if A is a vector space over F such that for all a, bE A and rx E F, rx(ab) = (rxa)b = a(rxb). Homomorphisms, isomorphisms, ideals, etc., of algebras are defined as for rings with the additional proviso that these must preserve, or be in- variant under, the vector space structure. Our remarks above indicate that Hom (V, V) is an algebra over F. For convenience of notation we henceforth shall write Hom (V, V) as A(V); whenever we want to emphasize the role of the field F we shall denote it by Ap(V). DEFINITION A linear transformation on V, over F, is an element of Ap(V). We shall, at times, refer to A ( V) as the ring, or algebra, of linear trans- formations on V. For arbitrary algebras A, with unit element, over a field F, we can prove the analog of Cayley's theorem for groups; namely, LEMMA 6.1.1 !fA is an algebra, with unit element, over F, then A is isomorphic to a subalgebra of A ( V) for some vector space V over F. Proof. Since A is an algebra over F, it must be a vector space over F. We shall use V = A to prove the theorem. If a E A, let Ta:A ~A be defined by vTa = va for every v EA. We assert that Ta is a linear transformation on V( =A). By the right-distribu- tive law (v1 + v2 ) Ta = (v1 + v2 )a = v1a + v2a = v1 Ta+ v2 Ta. Since A is an algebra, (rxv) Ta = (rxv)a = rx(va) = f'J.(vTa) for v E A, f'J. E F. Thus Ta is indeed a linear transformation on A. Consider the mapping 1/J :A ~ A(V) defined by at/J = Ta for every a EA. We claim that 1/J is an isomorphism of A into A(V). To begin with, if a, bE A and f'J., f3 E F, then for all v E A, vTaa+Pb = v(rxa + {Jb) ::::: rx(va) + f3(vb) [by the left-distributive law and the fact that_A is an algebra over F] = f'J.(vTa) + fJ(vTb) = v(f'J.Ta + f3Tb) since both Ta and Tb are linear transformations. In consequence, Taa+Pb = rxTa + f3Tb, whence t/1 is a vector-space homomorphism of A into A( V). Next, we compute, for Sec. 6.1 Algebra of Linear Transformations a, bE A, vTab = v(ab) = (va)b = (vTa) Tb = v(TaTb) (we have used the associative law of A in this computation), which implies that Tab = TaTb. In this way, t/1 is also a ring-homomorphism of A. So far we have proved that 1/J is a homomorphism of A, as an algebra, into A(V). All that remains is to determine the kernel of t/J. Let a E A be in the kernel of t/1; then at/J = 0, whence Ta = 0 and so vTa = 0 for all v E V. Now V =A, A has a unit element, e, hence eTa = 0. However, 0 = eTa = ea = a, proving that a = 0. The kernel of t/1 must therefore merely consist of 0, thus implying that t/1 is an isomorphism of A into A ( V). This completes the proof of the lemma. The lemma points out the universal role played by the particular algebras, A(V), for in these we can find isomorphic copies of any algebra. Let A be an algebra, with unit element e, over F, and let p(x) = cx0 + a 1x + · · · + cxnxn be a polynomial in F[x]. For a E A, by p(a), we shall mean the element cx0e + cx1a + · · · + cxnan in A. If p(a) = 0 we shall say a satisfies p ( x) . LEMMA 6.1.2 Let A be an algebra, with unit element, over F, and suppose that A is of dimension m over F. Then every element in A satisfies some nontrivial poly- nomial in F [ x] of degree at most m. Proof. Let e be the unit element of A; if a E A, consider the m + 1 elements e, a, a 2 , ••• , am in A. Since A ism-dimensional over F, by Lemma 4.2.4, e, a, a2 , ••• , am, being m + 1 in number, must be linearly dependent over F. In other words, there are elements cx0 , cx1, ••• , cxm in F, not all 0, such that cx0 e + cx1 a + · · · + cxmam = 0. But then a satisfies the non- trivial polynomial q(x) = cx0 + cx1x + · · · + cxm~' of degree at most m, in F[x]. If V is _a finite-dimensional vector space over F, of dimension n, by Corollary 1 to Theorem 4.3.1, A(V) is of dimension n 2 over F. Since A(V) is an algebra over F, we can apply Lemma 6.1.2 to it to obtain that every element in A( V) satisfies a polynomial over F of degree at most n 2 • This fact will be of central significance in all that follows, so we single it out as lf V fs an n-dimensional vector space over F, then, given any Tin A(V), there exists a nontrivial polynomial q(x) E F[x] of degree at n2 , such that q( T) = 0. We shall see later that we can assert much more about the degree of q(x); fact, we shall eventually be able to say that we can choose such a q(x) degree at most n. This fact is a famous theorem in the subject, and is as the Cayley-Hamilton theorem. For the moment we can get by 263 264 Linear Transformations Ch. 6 without any sharp estimate of the degree of q(x) ; all we need is that a suitable q(x) exists. Since for finite-dimensional V, given T E A(V), some polynomial q(x) exists for which q( T) = 0, a nontrivial polynomial of lowest degree with this property, p(x), exists in F[x]. We call p(x) a minimal polynomial for T over F. If T satisfies a polynomial h(x), then p(x) I h(x). DEFINITION An element T E A(V) is called right-invertible if there exists an S E A(V) such that TS = 1. (Here 1 denotes the unit element of A(V).) Similarly, we can define left-invertible, if there is a U E A(V) such that UT = 1. If T is both right- and left-invertible and if TS = UT = 1, it is an easy exercise that S = U and that S is unique. DEFINITION An element Tin A(V) is invertible or regular if it is both right- and left-invertible; that is, if there is an elementS E A(V) such that ST = TS = 1. We writeS as r- 1 . An element in A ( V) which is not regular is called singular. It is quite possible that an element in A(V) is right-invertible but is not invertible. An example of such: Let F be the field of real numbers and let V be F [ x], the set of all polynomials in x over F. In V let S be defined by d q(x)S = - q(x) dx and Thy q(x) T = r q(x) dx. Then ST =I= I, whereas TS = 1. As we shall see in a moment, if Vis finite-dimensional over F, then an element in A(V) which is right-invertible is invertible. THEOREM 6.1.2 If Vis finite-dimensional over F, then TeA(V) is in- vertible if and only if the constant term of the minimal polynomial for Tis not 0. Proof. Let p(x) = IXo + a 1 x + · · · + akx\\ ak =1= 0, be the minimal polynomial for T over F. If C<o =I= 0, since 0 = p( T) = C(k Tk + C(k-1 rk- 1 + ... + C(l T + CXo, we obtain Therefore, Sec. 6.1 Algebra of Linear Transformations 265 S = - _!._ ( IXk Tk- 1 + ' ' ' + IX1) IXo acts as an inverse for T, whence T is invertible. Suppose, on the other hand, that T is invertible, yet rx0 = 0. Thus 0 = rx1 T + rx2 T 2 + · · · + rxkTk = (rx1 + rx2 T + · · · + rxkTk- 1 )T. Multi- plying this relation from the right by r- 1 yields rx1 + rx2 T + · · · + a.kTk- 1 = 0, whereby T satisfies the polynomial q(x) = rx1 + rx2x + · · · + a.,/'- 1 in F[x]. Since the degree of q(x) is less than that of p(x), this is impossible. Consequently, ct0 =I= 0 and the other half of the theorem is established. COROLLARY 1 If Vis finite-dimensional over F and if TEA(V) is in- vertible, then r- 1 is a polynomial expression in T over F. Proof. Since T is invertible, by the theorem, IXo + rx1 T + · · · + a.1 Tk = 0 with rx0 =I= 0. But then r- 1 = - _!._ (rx1 + rx2 T + · · · + rxkrk- 1 ). IXo COROLLARY 2 If Vis finite-dimensional over F and if TEA( V) is singular, then there exists an S =/:; 0 in A(V) such that ST = TS = 0. Proof. Because T is not regular, the constant term of its minimal polynomial must be 0. That is, p(x) = rx1x + · · · + rxkx\\ whence 0 = cx1 T + · · · + rxkTk. If S = rx1 + · · · + rxkrk- 1 , then S =/:; 0 (since cx1 + · · · + rxkxk- 1 is of lower degree than p(x)) and ST = TS = 0. COROLLARY 3 If V is finite-dimensional over F and if T E A(V) is right- invertible, then it is invertible. Proof. Let TU = 1. If T were singular, there would be an S =/:; 0 such that ST = 0. However, 0 = (ST)U = S(TU) = Sl = S =I= 0, a contradiction. Thus Tis regular. We wish to transfer the information contained in Theorem 6.1.2 and its corollaries from A(V) to the~ction of Ton V. A most basic result in this vein is · THEOREM 6.1.3 If Vis finite-dimensional over F, then T E A(V) is singular if and only if there exists a v =I= 0 in V such that v T = 0. Proof. By Corollary 2 to Theorem 6.1.2, Tis singular if and only if there is an S =1= 0 in A(V) such that ST = TS = 0. Since S =I= 0 there is an element w E V such that wS =1= 0. 266 Linear Transformations Ch. 6 Let v = wS; then vT = (wS) T = w(ST) = wO = 0. We have produced a nonzero vector v in V which is annihilated by T. Conversely, if vT = 0 with v =F 0, we leave as an exercise the fact that Tis not invertible. We seek still another characterization of the singularity or regularity of a linear transformation in terms of its overall action on V. DEFINITION If T E A(V), then the range of T, VT, is defined by VT = {vT I v E V}. The range of Tis easily shown to be a subvector space of V. It merely consists of all the images by T of the elements of V. Note that the range ofT is all of V if and only if Tis onto. THEOREM 6.1.4 {f Vis finite-dimensional over F, then TE A(V) is regular if and only if T maps V onto V. Proof. As happens so often, one-half of this is almost trivial; namely, if T is regular then, given v E V, v = (vT- 1 ) T, whence VT = V and Tis onto. On the other hand, suppose that Tis not regular. We must show that Tis not onto. Since Tis singular, by Theorem 6.1.3, there exists a vector v1 =F 0 in V such that v1 T = 0. By Lemma 4.2.5 we can fill out, from v1, to a basis v1 , v2 , ••• , vn of V. Then every element in VT is a linear com- bination of the elements w1 = v1 T, w2 = v2 T, ... , wn = vnT. Since w1 = 0, VT is spanned by the n - 1 elements w2 , ••• , wn; therefore dim VT ~ n - 1 < n = dim V. But then VT must be different from V; that is, T is not onto. Theorem 6.1.4 points out that we can distinguish regular elements from singular ones, in the finite-dimensional case, according as their ranges are or are not all of V. If T E A ( V) this can be rephrased as: T is regular if and only if dim ( VT) = dim V. This suggests that we could use dim ( VT) not only as a test for regularity, but even as a measure of the degree of singularity (or, lack ofregularity) for a given TE A(V). DEFINITION If Vis finite-dimensional over F, then the rank of Tis the dimension of VT, the range of T, over F. We denote the rank of T by r ( T). At one end of the spectrum, if r ( T) == dim V, T is regular (and so, not at all singular). At th~ other end, if r(T) = 0, then T = 0 and so Tis as singular as it can possibly be. The rank, as a function on A(V), is an important function, and we now investigate some of its properties. Sec. 6.1 Algebra of Linear Transformations 267 LEMMA 6.1 .3 If V is finite-dimensional over F then for S, T E A ( V). I. r(ST) ~ r( T); 2. r(TS) ~ r(T); (and so, r(ST) ~ min {r( T), r(S) }) 3. r(ST) = r(TS) = r(T)forSregularinA(V). Proof. We go through 1, 2, and 3 in order. 1. Since VS c V, V(ST) = (VS) T c VT, whence, by Lemma 4.2.6, dim (V(ST)) ~ dim VT; that is, r(ST) ~ r(T). 2. Suppose that r( T) = m. Therefore, VT has a basis of m elements, w1, w2 , ••• , wm. But then (VT)S is spanned by w 1S, w2S, ... , wmS, hence has dimension at most m. Since r(TS) =dim (V(TS)) =dim ((VT)S) ~ m = dim VT = r(T), part 2 is proved. 3. If S is invertible then VS = V, whence V(ST) = (VS) T = VT. Thereby, r(ST) = dim (V(ST)) =dim (VT) = r(T). On the other hand, if VT has w 1, .•. , wm as a basis, the regularity of S implies that w1S, . :. , w,P are linearly independent. (Prove!) Since these span V(TS) they form a basis of V(TS). But then r(TS) = dim (V(TS)) = dim (VT) = r(T). COROLLARY ljTEA(V) andifSEA(V) is regular, thenr(T) = r(STS- 1 ). Proof. By part 3 of the lemma, r(STs- 1) = r(S( rs- 1)) = r(( rs- 1 )S) = r(T). Problems In all problems, unless stated otherwise, V will denote a finite-dimensional vector space over a field F. 1. Prove that S E A(V) is regular if and only if whenever v1 , ••• , vn E V are linearly independent, then v1 S, v2S, ... , vnS are also linearly independent. 2. Prove that TEA( V) is completely determined by its values on a basis of V. 3. Prove Lemma 6.1.1 even wherfA does not have a unit element. 4. If A. is the field of complex numbers and F is the field of real numbers, then A is an algebra over F of dimension 2. For a = ct + pi in A, compute the action of Ta (see Lemma 6.1.1) on a basis of A over F. 5. If Vis two-dimensional over F and A = A(V), write down a basis of A over F and compute Ta for each a in this basis. 6. If dimp V > 1 prove that A( V) is not commutative. 7. In A(V) let Z = {T E A(V) I ST = TS for all S E A(V) }. Prove that 268 Linear Transformations Ch. 6 Z merely consists of the multiples of the unit element of A(V) by the elements of F. *8. If dimF (V) > 1 prove that A(V) has no two-sided ideals other than (0) and A(V). **9. Prove that the conclusion of Problem 8 is false if V is not finite- dimensional over F. 10. If V is an arbitrary vector space over F and if T E A(V) is both right- and left-invertible, prove that the right inverse and left inverse must be equal. From this, prove that the inverse ofT is unique. 11. If V is an arbitrary vector space over F and if T E A(V) is right- invertible with a unique right inverse, prove that Tis invertible. 12. Prove that the regular elements in A(V) form a group. 13. IfF is the field of integers modulo 2 and if Vis two-dimensional over F, compute the group of regular elements in A(V) and prove that this group is isomorphic to s3, the symmetric group of degree 3. * 14. IfF is a finite field with q elements, compute the order of the group of regular elements in A( V) where Vis two-dimensional over F. * 15. Do Problem 14 if Vis assumed to be n-dimensional over F. *16. If Vis finite-dimensional, prove that every element in A(V) can be written as a sum of regular elements. 1 7. An element E E A ( V) is called an idempotent if E 2 = E. If E e A ( V) is an idempotent, prove that v = Vo ffi vl where VoE = 0 for all Vo E Vo and vlE = vl for all vl E vl. 18. If T E Ap(V), F of characteristic not 2, satisfies T 3 = T, prove that V = V0 ffi V1 ffi V2 where (a) v0 E V0 implies v0 T = 0. (b) vl E vl implies vl T = vl. (c) v2 E v2 implies v2 T = -v2. *19. If V is finite-dimensional and T ::f. 0 E A(V), prove that there is an S E A(V) such that E = TS ::f. 0 is an idempotent. 20. The element T E A(V) is called nilpotent if ym = 0 for some m. If Tis nilpotent and if vT = rxv for some v ::f. 0 in V, with rx E F, prove that rx = 0. 21. If T E A(V) is nilpotent, prove that rx0 + rx1 T + rx2 T 2 + · · · + rxk Tk is regular, provided that rx0 ::f. 0. 22. If A is a finite-dimensional algebra over F and if. a E A, prove that for some integer k > 0 and some polynomial p(x) E F[x], ak :::::: ak+ lp(a). 23. Using the result of Problem 22, prove that for a E A there is a poly- nomial q(x) E F[x] such that ak = a2kq(a). Sec. 6.1 Algebra of linear Transformations 269 24. Using the result of Problem 23, prove that given a E A either a is nilpotent or there is an element b =f. 0 in A of the form b = ah(a), where h(x) E F[x], such that b2 = b. 25. If A is an algebra over F (not necessarily finite-dimensional) and if for a E A, a2 - a is nilpotent, prove that either a is nilpotent or there is an element b of the form b = ah(a) =f. 0, where h(x) E F[x], such that b 2 = b. *26. If T =f. 0 E A( V) is singular, prove that there is an element SEA( V) such that TS = 0 but ST =f. 0. 27. Let V be two-dimensional over F with basis v1, v2 • Suppose that TEA( V) is such that v1 T = cw1 + {3v2 , v2 T = ')'7J1 + tJv2 , where ex, {3, y, bE F. Find a nonzero polynomial in F[x] of degree 2 satisfied by T. 28. If Vis three-dimensional over F with basis v1 , v2 , v3 and if T E A(V) is such that viT = exi1z'1 + exi2v2 + exi3v3 for i = I, 2, 3, with all exii E F, find a polynomial of degree 3 in F[x] satisfied by T. 29. Let V be n-dimensional over F with a basis v1 , ... , v,. Suppose that T E A(V) is such that v1 T = v2 , v2 T = v3 , ••• , v,_ 1 T = v,, v,T = -ex,v1 - ex,_ 1v2 - • • • - ex1v,, where ex1, ... , ex,. E F. Prove that T satisfies the polynomial p(x) = x\" + ex1x\"- 1 + ex2x\"- 2 + · · · + ex,. over F. 30. If T E A(V) satisfies a polynomial q(x) e F[x], prove that for S e A(V), S regular, srs- 1 also satisfies q(x). 31. (a) IfF is the field of rational numbers and if Vis three-dimensional over F with a basis v1 , v2 , v3 , compute the rank of TEA( V) defined by v1 T = v1 - v2 , v2 T = v1 + v3 , v3 T = v2 + v3 • (b) Find a vector v e V, v =f. 0. sqcil that vT = 0. 32. Prove that the range of T and U = { v E V I v T = 0} are subspaces of V. 33. If TeA(V), let V0 = {v E VI vTk = 0 for some k}. Prove that Vo is a subspace and that if vrm E Vo, then v E Vo. 34. Prove that the minimal polynomial ofT over F divides all polynomials satisfied by T over F. 35. If n( T) is the dimension of the U of Problem 32 prove that r( T) + n(T) =dim V. 270 Linear Transformations Ch. 6 6.2 Characteristic Roots For the rest of this chapter our interest will be limited to linear transfor- mations on finite-dimensional vector spaces. Thus, henceforth, V will always denote a finite-dimensional vector space over a field F. The algebra A(V) has a unit element; for ease ofnotation we shall write this as 1, and by the symbol A - T, for A E F, T E A ( V) we shall mean Al- T. DEFINITION If TE A(V) then A E F 1s called a characteristic root (or eigenvalue) of T if A - T is singular. We wish to characterize the property of being a characteristic root in the behavior of T on V. We do this in THEOREM 6.2.1 The element A E F is a characteristic root of T E A(V) if and only if for some v i= 0 in V, vT = AV. Proof. If A is a characteristic root ofT then A - Tis singular, whence, by Theorem 6.1.3, there is a vector v i= 0 in V such that v(A - T) = 0. But then AV = vT. On the other hand, if vT = AV for some v i= 0 in V, then v(A. - T) = 0, whence, again by Theorem 6.1.3, A - T must be singular, and so, A is a characteristic root of T. LEMMA 6.2.1 If A E F is a characteristic root of T E A(V), then for any polynomial q(x) E F[x], q(A) is a characteristic root of q(T). Proof. Suppose that A E F is a characteristic root of T. By Theorem 6.2.1, there is a nonzero vector v in V such that vT = AV. What about vT 2 ? Now vT 2 = (Av) T = A(vT) = A(Av) = A 2 v. Continuing in this way, we obtain that vTk = Akv for all positive integers k. If q(x) = cx0 xm + cx1xm- 1 + ... + CXm, (Xi E F, then q( T) = CXo ym + (X1 Tm- 1 + ... + CXm, whence vq(T) = v(exo Tm + cx1 Tm- 1 + · · · + cxm) = cx0 (vTm) + cx1 (vTm- 1) + · · · + cxmv = (cx0 Am + cx1Am- 1 + · · · + cxm)v = q(A)v by the remark made above. Thus v(q(A) - q(T)) = 0, hence, by Theorem 6.2.1, q(A) is a characteristic root of q( T). As immediate consequence of Lemma 6.2.1, in fact as a mere special case (but an extremely important one), we have THEOREM 6.2.2 If A E F is a characteristic root of T E A(V), then A is a root of the minimal polynomial of T. In particular, T only has a finite number of characteristic roots in F. Sec. 6.2 Characteristic Roots 271 Proof. Let p(x) be the minimal polynomial over F of T; thus p( T) = 0. If A E F is a characteristic root of T, there is a v ¥= 0 in V with v T = AV. As in the proof of Lemma 6.2.1, vp(T) = P(A)v; but p(T) = 0, which thus implies that P(A)v = 0. Since v ¥= 0, by the properties of a vector space, we must have that P(A) = 0. Therefore, A is a root of p(x). Since 'p(x) has only a finite number of roots (in fact, since deg p(x) ~ n 2 where n = dimp V, p(x) has at most n 2 roots) in F, there can only be a finite number of characteristic roots of Tin F. If T E A(V) and if s E A(V) is regular, then (Srs-t) 2 = srs-tsrs-t = ST 2S-I, (STs-t) 3 = ST 3S-I, ... , (STS-t)i = STis-t. Consequently, for any q(x) E F[ x], q(STs- t) = Sq( T)s- t. In particular, if q( T) = 0, then q(STS- t) = 0. Thus if p(x) is the minimal polynomial for T, then it follows easily that p(x) is also the minimal polynomial for srs- t. We have proved LEMMA 6.2.2 If T, S E A(V) and if Sis regular, then T and srs-t have The element 0 ¥= v E V is called a characteristic vector of T belonging to the characteristic root A E F if vT = AV. What relation, if any, must exist between characteristic vectors of T belonging to different characteristic roots? This is answered in THEOREM 6.2.3 If At, ... , Ak in F are distinct characteristic roots of T E A.(V) and if vt, ... , vk are characteristic vectors of T belonging to At, ... ~)k, . respectively, then vt, ... , vk are linearly independent over F. For the theorem to require any proof, k must be larger than 1 ; we suppose that k > 1. If vt, ... , vk arc linearly dependent over F, then there is a relation of the form OCt vt + · · · + r:xkvk = 0, where oct, ... , ock are all in F and not all of them are 0. In all such relations, there is one having as few nonzero co- . as possible. By suitably renumbering the vectors, we can assume this shortest relation to be / Pt ¥= o, ... , pi ¥= o. We know that viT = Aivi, so, applying T to equation (1), we obtain AtPt vt + · · · + AiPivi = 0. (1) (2) equation ( 1) by At and subtracting from equation (2), we 272 Linear Transformations Ch. 6 Now Ai - A1 =ft 0 for i > 1, and Pi =ft 0, whence (Ai - A1)Pi =ft 0. But then we have produced a shorter relation than that in (1) between v1 , v2, ... , vk. This contradiction proves the theorem. COROLLARY 1 lf T E A(V) and if dimp V = n then T can have at most n distinct characteristic roots in F. Proof. Any set of linearly independent vectors in V can have at most n elements. Since any set of distinct characteristic roots of T, by Theorem 6.2.3, gives rise to a corresponding set of linearly independent characteristic vectors, the corollary follows. COROLLARY 2 lf TEA( V) and if dimp V = n, and if T has n distinct characteristic roots in F, then there is a basis of V over F which consists of characteristic vectors ofT. We leave the proof of this corollary to the reader. Corollary 2 is but the first of a whole class of theorems to come which will specify for us that a given linear transformation has a certain desirable basis of the vector space on which its action is easily describable. Problems In all the problems V is a vector space over F. 1. If TeA(V) and if q(x) eF[x] is such that q(T) = 0, is it true that every root of q(x) in F is a characteristic root of T? Either prove that this is true or give an example to show that it is false. 2. If T E A(V) and if p(x) is the minimal polynomial for T over F, sup- pose that p(x) has all its roots in F. Prove that every root of p(x) is a characteristic root of T. 3. Let V be two-dimensional over the field F, of real numbers, with a basis v1 , v2 • Find the characteristic roots and corresponding charac- teristic vectors for T defined by (a) v1 T = v1 + v2 , v2 T = v1 - v2 • (b) v1 T = 5v1 + 6v2 , v2 T = -7v 2 • (c) v1 T = v1 + 2v2 , v2 T = 3v1 + 6v2 • 4. Let V be as in Problem 3, and suppose that T E A{V) is such that v1 T = av1 + Pv2 , v2 T = yv1 + bv2 , where a, p, y, b are in F. (a) Find necessary and sufficient conditions that 0 be a characteristic root of T in terms of a, p, y, b. Sec. 6.3 Matrices (b) In terms of a, /3, y, b find necessary and sufficient conditions that T have two distinct characteristic roots in F. 5. If V is two-dimensional over a field F prove that every element in A( V) satisfies a polynomial of degree 2 over F. •6. If V is two-dimensional over F and if S, T E A ( V), prove that (ST- TS) 2 commutes with all elements of A(V). 7. Prove Corollary 2 to Theorem 6.2.3. 8. If V is n-dimensional over F and TEA( V) is nilpotent (i.e., Tk = 0 for some k), prove that rn = 0. (Hint: If v E v use the fact that v, vT, v T 2 , • •• , v rn must be linearly dependent over F.) 6.3 Matrices ,Although we have been discussing linear transformations for some· time, it bas always been in a detached and impersonal way; to us a linear trans-, formation has been a symbol (very often T) which acts in a certain way on ,a vector space. When one gets right down to it, outside of the few concrete , examples encountered in the problems, we have really never come face to ~face with specific linear transformations. At the· same time it is clear that .·'if one were to pursue the subject further there would often arise the need .. of making a thorough and detailed study of a given linear transformation. mention one precise problem, presented with a linear transformation suppose, for the moment, that we have a means of recognizing it), does one go about, in a \"practical\" and computable way, finding)ts ~;c~narac:ten\"tstl .. c roots? What we seek first is a simple notation, or, perhaps more accurately, entation, for linear transformations. We shall accomplish this by of a particular basis of the vector space and by use of the action of a transformation on this basis. Once this much is achieved, by means the operations in A(V) we can induce operations for the symbols created, · of them an algebra. This new object, infused with an algebraic life its own, can be studied as a mathematical entity_)laving an interest by . This study is what comprises the subject of matrix theory. However, to ignore the source of these matrices, that is, to investigate the of symbols independently of what they represent, can be costly, for we be throwing away a great deal of useful information. Instead we always use the interplay between the abstract, A(V), and the concrete, matrix algebra, to obtain information one about the other. Let V be an n-dimensional vector space over a field F and let vv ... , vn a basis of V over F. If T E A ( V) then T is determined on any vector as as we know its action on a basis of V. Since T maps V into V, v1 T, 273 274 Linear Transformations Ch, 6 v2 T, ... , vn T must all be in V. As elements of V, each of these is realizable in a unique way as a linear combination of v1, ..• , vn over F. Thus v 1 T = oc11 v 1 + oc12 v 2 + · · · + oc1 nvn V2 T = OC21 V1 + OC22V2 + · · · + OC2nvn vi T = ocil v1 + oci2v2 + · · · + ocinvn vnT = ocn1v1 + ocn2v2 + ... + OCnnvn, where each ocii e F. This system of equations can be written more compactly as n viT = L ociivi' for i = 1, 2, ... , n. j=1 The ordered set of n 2 numbers ocii in F completely describes T. They will serve as the means of representing T. DEFINITION Let V be an n-dimensioned vector space over F and let v1, ••. , vn be a basis for V over F. If T E A(V) then the matrix ofT in the basis Vv • •• , vn, written as m( T), is m( T) = (r:: ~:: 0Cn1 0Cn2 where viT = Lj ociivi. A matrix then is an ordered, square array of elements ofF, with, as yet, no further properties, which represents the effect of a linear transformation on a given basis. Let us examine an example. Let F be a field and let V be the set of all polynomials in x of degree n - 1 or less over F. On V let D be defined by (Po+ P1x + · · · + Pn-1~- 1)D = P1 + 2P2x + · · · + ipixi- 1 + · · · + (n- 1)Pn-1~- 2 . It is trivial that Dis a linear transformation on V; in fact, it is merely the differentiation operator. What is the matrix of D? The questions is meaningless unless we specify a basis of V. Let us first compute the matrix of D in the basis v1 = 1, v2 = x, v3 = x 2, ... , vi= xi- 1, ... , vn = xn- 1 • Now, v1D = 1D = 0 = Ov1 + Ov2 + · · · + Ovn v2D = xD = 1 = 1v1 + Ov2 + · · · + Ovn ~iD = xi- 1D = (i- 1)x'f- 2 = Ov1 + Ov2 + · · · + Ovi_ 2 + (i - 1)vi_ 1 + Ovi + · · · + Ovn vnD = xn- 1D = (n- 1)~- 2 = Ov1 + Ov2 + · · · + Ovn_ 2 + (n - l)vn-l + Ovn. Sec. 6.3 Matrices 275 back to the very definition of the matrix of a linear transformation a given basis, we see the matrix of D in the basis v1, ... , v,., m1 (D), is fact ( 0 0 0 .. . 1 0 0 .. . m1 (D) = 0 2 0 .. . 0 0 3 .. . 0 0 0 .. . 0 0 0 0 (n - 1) !) However, there is nothing special about the basis we just used, or in how numbered its elements. Suppose we merely renumber the elements of basis; we then get an equally good basis w1 = x\"- I, w 2 = x\"- 2 , ••• , i = x\"- i, . .. , w,. = 1. What is the matrix of the same linear trans- tion Din this basis? Now, 1 w 1D = x\"- 1D = (n- 1)x\"- 2 · = Ow1 + (n - 1)w2 + Ow3 + · · · + Ow,. wiD= x\"-iD = (n- i)xn-i-t = Ow1 + · · · + Owi + (n - i)wi+l + Owi+ 2 + · · · + Ow,. w,.D = 1D = 0 = Ow1 + Ow2 + · · · + Ow,., m2 (D), the matrix of Din this basis is (n - 1) 0 0 0 0 0 (n - 2) 0 0 0 0 0 (n- 3) 0 0 m2 (D) 0 0 0 1 0 0 0 0 Before leaving this example, let us compute the matrix of D in still another · of Vover F. Let u1 =1, u2 =1+x, u3 =1+x 2 , .•• ,u,.=1+x\"- 1 ; is easy to verify that u1, •.• , u,. form a basis of V over F. What is the · of D in this basis? Since 1D = 0 = Ou1 + Ou2 + · · · + Ou,. (1 + x)D = 1 = 1u1 + Ou2 + · · · + Ou,. (1 + x 2 )D = 2x = 2(u2 - u1 ) = -2u 1 + 2u2 + Ou3 + · · · + Ou,. (1 + x\"- 1 )D = (n- I)x\"- 2 = (n- 1)(u,.- u1 ) = - (n - l)u 1 + Ou2 + · · · + Ou,._ 2 + (n - l)u,._ 1 + Ou,.. 276 Linear Transformations Ch. 6 The matrix, m3 (D), of Din this basis is 0 0 0 0 0 1 0 0 0 0 -2 2 0 0 0 -3 0 3 0 0 m3 (D) 0 0 0 0 - (n - 1) 0 0 (n - 1) 0 By the example worked out we see that the matrices of D, for the three bases used, depended completely on the basis. Although different from each other, they still represent the same linear transformation, D, and we could reconstruct D from any of them if we knew the basis used in their determi- nation. However, although different, we might expect that some relationship must hold between m1 (D), m2 (D), and m3 (D). This exact relationship will be determined later. Since the basis used at any time is completely at our disposal, given a linear transformation T (whose definition, after all, does not depend on any basis) it is natural for us to seek a basis in which the matrix of T has a particularly nice form. For instance, if Tis a linear transformation on V, which is n-dimensional over F, and if T has n distinct characteristic roots A.1, •.. , An in F, then by Corollary 2 to Thebrem 6.2.3 we can find a basis v1, ••• , vn of V over F such that vi T = A.ivi. In this basis T has as matrix the especially simple matrix, m(T) We have seen that once a basis of Vis picked, to every linear transforma- tion we can associate a matrix. Conversely, having picked a fixed basis v1, ... , vn of V over F, a given matrix ~ln) . ' 1Xnn gives rise to a linear transformation T defined on V by vi T = Lj ctiivi on this basis. Notice that the matrix of the linear transformation T, just con- structed, in the basis v1, ••• , vn is exactly the matrix with which we started. Thus every possible square array serves as the matrix of some linear trans- formation in the basis v1 , ... , vn. Sec. 6.3 Matrices 277 It is clear what is intended by the phrase the first row, second row, ... , of a matrix, and likewise by the first column, second column, . . . . In the matrix the element aii is in the ith row and jth column; we refer to it as the (i, j) entry of the matrix. To write out the whole square array of a matrix is somewhat awkward; instead we shall always write a matrix as (aii); this indicates that the (i, j) entry of the matrix is aii\" Suppose that Vis an n-dimensional vector space over F and v1, ••• , vn is a basis of V over F which will remain fixed in the following discussion. Suppose that Sand Tare linear transformations on V over F having matrices m(S) = (aii), m(T) = (r:i), respectively, in the given basis. Our objective is to transfer the algebraic structure of A ( V) to the set of matrices having en tries in F. To begin with, S = T if and only if vS = vT for any v E V, hence, if and only if viS = vi T for any v1 , ..• , vn forming a basis of V over F. Equivalently, S = T if and only if a ii = 1: ii for each i and j. Given that m(S) = (aii) and m(T) = (r:ii), can we explicitly write down m(S + T)? Because m(S) = (aii), viS= Lj aiivi; likewise, viT = Li r:iivi, whence vi(S + T) = viS + viT = L aiivi + L r:iivi = L (aii + r:ii)vi. j j j But then, by what is meant by the matrix of a linear transformation in a given basis, m(S + T) = (A.ii) where Aii = aii + r:ii for every i and j. A computation of the same kind shows that for y E F, m( yS)--= (Jlii) where llii = ya ii for every i and j. The most interesting, and complicated, computation is that of m(ST). Now v1(ST) = (v,S) T = ( 4= u,.v•) T = 4= u,.(v.T). However, vkT = Li r:kivi; substituting in the above formula yields (Prove!) Therefore, m(ST) = (vii), where for each i and J, vii = I:k a ikr:kj· :78 Linear Transformations Ch. 6 At first glance the rule for computing the matrix of the product of two linear transformations in a given basis seems complicated. However, note that the (i, j) entry of m(ST) is obtained as follows: Consider the rows of S as vectors and the columns ofT as vectors; then the (i, j) entry of m(ST) is merely the dot product of the ith row of S with the jth column of T. Let us illustrate this with an example. Suppose that m(S) = G !) and (-1 0) m(T) = 2 3 ; the dot product of the first row of S with the first column of Tis (1)( -1) + (2)(2) = 3, whence the (1, 1) entry ofm(ST) is 3; the dot product of the first row of S with the second column ofT is (1)(0) + (2)(3) = 6, whence the (1, 2) entry of m(ST) is 6; the dot product of the second row of S with the first column of T is (3) ( -1) + ( 4) (2) = 5, whence the (2, 1) entry of m(ST) is 5; and, finally the dot product of the second row of S with the second column of Tis (3)(0) + (4)(3) = 12, whence the (2, 2) entry of M(ST) is 12. Thus m(ST) = G ~~} The previous discussion has been intended to serve primarily as a motiva- tion for the constructions we are about to make. Let F be a field; an n X n matrix over F will be a square array of elements in F, (which we write as (IY.ij)). Let Fn = {(IY.ii) I r:J.ii E F}; in Fn we want to introduce the notion of equality of its elements, an addition, scalar multipli- cation by elements ofF and a multiplication so that it becomes an algebra over F. We use the properties of m(T) for T E A(V) as our guide in this. 1. We declare (!Y.ii) = ([:Jii), for two matrices in Fm if and only if IY.ij = fJ ii for each i and j. 2. We define (!Y.ii) + ([:Jij) = ()..ii) where ).ii = r:J.ii + pii for every i, j. 3. We define, for y E F, y(r:J.ii) = (Jlii) where Jlii = ')JIY.ii for every i and j. 4. We define (1Y.ii)(f3ii) = (vii), where, for every i and j~ vii = Lk r:J.ikpkj· Let V be an n-dimensional vector space over F and let v1 , .•. , vn be a basis of V over F; the matrix, m( T), in the basis v1 , .•• , vn associates with TEA( V) an element, m( T), in Fn. Without further ado we claim that the Sec. 6.3 Matrices 279 mapping from A(V) into Fn defined by mapping Tonto m( T) is an algebra isomorphism of A(V) onto Fn. Because of this isomorphism, Fn is an associative algebra over F (as can also be verified directly). We call Fn the algebra cif all n x n matrices over F. Every basis of V provides us with an algebra isomorphism of A(V) onto Fn. It is a theorem that every algebra isomorphism of A(V) onto Fn is so obtainable. In light of the very specific nature of the isomorphism between A ( V) and Fm we shall often identify a linear transformation with its matrix, in some basis, and A(V) with Fn. In fact, Fn can be considered as A(V) acting on the vector space V = p<n) of all n-tuples over F, where for the basis v1 = (1,0, ... ,0), v2 = (0, 1,0, ... ,0), ... , vn = (0,0, ... ,0, 1), (r:xi) EFn acts as vi(r:xii) = ith row of (r:xii). We summarize what has been done in THEOREM 6.3.1 The set cif all n x n matrices over F form an assoczatzve algebra, F\"' over F. lf V is an n-dimensional vector space over F, then A ( V) and Fn are isomorphic as algebras over F. Given any basis v1 , ••• , vn cif V over F, if for T E A ( V), m ( T) is the matrix cif T in the basis .a 1, . • . . v\"' the mapping T ~ m(T) provides an algebra isomorphism cif A(V) onto Fn. The zero under addition in Fn is the zero-matrix all of whose entries are 0; we shall often write it merely as 0. The unit matrix, which is the unit element of Fn under multiplication, is the matrix whose diagonal entries are 1 and whose entries elsewhere are 0; we shall write it as I, In (when we wish to emphasize the size of matrices), or merely as I. For r:x E F, the matrices al= C·.J (blank spaces indicate only 0 entries) are called scalar matrices. Because of the isomorphism between A(V) and Fn, it is clear that T E A(V) is invertible if and only if m( T), as a matrix, has an inverse in Fn. Given a linear transformation T E A(V), if we pick two bases, Vv ... , vn and w1,. \"'\", wn of V over F, each gives rise to a matrix, namely, m1 ( T) and tnz(T), the matrices of Tin the bases v1 , ••• , vn and w1 , ••• , wn, respec- tively. As matrices, that is, as elements of the matrix algebra Fn, what is the relationship between m1 ( T) and m2 ( T)? THEOREM 6.3.2 lf V is n-dimensional over F and if T E A(V) has the ma- . m1 (T) in the basis v1 , ••• , vn and the matrix m2 ( T) in the basis w1 , ••• , wn V over F, then there is an element C E Fn such that m2 (T) = Cm 1 (T)C- 1 • 280 Linear Transformations Ch. 6 In fact, if Sis the linear transformation of V defined by viS= wifor i = 1, 2, ... , n, then C can be chosen to be m1 (S). Proof. Let m1 (T) = (rx1i) and m2(T) = ({3ii); thus viT= Lir:t.iivi, wiT= Li Piiwi. Let S be the linear transformation on V defined by viS= wi. Since v1, ... , vn and w1, .•. , wn are bases of V over F, S maps V onto V, hence, by Theorem 6.1.4, Sis invertible in A(V). Now wiT = Li Piiwi; since wi = viS, on substituting this in the ex- pression for wiT we obtain (viS) T = Li Pii(viS). But then vi(ST) = (Li {3iivi)S; since S is invertible, this further simplifies to vi(STS- 1 ) = Lj Piivi. By the very definition of the matrix of a linear transformation in a given basis, m1 (STS- 1 ) = ({3ii) = m2 (T). However, the mapping T ~ m1 (T) is an isomorphism of A(V) onto Fn; therefore, m1 (STS- 1 ) = m1(S)m1 (T)m 1 (S- 1 ) = m1 (S)m 1 (T)m 1 (S)- 1 • Putting the pieces together, we obtain m2 (T) = m1 (S)m 1(T)m 1 (S)-1, which is exactly what is claimed in the theorem. We illustrate this last theorem with the example of the matrix of D, in various bases, worked out earlier. To minimize the computation, suppose that Vis the vector space of all polynomials over F of degree 3 or less, and let D be the differentiation operator defined by (IXo + rx1x + rx2x 2 + rx3x 3 )D = rx1 + 2rx2 x + 3a3x 2 • As we saw earlier, in the basis v1 = 1, v2 = x, v3 = x 2 , v4 = x 3 , the matrix of D is In the basis u1 = 1, u2 = 1 + x, u3 = 1 + x 2 , u4 = 1 + x 3 , the matrix of Dis (_! ~ ~ ~). -3 0 3 0 Let S be the linear transformation of V defined by v1S = w1 ( = v1), v2S = w 2 = 1 + x = v1 + v2 , v3S = w3 = 1 + x 2 = v1 + v3 , and also v4S = w4 = 1 + x 3 = v1 + v4 . The matrix of S in the basis v1 , v2 , v3 , V4 IS J r Sec. 6.3 Matrices 281 A simple computation shows that c-1 (-! ~ ~ ~) -1 0 0 . -1 0 0 1 Then (1 0 0 ~)(~ 0 0 ~) ( _: 0 0 ~) Cm1 (D)C- 1 1 0 0 0 1 0 0 1 2 0 0 -1 0 0 0 0 3 0 -1 0 0 (J 0 0 ~) 0 0 = m2 (D), 2 0 ' -3 0 3 as it should be, according to the theorem. (Verify all the computations used!) The theorem asserts that, knowing the matrix of a linear transformation in any one basis allows us to compute it in any other, as long as we know the linear transformation (or matrix) of the change of basis. We still have not answered the question: Given a linear transformation, how does one compute its characteristic roots? This will come later. From the matrix of a linear transformation we shall show how to construct a polynomial whose roots are precisely the characteristic roots of the linear transformation. Problems 1. Compute the following matrix products: (c) (t t ~ 3 :) 2 3 3 (d)(_: _:y 0 2 -1 ~). -1 2. Verify all the computations made in the example illustrating Theorem 6.3.2. 282 Linear Transformations Ch. 6 3. In Fn prove directly, using the definitions of sum and product, that (a) A(B + C) = AB + AC; (b) (AB)C = A(BC); for A, B, C E Fn. 4. In F2 prove that for any two elements A and B, (AB - BA) 2 is a scalar matrix. 5. Let V be the vector space of polynomials of degree 3 or less over F. In V define T by (oc0 + oc1x + oc2x 2 + oc3x3 ) T = oc0 + oc1 (x + l) + oc2 (x + l) 2 + oc3 (x + 1) 3 . Compute the matrix of Tin the basis (a) 1, x, x 2 , x 3 . (b) 1, 1 + x, 1 + x 2 , 1 + x 3 . (c) If the matrix in part (a) is A and that in part (b) is B, find a matrix C so that B = CAC- 1 • 6. Let V = F< 3> and suppose that ( -i ~ !) is the matrix of T E A(V) in the basis v1 = (1, 0, 0), v2 = (0, 1, 0), v3 = (0, 0, 1). Find the matrix of T in the basis (a) u1 = (1, 1, 1), u2 = (0, 1, 1), u3 = (0, 0, 1). (b) u1 = (1, 1, 0), u2 = (1, 2, 0), u3 = (1, 2, 1). 7. Prove that, given the matrix A=(~ 1 0) 0 1 E F3 -11 6 (where the characteristic ofF is not 2), then (a) A 3 - 6A 2 + llA- 6 = 0. (b) There exists a matrix C E F3 such that CAc-• = (~ ~ ~)· 8. Prove that it is impossible to find a matrix C E F2 such that 1) c-1 = (oc o) 1 0 f3 ' for any oc, f3 E F. 9. A matrix A E Fn is said to be a diagonal matrix if all the entries off the main diagonal of A are 0, i.e., if A = (ocii) and ocii = 0 for i #- j. If A is a diagonal matrix all of whose entries on the main diagonal Sec. 6.3 Matrices 283 are distinct, find all the matrices BE Fn which commute with A, that is, all matrices B such that BA = AB. 10. Using the result of Problem 9, prove that the only matrices in Fn which commute with all matrices in Fn are the scalar matrices. 11. Let A E Fn be the matrix 0 1 0 0 0 0 0 0 1 0 0 0 A 0 0 0 .. / 0 0 0 0 0 0 0 0 0 0 0 0 0 whose entries everywhere, except on the superdiagonal, are 0, and whose entries on the superdiagonal are 1 's. Prove An = 0 but An- 1 =j:. 0. *12. If A is as in Problem 11, find all matrices in Fn which commute with A and show that they must .be of the form a0 + a 1A + a2 A 2 + · · · + an_ 1An-l where a0 , a 1, ... , an-l E F. 13. Let A E F 2 and let C(A) = {BE F2 I AB = BA}. Let C(C(A)) = {G E F2 I GX = XG for all X E C(A) }. Prove that if G E C(C(A)) then G is of the form a0 + a 1 A, a0 , a 1 E F. 14. Do Problem 13 for A E F 3 , proving that every G E C(C(A)) is of theforma0 + a1A + a2A 2 • 15. In Fn let the matrices Eii be defined as follows: Eii is the matrix whose only nonzero entry is the (i, j) entry, which is 1. Prove (a) The Eij form a basis of Fn over F. (b) EiiEkz = 0 for j =/:- k; EiiEiz = Eu. (c) Given i, j, there exists a matrix C such that CEiiC- 1 = Ei'i• (d) If i =;6 j there exists a matrix C such that CEiiC- 1 = Ell. (e) Find all BE Fn commuting with Ell. (f) Find all BE Fn commuting with E 11 • 16. Let F be the field of real numbers and let C be the field of complex numbers. For a E C let T 0 :C--+ C by xTa = xa for all x E C. Using the basis 1, i find the matrix of the linear transformation Ta and so get an isomorphic representation of the complex numbers as 2 x 2 matrices over the real field. 17. Let Q be the division ring of quaternions over the real field. Using the basis 1, i, j, k of Q over F, proceed as in Problem 16 to find an isomorphic representation of Q by 4 x 4 matrices over the field of real numbers. *18. Combine the results of Problems 16 and 17 to find an isomorphic representation of Q as 2 x 2 matrices over the field of complex numbers. 284 Linear Transformations Ch. 6 19. Let .A be the set of all n x n matrices having entries 0 and 1 in such a way that there is one 1 in each row and column. (Such matrices are called permutation matrices.) (a) If ME .A, describe AM in terms of the rows and columns of A. (b) If ME .A, describe MA in terms of the rows and columns of A. 20. Let .A be as in Problem 19. Prove (a) .A has n! elements. (b) If ME .A, then it is invertible and its inverse is again in .R. (c) Give the explicit form of the inverse of M. (d) Prove that .A is a group under matrix multiplication. (e) Prove that .A is isomorphic, as a group, to Sn, the symmetric group of degree n. 21. Let A = (rJ.ii) be such that for each i, Lj rf.ii = 1. Prove that 1 is a characteristic root of A (that is, 1 - A is not invertible). 22. Let A = (rJ.ii) be such that for every j, Li rf.ii = 1. Prove that 1 is a characteristic root of A. 23. Find necessary and sufficient conditions on rJ., {3, y, b, so that A = G ~) is invertible. When it is invertible, write down A- 1 explicitly. 24. If E E Fn is such that E 2 = E =I= 0 prove that there 1s a matrix C E Fn such that 1 0 0 0 0 0 0 CEC- 1 0 0 0 0 0 0 0 0 0 0 0 0 where the unit matrix in the top left corner is r x r, where r is the rank of E. 25. If F is the real field, prove that it is impossible to find matrices A, B E Fn such that AB - BA = 1. 26. IfF is of characteristic 2, prove that in F2 it is possible to find matrices A, B such that AB - BA = 1. 27. The matrix A is called triangular if all the entries above the main diagonal are 0. (If all the entries below the main diagonal are 0 the matrix is also called triangular). (a) If A is triangular and no entry on the main diagonal is 0, prove that A is invertible. (b) If A is triangular and an entry on the main diagonal is 0, prove that A is singular. Sec. 6.4 Canonical Forms: Triangular Form 285 28. If A is triangular, prove that its cha7cteristic roots are precisely the elements on its main diagonal. 29. If Nk = 0, N E Fn, prove that 1 + N is invertible and find its inverse as a polynomial inN. 30. If A E Fn is triangular and all the entries on its main diagonal are 0, prove that An = 0. 31. If A E Fn is triangular and all the entries on its main diagonal are equal to a =1 0 E F, find A - 1. 32. Let S, T be linear transformations on V such that the matrix of S in one basis is equal to the matrix ofT in another. Prove there exists a linear transformation A on V such that T = ASA- 1 • 6.4 Canonical Forms: Triangular Form Let V be an n-dimensional vector space over a field F. DEFINITION The linear transformations S, TEA(V) are said to be similar if there exists an invertible element c E A ( V) such that T = esc- 1 . In view of the results of Section 6.3, this definition translates into one about matrices. In fact, since Fn acts as A(V) on p<n>, the above definition already defines similarity of matrices. By it, A, BE Fn are similar if there is an invertible C E Fn such that B = CAC- 1 • The relation on A ( V) defined by similarity is an equivalence relation; the equivalence class of an element will be called its similarity class. Given two linear transformations, how can we determine whether or not they are similar? Of course, we could scan the similarity class of one of these to~see if the other is in it, but this procedure is not a feasible one. Instead we try to establish some kind of landmark in each similarity class and a way of going from any element in the class to this landmark. We shall prove the existence of linear transformations in each similarity class whose matrix, in some basis, is of a particularly nice form. These matrices will be called the canonical forms. To determine if two linear transformations are similar, we need but compute a particular canonical form for each and check if these are the same. There ·are many possible canonical forms; we shall only consider three of these, namely, the triangular form, Jordan form, and the rational canonical form, in this and the next three sections. DEFINITION The subspace W of V 1s invariant under T E A(V) if .WTc W. LEMMA 6.4.1 If W c V is invariant under T, then T induces a linear transformation T on Vj W, defined by ( v + W) 'f' = v T + W. If T satisfies 286 Linear Transformations Ch. 6 the polynomial q(x) E F[x], then so does T. If Pi (x) is the minimal polynomial for T over F and if p(x) is that for T, then p1 (x) I p(x). Proof. Let V = Vf W; the elements of V are, of course, the cosets v + w of w in v. Given v = v + wE v define vf' = vT + w. To verify that T has all the formal properties of a linear transformation on V is an easy matter once it has been established that T is well defined on V. We thus content ourselves with proving this fact. Suppose that v = v1 + W = v2 + W where vi, v2 E V. We must show that vi T + W = v2 T + W. Since v1 + W = v2 + W, v1 - v2 must be in W, and since W is invariant under T, (v1 - v2 ) T must also be in W. Consequently v1 T - v2 T E W, from which it follows that v1 T + W = v2 T + W, as desired. We now know that T defines a linear transformation on V = VfW. If v = v + wE v, then v(T 2 ) = vT 2 + w = (vT) T + w = (vT + W)T = ((v + W)T)T = v(T) 2 ; thus (T 2 ) = (1\") 2 • Similarly, ( Tk) = ( f')k for any k ~ 0. Consequently, for any polynomial q(x) E F[x], q(T) = q(T). For any q(x) E F[x] with q(T) = 0, since U is the zero transformation on V, 0 = q( T) = q( T). Let p1 (x) be the minimal polynomial over F satisfied by 'f. If q( T) = 0 for q(x) E F[x], then Pi (x) I q(x). If p(x) is the minimal polynomial for T over F, then p( T) = 0, whence p( T) = 0; in consequence, p1 (x) I p(x). As we saw in Theorem 6.2.2, all the characteristic roots of T which lie in F are roots of the minimal polynomial of T over F. We say that all the characteristic roots of T are in F if all the roots of the minimal polynomial of T over F lie in F. In Problem 27 at the end of the last section, we defined a matrix as being triangular if all its entries above the main diagonal were 0. Equivalently, if T is a linear transformation on V over F, the matrix of T in the basis v 1' ... ' v n is triangular if v1 T = a11 v1 V2 T = tX21 V1 + IX22V2 viT = ailvl + 1Xi2V2 + · · · + aiivi, vn T = anl vl + ... + amnvn, i.e., if vi Tis a linear combination only of vi and its predecessors in the basis. THEOREM 6.4.1 If TeA(V) has all its characteristic'roots in F, then there is a basis of V in which the matrix ofT is triangular. Proof. The proof goes by induction on the dimension of V over F. If dimp V = I, then every element in A(V) is a scalar, and so the theorem is true here. Sec. 6.4 Canonical Forms: Triangular Form 287 Suppose that the theorem is true for all vector spaces over F of dimension n - 1, and let V be of dimension n over F. The linear transformation Ton V has all its characteristic roots in F; let i!.1 E F be a characteristic root of T. There exists a nonzero vector v1 in V such that v1 T = i!.1 v1 . Let W = { cw1 I ex E F}; W is a one-dimensional subspace of V, and is invariant under T. Let V = VfW; by Lemma 4.2.6, dim V =dim V- dim W = n- 1. By Lemma 6.4.1, T induces a linear transformation f' on V whose minimal polynomial over F divides the minimal polynomial of T over F. Thus all the roots of the minimal polynomial of f', being roots of the minimal polynomial of T, must lie in F. The linear transformation f' in its action on V satisfies the hypothesis of the theorem; since V is (n - I)-dimensional over F, by our induction hypothesis, there is a basis v2 , v3 , ..• , lin of V over F such that v2 f' = cx22v2 v3 f' = cx32v2 + cx33v3 Let v2, ... , vn be elements of V mapping into v2, ... , lim respectively. Then v1, v2 , ••• , vn form a basis of V (see Problem 3, end of this section). Since v2 f' = cx22v2, v2 f' - cx22v2 = 0, whence v2 T - cx22v2 must be in W. Thus v2 T - cx22v2 is a multiple of v1, say cx21 v1, yielding, after transposing, v2 T = cx21 v1 + cx22v2. Similarly, vi T - cxi2v2 - cxi3v3 - · • · - cxuvi E W, whence vi T = cxil v1 + cxi2v2 + · · · + cxiivi. The basis v1 , ... , vn of V over F provides us with a basis where every vi Tis a linear combination of vi and its predecessors in the basis. Therefore, the matrix of Tin this basis is triangular. This completes the induction and proves the theorem. We wish to restate Theorem 6.4.1 for matrices. Suppose that the matrix A E F n has all its characteristic roots in F. A defines a linear transforma- tion T on F(n) whose matrix in the basis v1 = (l,O, ... ,O),v2 = (0, l,O, ... ,O), ... ,vn = (0,0, ... ,0, 1), is precisely A. The characteristic roots of T, being equal to those of A, are all in F, whence by Theorem 6.4.1, there is a basis of F(n) in which the matrix of Tis triangular. However, by Theorem 6.3.2, this change of basis merely changes the matrix of T, namely A, in the first basis, into GAG- 1 for a suitable G c Fn. Thus ALTERNATIVE FORM OF THEOREM 6.4.1 If the matrix A E F\" has all its characteristic roots in F, then there is a matrix G E F n such that GAG - 1 is a triangular matrix. 288 Linear Transformations Ch. 6 Theorem 6.4.1 (in either form) is usually described by saying that T (or A) can be brought to triangular form over F. If we glance back at Problem 28 at the end of Section 6.3, we see that after T has been brought to triangular form, the elements on the main diagonal of its matrix play the following significant role: they are precisely the characteristic roots of T. We conclude the section with THEOREM 6.4.2 If V is n-dimensional over F and if T E A(V) has all its characteristic roots in F, then T satisfies a polynomial of degree n over F. Proof. By Theorem 6.4.1, we can find a basis vt, ... , vn of V over F such that: vt T = Atvt v2 T = oe21 vt + A2 v2 viT = oeilvt + · · · + oei,i-tvi-t + Aivi, for i = 1, 2, ... , n. Equivalently vt(T- At) = 0 v2(T- A2) = oe2tvt vi(T - At) = oeilvt + · · · + oei,i-tvi-t' for i = 1, 2, ... , n. What is v2 (T- A2 )(T- At)? As a result ofv 2 (T- A2 ) = oe21vt and vt (T - At) = 0, we obtain v2 ( T - A2) ( T - At) = 0. Since (T- A2)(T- At) = (T- At)(T- A2 ), vt(T- A2)(T- At) = vt(T- At)(T- A2) = 0. Continuing this type of computation yields v1(T- Ai)(T- Ai-t) · · · (T- At) = 0, v2 (T- )..i)(T- Ai-t) · · · (T- )..t) = 0, vi(T- )..i)(T- )..i-t)··· (T- At) = 0. For i = n, the matrix S = ( T - An)( T - An-t) · · · ( T - At) satisfies vtS = v2S = · · · = vnS = 0. Then, since S annihilates a basis of V, S must annihilate all of V. Therefore, S = 0. Consequently, T satisfies the poly- nomial (x - At)(x - )..2 ) • • • (x - An) in F[x] of 'degree n, proving the theorem. Unfortunately, it is in the nature of things that not every linear trans- formation on a vector space over every field F has all its characteristic roots Sec. 6.4 Canonical Forms: Triangular Form 289 in F. This depends totally on the field F. For instance, ifF is the field of real numbers, then the minimal equation of (-~ ~) over F is x 2 + I, which has no roots in F. Thus we have no right to assume that characteristic roots always lie in the field in question. However, we may ask, can we slightly enlarge F to a new field K so that everything works all right over K? The discussion will be made for matrices; it could be carried out equally well for linear transformations. What would be needed would be the follow- ing: given a vector space V over a field F of dimension n, and given an extension K ofF, then we can embed V into a vector space Vx over K of dimension n over K. One way of doing this would be to take a basis v1 , ••• , v,. of V over F and to consider Vx as the set of all a1 v1 + · · · + a,.v,. with the <Xj E K, considering the vi linearly independent over K. This heavy use of a basis is unaesthetic; the whole thing can be done in a basis-free way by introducing the concept of tensor product of vector spaces. We shall not do it here; instead we argue with matrices (which is effectively the route outlined above using a fixed basis of V). Consider the algebra F,.. If K is any extension field ofF, then F,. c K,. the set of n x n matrices over K. Thus any matrix over F can be considered as a matrix over K. If T E F,. has the minimal polynomial p(x) over F, considered as an element of K,. it might conceivably satisfy a different polynomial p0 (x) over K. But then p0 (x) I p(x), since p0 (x) divides all polynomials over K (and hence all polynomials over F) which are satisfied by T. We now specialize K. By Theorem 5.3.2 there is a finite extension, K, ofF in which the minimal polynomial, p(x), for T over F has all its roots. As an element of K ,., for this K, does T have all its characteristic roots in K? As an element of K,., the minimal polynomial for T over K, p0 (x) divides p(x) so all the roots of p0 (x) are roots of p(x) and therefore lie in K. Consequently, as an element in K,., T has all its characteristic roots in K. Thus, given Tin F,., by going to the splitting field, K, of its minimal polynomial we achieve the situation where the hypotheses ofTheorems 6.4.1 and 6.4.2 are satisfied, not over F, but over K. Therefore, for instance, T can be· brought to triangular form over K and satisfies a polynomial of degree n over K. Sometimes, when luck is with us, knowing that a certain result is true over K we can \"cut back\" to F and know that the result is still true over F. However, going to K is no panacea for there are frequent situations when the result for K implies nothing for F. This is why we have two types of \"canonical form\" theorems, those which assume that all the characteristic roots of T lie in F and those which do not. A final word; if T E F,., by the phrase \"a characteristic root of T\" we shall 290 Linear Transformations Ch. 6 mean an element A in the splitting field K of the minimal polynomial p(x) of T over F such that A - Tis not invertible in Kn. It is a fact (see Problem 5) that every root of the minimal polynomial of T over F is a characteristic root of T. Problems 1. Prove that the relation of similarity is an equivalence relation in A( V). 2. If T E Fn and if K :::> F, prove that as an element of Kn, T is in- vertible if and only if it is already invertible in Fn. 3. In the proof of Theorem 6.4.1 prove that vv ... , vn is a basis of V. 4. Give a proof, using matrix computations, that if A is a triangular n x n matrix with entries A1, .•• , An on the diagonal, then *5. If T E Fn has minimal polynomial p(x) over F, prove that every root of p(x), in its splitting field K, is a characteristic root of T. 6. If TEA( V) and if A E F is a characteristic root of T in F, let U;.. = {v E VI vT = Av}. If S E A(V) commutes with T, prove that U;.. is invariant under S. *7. If .A is a commutative set of elements in A(V) such that every ME At has all its characteristic roots in F, prove that there is a C E A(V) such that every CMC-1, forME .R, is in triangular form. 8. Let W be a subspace of V invariant under T E A ( V). By restricting T to W, T induces a linear transformation T (defined by w f = wT for every wE W). Let p(x) be the minimal polynomial of T over F. (a) Prove that p(x) I p(x), the minimal polynomial ofT over F. (b) If T induces f' on VJW satisfying the minimal polynomial p(x) over F, prove that p(x) I p(x)p(x). *(c) If p(x) and p(x) are relatively prime, prove that p(x) = p(x)p(x). *(d) Give an example of a Tfor whichp(x) =f. p(x)p(x). 9. Let .R be a nonempty set of elements in A(V); the subspace W c V is said to be invariant under .A if for every M E A, W M c W. If W is invariant under .A and is of dimension rover F, prove that there exists a basis of V over F such that every ME .A has a matrix, in this basis, of the form where M 1 is an r x r matrix and M 2 is an (n - r) x (n - r) matrix. Sec. 6.4 Canonical Forms: Triangular Form 291 10. In Problem 9 prove that M 1 is the matrix of the linear transformation M induced by M on W, and that M 2 is the matrix of the linear trans- formation Nf induced by M on Vj W. * 11. The nonempty set, .A, of linear transformations in A ( V) is called an irreducible set if the only subspaces of V invariant under .A are (0) and V. If .A is an irreducible set of linear transformations on V and if D = {TEA(V) I TM = MTforallME.A}, prove that D is a division ring. *12. Do Problem 11 by using the result (Schur's lemma) of Problem 14, end of Chapter 4, page 206. * 13. If F is such that all elements in A ( V) have all their characteristic roots in F, prove that the D of Problem 11 consists only of scalars. 14. Let F be the field of real numbers and let ( O 1 ) E F 2 • -1 0 (a) Prove that the set .A consisting only of is an irreducible set. (b) Find the set D of all matrices commuting with (_~ ~) and prove that D is isomorphic to the field of complex numb.ers. 15. Let F be the field of real numbers. (a) Prove that the set 0 0) ( 0 0 0 I)l 0 0 0 0 I 0 0 I ' 0 -I 0 0 -I 0 -I 0 0 0 is an irreducible set. (b) Find all A E F 4 such that AM= MA for all ME .A. (c) Prove that the set of all A in part (b) is a division ring isomorphic to the division ring of quaternions over the real field. 16. A set of linear transformations, .A c A ( V), is called decomposable if there is a subspace W c V such that V = WEB W1 , W =1 (0), W =f V, and each of W and W 1 is invariant under .A. If .A is not decomposable, it is called indecomposable. l92 Linear Transformations Ch. 6 (a) If .A is a decomposable set of linear transformations on V, prove that there is a basis of V in which every ME .A has a matrix of the form where M 1 and M 2 are square matrices. (b) If V is an n-dimensional vector space over F and if T E A ( V) satisfies yn = 0 but yn-l =j:. 0, prove that the set {T} (con- sisting of T) is indecomposable. 17. Let T E A(V) and suppose that p(x) is the minimal polynomial for T over F. (a) If p(x) is divisible by two distinct irreducible polynomials p1 (x) and p2 (x) in F[x], prove that {T} is decomposable. (b) If {T}, for some T E A(V) is indecomposable, prove that the minimal polynomial for T over F is the power of an irreducible polynomial. 18. If T E A(V) is nilpotent, prove that T can be brought to triangular form over F, and in that form all the elements on the diagonal are 0. 19. If T E A(V) has only 0 as a characteristic root, prove that Tis nil- potent. 6.5 Canonical Forms: Nilpotent Transformations One class of linear transformations which have all their characteristic roots in F is the class of nilpotent ones, for their characteristic roots are all 0, hence are in F. Therefore by the result of the previous section a nilpotent linear transformation can always be brought to triangular form over F. For some purposes this is not sharp enough, and as we shall soon see, a great deal more can be said. Although the class of nilpotent linear transformations is a rather re- stricted one, it nevertheless merits study for its own sake. More important for our purposes, once we have found a good canonical form for these we can readily find a good canonical form for all linear transformations which have all their characteristic roots in F. A word about the line of attack that we shall follow is in order. We could study these matters from a \"ground-up\" approach or we could invoke results about the decomposition of modules which we obtained in Chapter 4. We have decided on a compromise between the twq; we treat the material in this section and the next (on Jordan forms) independently of the notion of a module and the results about modules developed in Chapter 4. How- ever, in the section dealing with the rational canonical form we shall com- pletely change point of view, introducing via a given linear transformation a module structure on the vector spaces under discussion; making use of I Sec. 6.5 Canonical Forms: Nilpotent Transformations 293 Theorem 4.5.1 we shall then get a decomposition of a vector space, and the resulting canonical form, relative to a given linear transformation. Even though we do not use a module theoretic approach now, the reader should note the similarity between the arguments used in proving Theorem 4.5.1 and those used to prove Lemma 6.5.4. Before concentrating our efforts on nilpotent linear transformations we prove a result of interest which holds for arbitrary ones. LEMMA 6.5.1 If V = V1 EB V2 EB · · · EB Vk, where each subspace Vi is of dimension ni and is invariant under T, an element of A(V), then a basis of V can be found so that the matrix of T in this basis is of the form (f 0 where each Ai is an ni x ni matrix and is the matrix of the linear transformation induced by Ton Vi. Choose a basis of Vas follows: v1 (1>, ... , vn 1 < 1> is a basis of Vv v2<2>, ... , vn2<2> is a basis of V2, and so on. Since each Vi is invariant under T, v}i)TE vi so is a linear combination of v1<i), v2(i>, ... ' vn/i), and of only these. Thus the matrix of T in the basis so chosen is of the desired form. That each Ai is the matrix of Ti, the linear transformation induced on Vi by T, is clear from the very definition of the matrix of a linear transformation. We now narrow our attention to nilpotent linear transformations. LEMMA 6.5.2 If T E A(V) is nilpotent, then OCo + oc1 T + ... + ocmrm, where the oci E F, is invertible if oc0 =I= 0. Proof. If Sis nilpotent and oc0 =I= 0 E F, a simple computation shows that (oc0 +S) ---+-+·\"\"+(-1Y- 1 - =1, ( 1 s S2 sr-1) OCo OCo 2 OCo 3 OCo r Now if yr = 0, S = oc1 T + oc2 T 2 + · · · + ocmTm also must ~-~).Baltisfv sr ~ 0. (Prove!) Thus for OCo =I= 0 in F, OCo + s is invertible. Mt will denote the t x t matrix ( f 0 0 ~ ~), 0 0 0 I 0 0 0 0 of whose entries are 0 except on the superdiagonai, where they are all I 's. 14 Linear Transformations Ch. 6 DEFINITION If TEA(V) is nilpotent, then k is called the index ofni[, potence of T if Tk = 0 but yk- 1 =1= 0. The key result about nilpotent linear transformations is THEOREM 6.5.1 If T E A(V) is nilpotent, of index of nilpotence n1, then a basis of V can be found such that the matrix of Tin this basis has the form 0 ~ ) ' Mnr where n 1 ~ n2 ~ · · · ~ n, and where n 1 + n2 + · · · + n, = dimp V. Proof. The proof will be a little detailed, so as we proceed we shall separate parts of it out as lemmas. Since T\" 1 = 0 but T\" 1 - 1 =1= 0, we can find a vector v E V such that vT\" 1 - 1 =/= 0. We claim that the vectors v, v T, ... , vT\" 1 - 1 are linearly independent over F. For, suppose that cx1v + cx2vT + · · · + ctn 1vT\" 1 - 1 = 0 where the cti E F; let CX 8 be the first nonzero ex, hence Since ct8 =I= 0, by Lemma 6.5.2, CX 8 + ct 8 +1 T + · · · + ctn1 T\" 1 -s is invertible, and therefore vrs- 1 = 0. However, s < n1, thus this contradicts that vT\" 1 - 1 =/= 0. Thus no such nonzero ct 8 exists and v, vT, ... , vT» 1 - 1 have been shown to be linearly independent over F. Let V1 be the subspace of V spanned by v1 = v, v2 = vT, ... , vn 1 = vTn 1 - 1 ; V1 is invariant under T, and, in the basis above, the linear trans- formation induced by Ton V1 has as matrix Mn 1 • So far we have produced the upper left-hand corner of the matrix of the theorem. We must somehow produce the rest of this matrix. LEMMA 6.5.3 If u E v1 is such that uTnt-k = 0, where 0 < k ::::;; n1, then u = u0 Tk for some u0 E V1. Proof. Since u E v1, u = ct1V + cx2vT + ... + cxkvrk- 1 + ak+1vTk + ·· · + ctn 1vT\" 1 - 1 . Thus 0 = uT\" 1 -k = cx1vTn 1 -k + · · · + cxkvTn 1 - 1• However, vT\" 1 -k, ... , vTn 1 - 1 are linearly inc;lependent over F, whencke cx1 = ct2 = · · · = ctk = 0, and so, u = ctk+ 1vTk + · · · + ctn1vTn 1 - 1 = u0 T' where Uo = ctk+lv + ... + ctntvTnt-k- 1 E v1. The argument, so far, has been fairly straightforward. Now it becomes a little sticky. \\ Sec. 6.5 Canonical Forms: Nilpotent Transformations 295 LEMMA 6.5.4 There exists a subspace W of V, invariant under T, such that v = v1 EB w. Proof. Let W be a subspace of V, of largest possible dimension, such that I. vl n w = (0); 2. W is invariant under T. We want to show that V = V1 + W. Suppose not; then there exists an element z E v such that z ¢ vl + w. Since ynt = 0, there exists an in- teger k, 0 < k ~ n1, such that zTk E vl + wand such that zTi ¢ vl + w for i < k. Thus zTk = u + w, where u E vl and where wE w. But then 0 = zTn 1 = (zTk) ynt -k = uTn 1 -k + wTn 1 -k; however, since both V1 and ware invariant under T, uTnt-k E vl and wTnt-k E w. Now, since v1 n w = (0), this leads to uTnt-k = -wTnt-k E vl n w = (0), resulting in uTn 1 -k = 0. By Lemma 6.5.3, u = u0 Tk for some u0 E V1; therefore, zTk = u + w = u0 Tk + w. Let z 1 = z- u0 ; then z 1Tk = zTk- u0 Tk = w E W, and since W is invariant under T this yields z 1 ym E W for all m ~ k. On the other hand, if i < k, Zt Ti = zTi - UoTi ¢ vl + w, for otherwise zTi must fall in V1 + W, contradicting the choice of k. Let w1 be the subspace of v spanned by w and Zt, Zt T, ... ' Zt yk- 1. Since z1 ¢ w, and since wl :::> W, the dimension of w1 must be larger than that of W. Moreover, since z 1 Tk E W and since W is invariant under T, W1 must be invariant under T. By the maximal nature of W there must be an element of the form w0 + a1z 1 + a2 z 1 T + · · · + akz1 yk- 1 =I= 0 in w1 n Vv where WoE w. Not all of al, ... ' ak can be 0; otherwise we would have 0 =I= Wo E w n v1 = (0)' a contradiction. Let as be the first nonzero a; then w0 +z 1Ts- 1 (as+as+ 1T+ .. ·+akrk-s)EV1. Since .... «s =f:. 0, by Lemma 6.5.2, as + as+l T + · · · + akrk-s is invertible and its inverse, R, is a polynomial in T. Thus Wand V1 are invariant under R; however, from the above, WoR+ztrs-lEV1Rc v1, forcing Ztys- 1 E V1 + WR c V1 + W. Since s - I < k this is impossible; therefore V1 + W = V. Because V1 n W = (0), V = V1 EB W, and the lemma is proved. The hard work, for the moment, is over; we now complete the proof of Theorem 6.5.1. By Lemma ·6.5.4, V = V1 EB W where W is invariant under T. Using .the basis v1 , ••• , vn 1 of V1 and any basis of Was a basis of V, by Lemma .5.1, the matrix of T in this basis has the form (~\"' ~J here A 2 is the matrix of T 2 , the linear transformation induced on W by T. ince Tn 1 = 0, T 2n 2 = 0 for some n2 ~ n1. Repeating the argument used 296 Linear Transformations Ch. 6 for T on V for T 2 on W we can decompose W as we did V (or, invoke an induction on the dimension of the vector space involved). Continuing this way, we get a basis of V in which the matrix ofT is of the form ~J. That n1 + n2 + · · · + nr = dim V is clear, since the size of the matrix is n x n where n = dim V. DEFINITION The integers n1, n2 , ••• , nr are called the invariants of T. DEFINITION If T E A(V) is nilpotent, the subspace M of V, of dimen- sion m, which is invariant under T, is called cyclic with respect to T if 1. MTm = (0), Mrm- 1 =1- (0); 2. there is an element z EM such that z, zT, ... 'zrm- 1 form a basis of M. (Note: Condition 1 is actually implied by Condition 2). LEMMA 6.5.5 lj M, of dimension m, is cyclic with respect to T, then the dimension of MTk ism - kfor all k ~ m. Proof. A basis of MTk is provided us by taking the image of any basis of M under Tk. Using the basis z, zT, ... , zrm- 1 of M leads to a basis zT\\ zTk+ 1, ... , zrm-t of MTk. Since this basis has m - k elements, the lemma is proved. Theorem 6.5.1 tells us that given a nilpotent Tin A(V) we can find integers nl ;;:::: nz ;;:::: ... ;;:::: nr and subspaces, v1, ... ' vr of v cyclic with respect to T and of dimensions n1 , n2 , ••• , nn respectively such that V = V1 EB · · · EB Vr. Is it possible that we can find other integers m1 ;;:::: m2 ;;:::: • • • ;;:::: ms and subspaces U1 , ••• , Us of V, cyclic with respect to T and of dimensions m1, ••. , ms, respectively, such that V = U1 EB · · · EB Us? We claim that we cannot, or in other words that s = r and m1 = n1 , m2 = n2 , ••• , mr == nr. Suppose that this were not the case; then there is a first integer i such that mi =1- ni. We may assume that mi < ni. Consider vrm~. On one _hand, since V = V1 EB · · · EB Vr, vrmt == V1 ymt EB · · · EB virm~ EB · · · EB VrTmt. Since dim V1 ymt = n1 - mi, dim V2 Tm 1 = n2 - mi, ... , dim ViTm 1 = ni - mi (by Lemma 6.5.5), dim VTm 1 ;;:::: (n1 - mi) + (n 2 - mi) + · · · + (ni - mi). On the other hand, since V = U1 EB · · • EB Us and since uiymt = (0) for j ;;:::: i, VTm 1 == U 1 Tmt EB U2 Tmt + · · · EB Ui_ 1 Tm 1• Thus dim vrmi = (ml - mi) + (mz - mi) + ... + (mi-l - mi)• Sec. 6.5 Canonical Forms: Nilpotent Transformations 297 dim vrm; = (n1 - mi) + (nz - mi) + ... + (ni-1 - mi)• However, this contradicts the fact proved above that dim VTm 1 ~ (n1 - mi) + · · · + (ni_ 1 - mi) + (ni - mi), since ni - mi > 0. Thus there is a unique set of integers n1 ~ n2 ~ • • • ~ nr such that V is the direct sum of subspaces, cyclic with respect to T of dimensions n1, n2 , ••• , nr. Equivalently, we have shown that the invariants ofT are unique. Matricially, the argument just carried out has proved that if n1 ~ n2 ~ · · · ~ nr and m1 ~ m2 ~ • • • ~ m8 , then the matrices are similar only if r = sand n1 = m1 , n2 = m2 , ••• , nr = mr. So far we have proved the more difficult half of THEOREM 6.5.2 Two nilpotent linear transformations are similar if and only if they have the same invariants. Proof. The discussion preceding the theorem has proved that if the two nilpotent linear transformations have different invariants, then they can- not be similar, for their respective matrices LJ and (t' LJ cannot be similar. In the other direction, if the two nilpotent linear transformations Sand T have the same invariants n1 ~ • • • ~ nn by Theorem 6.5.1 there are bases v1, •• • , vn and w 1, •.. , wn of V such that the matrix of Sin v1 , ..• , vn and that ofT in wv ... , wn, are each equal to But if A is the linear transformation defined on V by viA = wi, then S = ATA- 1 (Prove! Compare with Problem 32 at the end of Section 6.3), Whence S and T are similar. Let us compute an example. Let ~98 Linear Transformations Ch. 6 act on p(3> with basis u1 = (1, 0, 0), u2 = (0, 1, 0), u3 = (0, 0, 1). Let v1 = u1 , v2 = u1 T = u2 + u3 , v3 = u3 ; in the basis v1 , v2 , v3 the matrix ofT is ( 0 1 0) ~~' 0 0 0 so that the invariants of Tare 2, 1. If A is the matrix of the change of basis, namely ( 1 0 0) 0 1 1 ' 0 0 1 a simple computation shows that ( 0 1 ATA- 1 = 0 0 0 0 One final remark: the invariants of T determine a partition of n, the dimension of V. Conversely, any partition of n, n1 ~ • • · ~ n,, n1 + n2 + · · · + n, = n, determines the invariants of the nilpotent linear transformation. Thus the number of distinct similarity classes of nilpotent n x n matrices is precisely p(n), the number of partitions of n. 6.6 Canonical Forms: A Decomposition of V: Jordan Form Let V be a finite-dimensional vector space over F and let T be an arbitrary element in AF( V). Suppose that vl is a subspace of v invariant under T. Therefore T induces a linear transformation T 1 on V1 defined by u T 1 = uT for every UE vl. Given any polynomial q(x) EF[x], we claim that the linear transformation induced by q(T) on V1 is precisely q(T 1 ). (The proof of this is left as an exercise.) In particular, if q(T) = 0 then q(T 1 ) = 0. Thus T 1 satisfies any polynomial satisfied by T over F. What can be said in the opposite direction? LEMMA 6.6.1 Suppose that V = V1 ffi V2 , where V1 and V2 are subspaces of V invariant under T. Let T1 and T 2 be the linear transformations induced by Ton V1 and V2 , respectively. If the minimal polynomial of T 1 over F is p1 (x) while that of T 2 is p 2 ( x), then the minimal polynomial for T over F is the least common multiple cifp1 (x) andp2 (x). Sec. 6.6 Canonical Forms: Decomposition of V: Jordan Form 299 Proof. If p(x) is the minimal polynomial for T over F, as we have seen above, both p(T 1 ) and p(T2) are zero, whence P1 (x) I p(x) and p2(x) I p(x). But then the least common multiple of p1 (x) and p2(x) must also divide p(x). On the other hand, if q(x) is the least common multiple of p1 (x) and p2 (x), consider q(T). For v1 E V1, since p1 (x) I q(x), v1q(T) = v1q(T1) = 0; similarly, for v2 E V2, v2 q(T) = 0. Given any v E V, v can be written as v = v1 + v2, where v1 E V1 and V2 E V2, in consequence of which vq(T) = (v1 + v2 )q(T) = v1q(T) + v2q(T) = 0. Thus q(T) = 0 and T satisfies q(x). Combined with the result of the first paragraph, this yields the lemma. COROLLARY If V = V1 ffi · · · ffi Vk where each Vi is invariant under T and if pi ( x) is the minimal polynomial over F if Ti, the linear tran.iformation induced by T on Vi, then the minimal polynomial if T over F is the least common multiple of Pt (x), P2(x), · · ·, Pk(x). We leave the proof of the corollary to the reader. Let T E Ap(V) and suppose that p(x) in F[x] is the minimal polynomial ofT over F. By Lemma 3.9.5, we can factor p(x) in F[x] in a unique way as p(x) = q1 (x)hq 2 (x) 12 · · · qk(x) 1k, where the qi(x) are distinct irreducible polynomials in F[x] and where 11, l2, ... , lk are positive integers. Our objective is to decompose V as a direct sum of subspaces invariant under T such that on each of these the linear transformation induced by T has, as minimal polynomial, a power of an irreducible polynomial. If k = 1, V itself already does this for us. So, suppose that k > 1. Let vl = {v E vI vql (T)lt = 0}, v2 = {v E vI vq2(T) 12 = 0}, ... ' vk = {v E vI vqk(T) 1k = 0}. It is a triviality that each vi is a subspace of v. In addition, vi is invariant under T, for if u E vi, since T and qi(X) commute, (uT)qi(Tl 1 = (uqi(T) 11)T = OT = 0. By the definition of Vi, this places u Tin Vi. Let Ti be the linear transformation induced by Ton Vi. THEOREM 6.6.1 For each i = 1, 2, ... , k, Vi =1- (0) and V = V1 ffi V2 ffi · · · ffi Vk. The minimal polynomial if Ti is qi(x) 11. Proof. If k = 1 then V = V1 and there is nothing that needs proving. Suppose then that k > 1. We first want to prove that each Vi =1- (0). Towards this end, we intro- duce the k polynomials: h1 (x) = q2 (x) 12q3 (x)h · · · qk(x) 1\\ h2(x) = q1(x)'tq3(x)'3 ... qk(x)'k, ... ' hi(x) = II qi(x) 1 i, ... , j=fti hk(x) = ql (x)ltq2 (x)'2 ... qk-1 (x)'k-t. Since k > 1, hi(x) =f. p(x), whence hi( T) =f. 0. Thus, given i, there is a v E V such that_ w = vhi( T) =f. 0. But wqi(T) 11 = v(hi(T)qi(T) 11) = vp(T) 300 Linear Transformations Ch. 6 = 0. In consequence, w =F 0 is in V; and so V; =F (0). In fact, we have shown a little more, namely, that Vh;(T) =F (0) is in V;. Another remark about the h;(x) is in order now: if vj E vj for j =F i, since qj(x) 11 I hi(x), vih;(T) = 0. The polynomials h1 (x), h2 (x), ... , hk(x) are relatively prime. (Prove!) Hence by Lemma 3.9.4 we can find polynomials a 1 (x), ... , ak(x) in F[x] such that a 1 (x)h1 (x) + · · · + ak(x)hk(x) = 1. From this we get a 1 (T)h 1 (T) + · · · + ak(T)hk(T) = 1, whence, given v E V, v = v1 = v(a 1 (T)h 1 (T) + · · · + ak(T)hk(T)) = va1 (T)h 1 (T) + · · · + vak(T)hk(T). Now, each va;(T)h;(T) is in Vh;(T), and since we have shown above that Vh;(T) c V;, we have now exhibited v as v = v1 + · · · + vk, where each V; = Va;(T)hi(T) is in V;. Thus v = vl + Vz + ... + vk. We must now verify that this sum is a direct sum. To show this, it is enough to prove that if u1 + u2 + · · · + uk = 0 with each U; E V;, then each U; = 0. So, suppose that u1 + u2 + · · · + uk = 0 and that some u;, say u1 , is not 0. Multiply this relation by h1 (T); we obtain u1h1 (T) + · · · + ukh1 (T) = Oh1 (T) = 0. However, uih1 (T) = 0 for j =F 1 since ui E Vi; the equation thus reduces to u1h1 (T) = 0. But u1q1 (T) 11 = 0 and since h1 (x) and q1 (x) are relatively prime, we are led to u1 = 0 (Prove!) which is, of course, inconsistent with the assumption that u1 =F 0. So far we have succeeded in proving that V = V1 EB V2 EB • • • EB Vk. To complete the proof of the theorem, we must still prove that the minimal polynomial of Ti on V; is q(x) 1 i. By the definition of V;, since V;q;(T) 1 ; = 0, q;(TY; = 0, whence the minimal equation ofT; must be a divisor of q;(x)'t, thus of the form q;(x)f; withh :::;; l;. By the corollary to Lemma 6.6.1 the minimal polynomial of T over F is the least common multiple of q1 (x)ft, ... , qk(x)fk and so must be q1 (x)ft · · · qk(x)fk. Since this minimal polynomial is in fact q1 (x) 11 • • • qk(x) 1k we must have that !1 ;;?: l1, ! 2 ;;?: l2 , •.• , fk ;;?: lk. Combined with the opposite inequality above, this yields the desired result l; = fi for i = 1, 2, ... , k and so com~ pletes the proof of the theorem. If all the characteristic roots of T should happen to lie in F, then the minimal polynomial of T takes on the especially nice form q(x) == (x - A1 ) 11 • • • (x - Ak) 1k where Au ... , Ak are the distinct characteristic roots of T. The irreducible factors q;(x) above are merely q;(x) = x - A;· Note that on V;, T i only has A; as a characteristic root. COROLLARY If all the distinct characteristic roots A;, ... , Ak ofT lie in F, then v can be written as v = vl EB .•• EB vk where vi = {v E vI v( T- A;) 1 i == 0} and where T; has only one characteristic root, A;, on V;. Let us go back to the theorem for a moment; we use the same notation Sec. 6.6 Canonical Forms: Decomposition of V: Jordan Form 301 Ti, Vi as in the theorem. Since V = V1 EB · · · EB Vk, if dim Vi = ni, by Lemma 6.5.1 we can find a basis of V such that in this basis the matrix of Tis of the form where each Ai is an ni x ni matrix and is in fact the matrix of Ti. What exactly are we looking for? We want an element in the similarity class ofT which we can distinguish in some way. In light of Theorem 6.3.2 this can be rephrased as follows: We seek a basis of V in which the matrix ofT has an especially simple (and recognizable) form. By the discussion above, this search can be limited to the linear trans- formations Ti; thus the general problem can be reduced from the discussion of general linear transformations to that of the specia1linear transformations whose minimal polynomials are powers of irreducible polynomials. For the special situation in which all the characteristic roots of T lie in F we do it below. The general case in which we put no restrictions on the charac- teristic roots of Twill be done in the next section. We are now in the happy position where all the pieces have been con- structed and all we have to do is to put them together. This results in the highly important and useful theorem in which is exhibited what is usually called the Jordan canonical form. But first a definition. DEFINITION The matrix A. 1 0 0 0 A. 0 1 A. with A.'s on the diagonal, 1 's on the superdiagonal, and O's elsewhere, is a basic Jordan block belonging to A. THEOREM 6.6.2 Let T E Ap(V) have all its distinct characteristic roots, At, ... , ILk, in F. Then a basis of V can be found in which the matrix T is of the form J 302 Linear Transformations Ch. 6 where each and where Bil, ... , Bi,1 are basic Jordan blocks belonging to .Ai. Proof. Before starting, note that an m x m basic Jordan block belonging to .A is merely .A + Mm, where Mm is as defined at the end of Lemma 6.5.2. By the combinations of Lemma 6.5.1 and the corollary to Theorem 6.6.1, we can reduce to the case when T has only one characteristic root .A, that is, T - .A is nilpotent. Thus T = .A + ( T - .A), and since T - .A is nil- potent, by Theorem 6.5.1 there is a basis in which its matrix is of the form r .. MJ But then the matrix of T is of the form r .A ) + r .. MJ c-· BJ using the first remark made in this proof about the relation of a basic Jordan block and the Mm's. This completes the theorem. Using Theorem 6.5.1 we could arrange things so that in each Ji the size of Bil ~ size of Bi 2 ~ When this has been done, then the matrix is called the Jordan form of T. Note that Theorem 6.6.2, for nilpotent matrices, reduces to Theorem 6.5.1. We leave as an exercise the following: Two linear transformations in AF( V) which have all their characteristic roots in F are similar if and only if they can be brought to the same Jordanform. Thus the Jordan form- acts as a \"determiner\" for similarity classes of this type of linear transformation. In matrix terms Theorem 6.6.2 can be stated as follows: Let A E Fn and suppose that K is the splitting field of the minimal polynomial of A over F; then an invertible matrix C E K n can be found so that CAC- 1 is in Jordan form. Sec. 6.6 Canonical Forms: Decomposition of V: Jordan Form 303 We leave the few small points needed to make the transition from Theorem 6.6.2 to its matrix form, just given, to the reader. One final remark: If A E Fn and if in Km where K is the splitting field of the minimal polynomial of A over F, CAC- 1 = ( It J where each Ji corresponds to a different characteristic root, .Ai, of A, then the multiplicity of Ai as a characteristic root of A is defined to be ni, where Ji is an ni x ni matrix. Note that the sum of the multiplicities is exactly n. Clearly we can similarly define the multiplicity of a characteristic root of a linear transformation. Problems I. If S and T are nilpotent linear transformations which commute, prove that STandS + Tare nilpotent linear transformations. 2. By a direct matrix computation, show that (~ ~ ; ~) and (~ ! ; !) are not similar. 3. If n1 ;;:::: n2 and m1 ;;:::: m2 , by a direct matrix computation prove that ( M nt ) and (Mm1 ) Mn2 Mm2 are similar if and only if n1 = mv n2 = m2 • *4. If n1 ;;:::: n2 ;;:::: n3 and m1 ;;:::: m2 ;;:::: m3 , by a direct matrix computation prove that and are similar if and only if n1 = m1 , n2 = m2 , n3 = m3 . 5. (a) Prove that the matrix (-: -1 I -l) is nilpotent, and find its invariants and Jordan form. 304 Linear Transformations Ch. 6 (b) Prove that the matrix in part (a) is not similar to (-: -i -i). 6. Prove Lemma 6.6.1 and its corollary even if the sums involved are not direct sums. 7. Prove the statement made to the effect that two linear transformations in Ap( V) all of whose characteristic roots lie in F are similar if and only if their Jordan forms are the same (except for a permutation in the ordering of the characteristic roots). 8. Complete the proof of the matrix version of Theorem 6.6.2, given in the text. 9. Prove that then x n matrix 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 having entries 1 'son the subdiagonal and O's elsewhere, is similar to Mn. 10. IfF has characteristic p > 0 prove that A = G ~)satisfies A• = I. 11. If F has characteristic 0 prove that A = (~ ~) satisfies Am = 1, form > 0, only if rx = 0. 12. Find all possible Jordan forms for (a) All 8 x 8 matrices having x 2 ( x - 1) 3 as minimal polynomial. (b) All 10 x 10 matrices, over a field of characteristic different from 2, having x 2 (x - 1) 2 (x + 1) 3 as minimal polynomial. 13. Prove that then x n matrix is similar to if the characteristic ofF is 0 or if it is p and p .1' n. What is the multi- plicity of 0 as a characteristic root of A? \" ' ' i Sec. 6.7 Canonical Forms: Rational Canonical Form 305 A matrix A = (r:tii) is said to be a diagonal matrix if r:tii = 0 for i =f:. j, that is, if all the entries off the main diagonal are 0. A matrix (or linear transformation) is said to be diagonalizable if it is similar to a diagonal matrix (has a basis in which its matrix is diagonal). 14. If Tis in A(V) then Tis diagonalizable (if all its characteristic roots are in F) if and only if whenever v(T - A)m = 0, for v E V and A E F, then v(T - A) = 0. 15. Using the result of Problem 14, prove that if E 2 = E then E is diagonalizable. 16. If E 2 = E and F 2 = F prove that they are similar if and only if they have the same rank. 17. If the multiplicity of each characteristic root of T is 1, and if all the characteristic roots of T are in F, prove that T is diagonalizable over F. 18. If the characteristic of F is 0 and if T E Ap(V) satisfies ym = 1, prove that if the characteristic roots of Tare in F then Tis diagonaliz- able. (Hint: Use the Jordan form of T.) * 19. If A, B E F are diagonalizable and if they commute, prove that there is an element C E Fn such that both CAC- 1 and CBC- 1 are diagonal. 20. Prove that the result of Problem 19 is false if A and B do not commute. 6.7 Canonical Forms: Rational Canonical Form The Jordan form is the one most generally used to prove theorems about linear transformations and matrices. Unfortunately, it has one distinct, serious drawback in that it puts requirements on the location of the charac- teristic roots. True, if T E Ap( V) (or A E Fn) does not have its characteristic roots in F we need but go to a finite extension, K, ofF in which all the char- acteristic roots of T lie and then to bring T to Jordan form over K. In fact, this is a standard operating procedure; however, it proves the result in Kn and not in Fn. Very often the result in Fn can be inferred from that in Km but there are many occasions when, after a result has been established A E Fn, ~onsidered as an element in Km we cannot go back from Kn to the desired information in Fn. Thus we need some canonical form for elements in Ap(V) (or in Fn) presumes nothing about the location of the characteristic roots of its ~~.u.n ... ~~ts, a canonical form and a set of invariants created in Ap(V) itself only its elements and operations. Such a canonical form is provided by the rational canonical form which is described below in Theorem 6. 7 .I its corollary. 306 linear Transformations Ch. 6 Let T E Ap( V); by means of T we propose to make V into a module over F[x], the ring of polynomials in x over F. We do so by defining, for any polynomial f(x) in F[x], and any v E V, f(x)v = zif(T). We leave the verification to the reader that, under this definition of multiplication of elements of V by elements of F[x], V becomes an F[x]-module. Since Vis finite-dimensional over F, it is finitely generated over F, hence, all the more so over F[x] which contains F. Moreover, F[x] is a Euclidean ring; thus as a finitely generated module over F[x], by Theorem 4.5.1, Vis the direct sum of a finite number of cyclic submodules. From the very way in which we have introduced the module structure on V, each of these cyclic submodules is invariant under T; moreover there is an element m0 , in such a submodule M, such that every element m, in M, is of the form m = m0 f(T) for somef(x) E F[x]. To determine the nature ofT on V it will be, therefore, enough for us to know what T looks like on a cyclic submodule. This is precisely what we intend, shortly, to determine. But first to carry out a preliminary decomposition of V, as we did in Theorem 6.6.1, according to the decomposition of the minimal polynomial of T as a product of irreducible polynomials. Let the minimal polynomial of T over F be p(x) = q1 (x)e 1 • • • qk(x)ek, where the qi(x) are distinct irreducible polynomials in F[x] and where each ei > 0; then, as we saw earlier in Theorem 6.6.1, V = V1 EB V2 EB · · · EB Vk where each Vi is invariant under T and where the minimal polynomial of T on Vi is qi(x)e;. To solve the nature of a cyclic submodule for an arbitrary T we see, from this discussion, that it suffices to settle it for a T whose minimal polynomial is a power of an irreducible one. We prove the LEMMA 6.7.1 Suppose that T, in Ap(V), has as minimal polynomial over F the polynomial p(x) = Yo + y1x + · · · + y,_ 1x'- 1 + x'. Suppose, further, that V, as a module (as described above), is a cyclic module (that is, is cyclic relative to T.) Then there is basis of V over F such that, in this basis, the matrix of Tis 0 0 0 0 0 0 0 _-'}'1 Proof. Since V is cyclic relative to T, there exists a vector v in V such that every element w, in V, is of the form w = if (T) for somef (x) in F[x]. Now if for some polynomial s(x) in F[x], vs(T) = 0, then for any W in V, ws(T) = (zif(T))s(T) = vs(T)f(T) = 0; thus s(T) annihilates all of V and so s(T) = 0. But then p(x) I s(x) since p(x) is the minimal poly- Sec. 6.7 Canonical Forms: Rational Canonical Form 307 . nomial of T. This remark implies that v, vT, vT 2 , ••• , vrr- 1 are linearly independent over F, for if not, then a0 v + a1vT + · · · + ar_ 1vrr- 1 = 0 with a0 , •.. , ar- 1 in F. But then v(a0 + a1 T + · · · + ar_ 1 yr- 1) = 0, hence by the above discussion p(x) I (a0 + a1x + · · · + ar_ 1xr- 1 ), which is impossible since p(x) is of degree runless ao = C{1 = ... = C{r-1 = 0. Since yr = -Yo - y1 T- · · · - Yr- 1 yr- 1, we immediately have that rr+k, for k ~ 0, is a linear combination of I, T, ... , yr- 1, and so f (T), for any f (x) E F[ x ], is a linear combination of I, T, ... , yr- 1 over F. Since any win Vis of the form w = if (T) we get that w is a linear com-bination ofv, vT, ... , vrr- 1. We have proved, in the above two paragraphs, that the elements v, vT, ... , vrr-t form a basis of V over F. In this basis, as is immediately veri-fied, the matrix ofT is exactly as claimed DEFINITION If f(x) = Yo + y1x + · · · + Yr- 1xr- 1 + xr 1s in F[x], then the r X r matrix 0 0 0 0 0 0 0 0 0 -Yo -y1 -Yr-1 is called the companion matrix off(x). We write it as C(f(x)). Note that Lemma 6. 7 .I says that if V is cyclic relative to T and if the minimal ·; ·polynomial of T in F [ x] is p ( x) then for some basis of V the matrix of T is C (p ( ~)). . Note further that the matrix C ( f ( x)), for any monic f ( x) in F [ x], satisfies ~~j(x) and has f(x) as its minimal polynomial. (See Problem 4 at the end of ~·this section; also Problem 29 at the end of Section 6: I.) , We now prove the very important ,T': > }*THEOREM 6.7.1 If T in Ap(V) has as minimal polynomial p(x) = q(x)e, ... ,..~\"''\"''\"' q(x) is a monic, irreducible polynomial in F[x], then a basis of V over F can found in which the matrix of T is of the form r(q(x)\") C(q(x)'J e = e1 ~ e2 ~ • • • ~ er. Proof. Since V, as a module over F[x], is finitely generated, and since [x] is Euclidean, we can decompose Vas V = V1 Ee · · · Ee Vr where the 308 Linear Transformations Ch. 6 Vi are cyclic modules. The Vi are thus invariant under T; if Ti is the linear transformation induced by Ton Vi, its minimal polynomial must be a divisor of p(x) = q(x)e so is of the form q(x)e 1• We can renumber the spaces so that e1 ;;:::: e2 ;;:::: • • • ;;:::: e,. Now q(T)e 1 annihilates each Vi, hence annihilates V, whence q(T)e 1 = 0. Thus e1 ;;:::: e; since e1 is clearly at most ewe get that e1 = e. By Lemma 6. 7.1, since each Vi is cyclic relative to T, we can find a basis such that the matrix ofthe linear transformation of Ti on V, is C(q(x)e 1). Thus by Theorem 6.6.1 a basis of V can be found so that the matrix ofT in this basis is c(q(x)') C(q(x)'•)). COROLLARY If Tin Ap(V) has minimal polynomial p(x) = q1 (x) 11 • • • qk(x) 1k over F, where q1 (x), ... , qk(x) are irreducible distinct polynomials in F[x], then a basis of V can be found in which the matrix qf T is of the form where each Proof. By Theorem 6.5.1, V can be decomposed into the direct sum V = V1 EB • • • EB Vk, where each Vi is invariant under T and where the minimal polynomial of Ti, the linear transformation induced by Ton Vi, has as minimal polynomial qi(xy 1• Using Lemma 6.5.1 and the theorem just proved, we obtain the corollary. If the degree of qi(x) is di, note that the sum of all the dieii is n, the dimension of V over F. DEFINITION The matrix of Tin the statement of the above corollary is called the rational canonical form or' T. DEFINITION The polynomials q1 (x)e 11 , q1 (x)e 12 , ••• , q1 (x)e 1rt, ... , qk(xt 1 c~, ... , qk(xykrk in F[x] are called the elementary divisors ofT. One more definition! Sec. 6.7 Canonical Forms: Rational Canonical Form 309 DEFINITION If dimF (V) = n, then the characteristic polynomial of T, Pr(x), is the product of its elementary divisors. We shall be able to identify the characteristic polynomial just defined with another polynomial which we shall explicitly construct in Section 6.9. The characteristic polynomial of Tis a polynomial of degree n lying in F[x]. It has many important properties, one of which is contained in the REMARK Every linear transformation T E Ap(V) satisfies its characteristic polynomial. Every characteristic root ofT is a root of Pr(x). Note 1. The first sentence of this remark is the statement of a very famous theorem, the Cayley-Hamilton theorem. However, to call it that in the form we have given is a little unfair. The meat of the Cayley-Hamilton theorem is the fact that T satisfies Pr(x) when Pr(x) is given in a very specific, con- crete form, easily constructible from T. However, even as it stands the remark does have some meat in it, for since the characteristic polynomial is a polynomial of degree n, we have shown that every element in Ap(V) does satisfy a polynomial of degree n lying in F[x]. Until now, we had only proved this (in Theorem 6.4.2) for linear transformations having all their characteristic roots in F. Note 2. As stated the second sentence really says nothing, for whenever T satisfies a polynomial then every characteristic root of T satisfies this same polynomial; thus Pr(x) would be nothing special if what were stated in the theorem were all that held true for it. However, the actual story is the following: Every characteristic root ofT is a root of Pr(x), and conversely, every root of Pr(x) is a characteristic root of T; moreover, the multiplicity of..,.any root of Pr(x), as a root of the polynomial, equals its multiplicity as a characteristic root ofT. We could prove this now, but defer the proofuntillater when we shall be able to do it in a more natural fashion. Proof of the Remark. We only have to show that T satisfies Pr(x), but this beomes almost trivial. Since Pr(x) is the product of q1 (x)e 11 , q1 (x)e12 , ... , qk(xYk 1 , ••• , and since e11 = e1 , e21 = e2 , ••• , ekl = ek, Pr(x) is di- visible by p(x) = q1 (x)e 1 • • • qk(x)ek, the minimal polynomial of T. Since P(T) = 0 it follows that Pr(T) = 0. We have called the set of polynomials arising in the rational canonical form of T the elementary divisors of T. It would be highly desirable if these determined similarity in Ap(V), for then the similarity classes in Ap(V) Would be in one-to-one correspondence with sets of polynomials in F[x]. We propose to do this, but first we establish a result which implies that two linear transformations have the same elementary divisors. THEOREM 6.7.2 Let V and W be two vector spaces over F and suppose that l/1 310 Linear Transformations Ch. 6 is a vector space isomorphism of V onto W. Suppose that S E Ap( V) and T E Ap( W) are such that for any v E V, (vS)t/J = (vt/J) T. Then S and T have the same elementary divisors. Proof. We begin with a simple computation. If v E V, then (vS2)t/l = ((vS)S)t/J = ((vS)t/1) T = ((vt/J)T) T = (vt/J)T 2 • Clearly, if we continue in this pattern we get (vSm)t/1 = (vt/J) ym for any integer m ~ 0 whence for any polynomial f(x) E F[x] and for any v E V, (vj(S))t/1 = (vt/J)f(T). Hf(S) =0 then (vt/J)f(T) =0 for any vEV, and since t/1 maps V onto W, we would have that Wf (T) = (0), in consequence of which f (T) = 0. Conversely, if g(x) E F[x] is such that g(T) = 0, then for any v E V, (vg(S))t/1 = 0, and since t/1 is an isomorphism, this results in vg(S) = 0. This, of course, implies that g(S) = 0. Thus S and T satisfy the same set of polynomials in F [x], hence must have the same minimal polynomial. where q1 (x), ... , qk(x) are distinct irreducible polynomials in F[x] If U is a subspace of V invariant under S, then Ut/J is a subspace of W invariant under T, for ( Uljl) T = ( US)t/J c Ut/J. Since U and Uljl are isomorphic, the minimal polynomial of sl, the linear transformation induced by Son U is the same, by the remarks above, as the minimal polynomial of T 1 , the linear transformation induced on Uljl by T. Now, since the minimal polynomial for Son Vis p(x) = q1 (x)e 1 • • • qk(x)ek, as we have seen in Theorem 6. 7.1 and its corollary, we can take as the first elementary divisor of S the polynomial q1 (x)e 1 and we can find a sub- space of vl of v which is invariant under s such that 1. V = V1 EB M where M is invariant under S. 2. The only elementary divisor of S1 , the linear transformation induced on vl by s, is ql (x)e 1 • 3. The other elementary divisors of S are those of the linear transformation S2 induced by Son M. We now combine the remarks made above and assert 1. W = W1 EB N where W1 = V1 t/1 and N = Mljl are invariant under T. 2. The only elementary divisor of T 1 , the linear transformation induced by Ton Wu is q1 (x)e 1 (which is an elementary divisor ofT since the minimal polynomial of Tis p(x) = q1 (x)e 1 • • • qk(x)ek). 3. The other elementary divisors of Tare those of the linear transformation T 2 induced by Ton N. \\ Since N = Mljl, M and N are isomorphic vector spaces over F under the isomorphism t/12 induced by 'ljl. Moreover, if u EM then (uS2 )t/12 === Sec. 6.7 Canonical Forms: Rational Canonical Form 311 (uS)l/1 = (ul/J) T = (ul/12 ) T 2 , hence S2 and T 2 are in the same relation vis-a-vis 1/12 as S and T were vis-a-vis 1/J. By induction on dimension (or repeating the argument) S2 and T 2 have the same elementary divisors. But since the elementary divisors of S are merely q1 (xY 1 and those of S2 while those of Tare merely q1 (x)e 1 and those of T 2 , S, and T must have the same elementary divisors, thereby proving the theorem. Theorem 6.7.1 and its corollary gave us the rational canonical form and gave rise to the elementary divisors. We should like to push this further and to be able to assert some uniqueness property. This we do in THEOREM 6.7.3 The elements Sand Tin Ap(V) are similar in Ap(V) if and only if they have the same elementary divisors. Proof. In one direction this is easy, for suppose that S and T have the same elementary divisors. Then there are two bases of V over F such that the matrix of Sin the first basis equals the matrix of Tin the second (and each equals the matrix of the rational canonical form). But as we have seen several times earlier, this implies that Sand Tare similar. We now wish to go in the other direction. Here, too, the argument resembles closely that used in Section 6.5 in the proof of Theorem 6.5.2. Having been careful with details there, we can afford to be a little sketchier here. We first remark that in view of Theorem 6.6.1 we may reduce from the general case to that of a linear transformation whose minimal polynomial is a power of an irreducible one. Thus without loss of generality we may suppose that the minimal polynomial of Tis q(x)e where q(x) is irreducible in F[x] of degree d. ~· The rational canonical form tells us that we can decompose Vas V = V1 E9 · · · E9 Vn where the subspaces Vi are invariant under T and where the linear transformation induced by Ton Vi has as matrix C(q(x)e;), the companion matrix of q(x)e;. We assume that what we are really trying to prove is the following: If V = U1 E9 U2 E9 · · · EB Us where the Ui are invariant under T and where the linear transformation induced by Ton Ui has as matrix C(q(x)fi), f 1 2 f 2 2 · · · 2/s, then r = s and e1 = j 1 , e2 = j 2 , .•• , er = j,.. (Prove that the proof of this is equivalent to proving the theorem ! ) Suppose then that we do have the two decompositions described above, V = V1 EB · · · EB Vr and V = U1 EB · · · EB Us, and that some ei =1= i'i· Then there is a first integer m such that em =I= fm, while e1 = j 1 , •.. , em_ 1 = fm- 1 • We may suppose that em > fm· Now g(T)fm annihilates Um, Um+ 1, .•• , Us, whence 312 Linear Transformations Ch. 6 However, it can be shown that the dimension of Uiq(T) 1 m for i ~ m is d (fi - fm) (Prove!) whence dim (Vq(T)fm) = d(j1 - fm) + · · · + d(fm-1 - fm)· On the other hand, Vq(T)fm ::> V1q(T)fm Ee · · · Ee · · · Ee Vmq(T)fm and since Viq(T)fm has dimension d(ei - fm), fori ~ m, we obtain that dim (Vq(T) 1 m) ;;::: d(ei - fm) + · · · + d(em - fm)· Since e1 = j 1 , ... , em_ 1 = fm- 1 and em > fm, this contradicts the equality proved above. We have thus proved the theorem. COROLLARY 1 Suppose the two matrices A, B in Fn are similar in Kn where K is an extension of F. Then A and B are already similar in Fn. Proof. Suppose that A, BE Fn are such that B = c- 1 AC with c E Kn. We consider Kn as acting on K<n>, the vector space of n-tuples over K. Thus F<n> is contained in x<n> and although it is a vector space over Fit is not a vector space over K. The image of F<n>, in x<n>, under C need not fall back in F(n) but at any rate F<n>c is a subset of K<n> which is a vector space over F. (Prove!) Let V be the vector space F(n) over F, W the vector space F<n>c over F, and for v E V let vl/J = vC. Now A E Ap(V) and BE Ap(W) and for any v E V, (vA)t/J = vAG = vCB = (vt/J)B whence the conditions of Theorem 6. 7.2 are satisfied. Thus A and B have the same elementary divisors; by Theorem 6. 7. 3, A and B must be similar in F n· A word of caution: The corollary does not state that if A, B E F n are such that B = c- 1AC with CEKn then c must of necessity be in Fn; this is false. It merely states that if A, BE Fn are such that B = c- 1 AC with C E Kn then there exists a (possibly different) DE Fn such that B = n- 1AD. Problems 1. Verify that V becomes an F[x]-module under the definition given. 2. In the proof of Theorem 6. 7.3 provide complete proof at all points marked \"(Prove).\" *3. (a) Prove that every root of the characteristic polynomial of T is a characteristic root of T. (b) Prove that the multiplicity of any root of Pr(x) is equal to its multiplicity as a characteristic root of T. 4. Prove that for f(x) EF[x], (f;(j(x)) satisfiesf(x) and hasf(x) as its minimal polynomial. What is its characteristic polynomial? 5. IfF is the field of rational numbers, find all possible rational canonical forms and elementary divisors for 1 Sec. 6.8 Trace and Transpose 313 (a) The 6 X 6 matrices m F 6 having (x- l)(x 2 + 1) 2 as minimal polynomial. (b) The 15 x 15 matrices m Fts having (x 2 + x + 1) 2(x 3 + 2)2 as minimal polynomial. (c) The 10 x 10 matrices in F 10 having (x 2 + 1) 2 (x 3 + 1) as mini- mal polynomial. 6. (a) If K is an extension ofF and if A is in Kn, prove that A can be written as A = AtAt + · · · + A0k where At, ... , Ak are in Fn and where At, ... , A.k are in K and are linearly independent over F. (b) With the notation as in part (a), prove that if BE Fn is such that AB = 0 then At B = A 2B = · · · = AkB = 0. (c) If C in Fn commutes with A prove that C commutes with each of A 1 , A2 , ••• , Ak. *7. If At, ... , Ak are in Fn and are such that for some A1, ••• , A.k in K, an extension ofF, AtAt + · · · + AkAk is invertible in Kn, prove that ifF has an infinite number of elements we can find OCt, ••• , rxk in F such that rx1At + · · · + rxkAk is invertible in Fn. *8. IfF is a finite field prove the result of Problem 7 is false. *9. Using the results of Problems 6(a) and 7 prove that ifF has an infinite number of elements then whenever A, BE Fn are similar in Km where K is an extension ofF, then they are familiar in Fn. (This provides us with a proof, independent of canonical forms of Corollary 1 to Theorem 6.7.3 in the special case when Pis an infinite field.) 10. Using matrix computations (but following the lines laid out in Problem 9), prove that if F is the field of real numbers and K that of corriplex numbers, then two elements in F2 which are similar with K2 are already similar in F 2 • 6.8 Trace and Transpose After the rather heavy going of the previous few sections, the uncomplicated nature of the material to be treated now should come as a welcome respite. Let F be a field and let A be a matrix in Fn. DEFINITION The trace of A is the sum of the elements on the main diagonal of A. We shall write the trace of A as tr A; if A = ( rxii), then n tr A = L rxii. i=t The fundamental formal properties of the trace function are contained in 314 Linear Transformations Ch. 6 LEMMA 6.8.1 For A, BE Fn and A E F, 1. tr (AA) = A tr A. 2. tr (A + B) = tr A + tr B. 3. tr (AB) = tr (BA). Proof. To establish parts 1 and 2 (which assert that the trace is a linear functional on Fn) is straightforward and is left to the reader. We only present the proof of part 3 of the lemma. If A = (r:t.ii) and B = (f3ii) then AB = (yii) where n \"lij = L: r:t.ikpkj k=l and BA (Jlii) where n Jlij L: Pikr:t.kj· k=l Thus tr (AB) = ~ Yu = ~ (~\"'\"'fl • .): if we interchange the order of summation in this last sum, we get COROLLARY If A is invertible then tr (ACA- 1 ) = tr C. Proof. Let B = CA- 1 ; then tr (ACA- 1 ) = tr (AB) = tr (BA) = tr (CA- 1A) = tr C. This corollary has a twofold importance; first, it will allow us to define the trace of an arbitrary linear transformation; secondly, it will enable us to find an alternative expression for the trace of A. DEFINITION If T E A(V) then tr T, the trace ofT, is the trace of m1 (T) where m1 (T) is the matrix of Tin some basis of V. We claim that the definition is meaningful and depends only on T and not on any particular basis of V. For ifm 1 (T) and m2 (T) are the matrices of Tin two different bases of V, by Theorem 6.3.2, m1 (T) and m2 (T) are similar matrices, so by the corollary to Lemma 6.8.1 they have the same trace. LEMMA 6.8.2 If T E A ( V) then tr T is the sum of the characteristic roots of T (using each characteristic root as often as its multiplicity). Sec. 6.8 Trace and Transpose 315 Proof. We can assume that Tis a matrix in Fn; if K is the splitting field for the minimal polynomial of T over F, then in Km by Theorem 6.6.2, T can be brought to its Jordan form, ]. J is a matrix on whose diagonal appear the characteristic roots of T, each root appearing as often as its multiplicity. Thus tr J = sum of the characteristic roots of T; however, since J is of the form AT A- 1 , tr J = tr T, and this proves the lemma. If Tis nilpotent then all its characteristic roots are 0, whence by Lemma 6.8.2, tr T = 0. But if T is nilpotent, then so are T 2 , T 3 , ••• ; thus tr Ti = 0 for all i ~ 1. What about other directions, namely, if tr Ti = 0 for i = 1, 2, ... does it follow that Tis nilpotent? In this generality the answer is no, for ifF is a field of characteristic 2 then the unit matrix in F2 has trace 0 (for 1 + 1 = 0) as do all its powers, yet clearly the unit matrix is not nilpotent. However, if we restrict the characteristic ofF to be 0, the result is indeed true. LEMMA 6.8.3 IfF is a field of characteristic 0, and if T E Ap(V) is such that tr Ti = 0 for all i ~ 1 then Tis nilpotent. Proof. Since T E Ap(V), T satisfies some minimal polynomial p(x) = xm + OCtXm-l + ... + ocm; from ym + OCt ym-l + ... + ocm-1 T + am = 0, taking traces of both sides yields tr ym + OCt tr ym- 1 + ... + OCm-1 tr T + tr ocm = 0. However, by assumption, tr Ti = 0 for i ~ 1, thus we get tr ocm = 0; if dim V = n, tr ocm = nocm whence nocm = 0. But the characteristic of F is 0; therefore, n =f. 0, hence it follows that ocm = 0. Since the constant term of the minimal polynomial of T is 0, by Theorem 6.1.2 T is singular and so 0 is a characteristic root of T. We can consider T as a matrix in Fn and therefore also as a matrix in Kn, where K is an extension of F which in turn contains all the characteristic roots of T. In Km by Theorem 6.4.1, we can bring T to triangular form, and since 0 is a characteristic root of T, we can actually bring it to the form = (g_l ~J 316 Linear Transformations Ch. 6 where is an (n - 1) x (n- 1) matrix (the *'s indicate parts in which we are not interested in the explicit entries). Now hence 0 = tr Tk = tr T/. Thus T 2 is an (n - 1) x (n - 1) matrix with the property that tr T/ = 0 for all k ~ 1. Either using induction on n, or repeating the argument on T 2 used for T, we get, since a2 , ••• , an are the characteristic roots of T2 , that a2 = · · · = an = 0. Thus when T is brought to triangular form, all its entries on the main diagonal are 0, forcing T to be nilpotent. (Prove!) This lemma, though it might seem to be special, will serve us in good stead often. We make immediate use of it to prove a result usually known as the Jacobson lemma. LEMMA 6.8.4 IfF is of characteristic 0 and if Sand T, in Ap(V), are such that ST- TS commutes with S, then ST- TS is nilpotent. Proof. For any k ~ 1 we compute (ST - TS)k. Now (ST - TS)k = (ST - TS)k- 1 (ST - TS) = (ST - TS)k- 1ST - (ST - TS)k-l TS. Since ST - TS commutes with S, the term (ST - TS)k- 1ST can be written in the form S((ST- TS)k- 1T). If we let B = (ST- TS)k-I T, we see that (ST - TS)k = SB - BS; hence tr ((ST - TSl) = tr (SB - BS) = tr (SB) - tr (BS) = 0 by Lemma 6.8.1. The previous lemma now tells us that ST - TS must be nilpotent. The trace provides us with an extremely useful linear functional on Fn (and so, on Ap(V)) into F. We now introduce an important mapping of F n in to itself. DEFINITION If A = (aii) E Fn then the transpose of A, written as A', is the matrix A' = (yij) where \"'ii = aii for each i andj. The transpose of A is the matrix obtained by interchanging the rows and columns of A. The basic formal properties of the transpose are contained in LEMMA 6.8.5 For all A, B E Fn, I. (A')' = A. 2. (A + B)' = A' + B'. 3. (AB)' = B'A'. Sec. 6.8 Trace and Transpose 317 Proof. The proofs of parts I and 2 are straightforward and are left to the reader; we content ourselves with proving part 3. Suppose that A = (rx;i) and B = (fJii); then AB = (A.;) where n Aij = L (XikfJkj' k=l Therefore, by definition, (AB)' = (Jlij), where n Jlij = Aji = L (XjkfJki' k=l On the other hand, A' = (yij) where Yii = rxii and B' eii = fJii' whence the (i,j) element of B'A' is n n n I: eikYkj = L:: Pkirxjk = L:: rxjkpki = Jlij· k=l k=l k=l That is, (AB)' = B' A' and we have verified part 3 of the lemma. In part 3, ifwe specialize A = B we obtain (A 2 )' = (A') 2 . Continuing, we obtain (Ak)' = (A')k for all positive integers k. When A is invertible, then (A- 1 )' = (A')- 1 . There is a further property enjoyed by the transpose, namely, if A E F then (A-A)' = A.A' for all A E Fn. Now, if A E Fn satisfies a polynomial rx0 Am + rx1Am- 1 + · · · + rxm = 0, we obtain (cx0 Am + · · · + rxm)' = 0' F 0. Computing out (rx0Am + · · · + rxm)' using the properties of the transpose, we obtain rx0 (A')m + rx1 (A')m- 1 + · · · + rxm = 0, that is to say, A' satisfies any polynomial over F which is satisfied by A. Since A = (A')', by the same token, A satisfies any polynomial over F which is satisfied by A'. In particular, A and A' have the same minimal polynomial over F and so they have the same characteristic roots. One can show each root occurs with the same multiplicity in A and A'. This is evident once it is established that A and A' are actually similar (see Problem 14). /DEFINITION The matrix A is said to be a symmetric matrix if A' = A. DEFINITION The matrix A is said to be a skew-symmetric matrix if A'= -A. When the characteristic ofF is 2, since I = -I, we would not be able to distinguish between symmetric and skew-symmetric matrices. We make 318 linear Transformations Ch. 6 the flat assumption for the remainder rif this section that the characteristic of F is different from 2. Ready ways for producing symmetric and skew-symmetric matrices are available to us. For instance, if A is an arbitrary matrix, then A + A' is symmetric and A - A' is skew-symmetric. Noting that A = !(A + A') + !(A - A'), every matrix is a sum of a symmetric one and a skew-symmetric one. This decomposition is unique (see Problem 19). Another method of producing symmetric matrices is as follows: if A is an arbitrary matrix, then both AA' and A'A are symmetric. (Note that these need not be equal.) It is in the nature of a mathematician, once given an interesting concept arising from a particular situation, to try to strip this concept away from the particularity of its origins and to employ the key properties of the con- cept as a means of abstracting it. We proceed to do this with the transpose. We take, as the formal properties of greatest interest, those properties of the transpose contained in the statement of Lemma 6.8.5 which asserts that on Fn the transpose defines an anti-automorphism of period 2. This leads us to make the DEFINITION A mapping * from Fn into Fn is called an adjoint on Fn if 1. (A*)* = A; 2. (A + B)* = A* + B*; 3. (AB)* = B*A*; for all A, B E Fn. Note that we do not insist that ().A)* = ).A* for ). E F. In fact, in some of the most interesting adjoints used, this is not the case. We discuss one such now. Let F be the field of complex numbers; for A = (rxii) E F\"' let A* = (yii) where Yii = fiii the complex conjugate of rxji· In this case *is usually called the Hermitian adjoint on Fn. A few sections from now, we shall make a fairly extensive study of matrices under the Hermitian adjoint. Everything we said about transpose, e.g., symmetric, skew-symmetric, can be carried over to general adjoints, and we speak about elements sym- metric under* (i.e., A* = A), skew-symmetric under*, etc. In the exercises at the end, there are many examples and problems referring to general adjoints. However, now as a diversion let us play a little with the Hermitian adjoint. We do not call anything we obtain a theorem, not because it is not worthy of the title, but rather because we shall redo it later (and properly label it) from one ce:tltral point ofview. So, let us suppose that F is the field of complex numbers and that the adjoint, *, on Fn is the Hermitian adjoint. The matrix A is called Hermitian if A* =A. Sec. 6.8 Trace and Transpose 319 First remark: If A =1- 0 E Fm then tr (AA*) > 0. Second remark: As a consequence of the first remark, if A1 , ..• , Ak E Fn and if A1A1 * + A 2 A 2 * + · · · + AkAk* = 0, then A 1 = A 2 = · · · = Ak = 0. Third remark: If A is a scalar matrix then A* = X, the complex conjugate of A. Suppose that A E Fn is Hermitian and that the complex number ex + {3i, where ex and f3 are real and i 2 = - 1, is a characteristic root of A. Thus A - (ex + f3i) is not invertible; but then (A - (ex + f3i)) (A - (ex - f3i)) = (A - ex) 2 + {3 2 is not invertible. However, if a matrix is singular, it must annihilate a nonzero matrix (Theorem 6.1.2, Corollary 2). There must therefore be a matrix C =1- 0 such that C( (A - ex) 2 + {32 ) = 0. We multiply this from the right by C* and so obtain C (A - ex) 2C* + f3 2CC* = 0. (1) Let D = C(A - ex) and E = pc. Since A* = A and ex is real, C(A- ex) 2 C* = DD*; since f3 is real, f3 2 CC* = EE*. Thus equation (1) becomes DD* + EE* = 0; by the remarks made above, this forces D = 0 and E = 0. We only exploit the relation E = 0. Since 0 = E = f3C and since C =1- 0 we must have f3 = 0. What exactly have we proved? In fact, we have proved the pretty (and important) result that if a complex number A is a characteristic root qf a Hermitian matrix, then A must be real. Ex- ploiting properties of the field of complex numbers, one can actually restate this as follows: The characteristic roots qf a Hermitian matrix are all real. We continue a little farther in this vein. For A E Fm let B = AA*; B is a Hermitian matrix. If the real number ex is a characteristic root of B, can ex be an arbitrary real number or must it be restricted in some way? Indeed, we claim that ex must be nonnegative. For if ex were negative then ex = - {3 2 , where f3 is a real number. But then B - ex = B + p'f: = AA* + {3 2 is not invertible, and there is a C =1- 0 such that C (AA* + {32 ) = 0. Multiplying by C* from the right and arguing as before, we obtain f3 = 0, a contradiction. We have shown that any real characteristic root of AA* must be nonnegative. In actuality, the \"real\" in this statement is superfluous and we could state: For any A E Fn all the characteristic roots of AA* are nonnegative. Problems Unless otherwise specified, symmetric and skew-symmetric refer to transpose. I. Prove that tr (A + B) = tr A + tr B and that for A E F, tr (AA) = A trA. 2. (a) Using a trace argument, prove that if the characteristic ofF is 0 then it is impossible to find A, B E Fn such that AB - BA = I. 320 Linear Transformations Ch. 6 (b) In part (a), prove, in fact, that 1 - (AB - BA) cannot be nil- potent. 3. (a) Letfbe a function defined on Fn having its values in F such that 1. f(A +B) = f(A) + f(B); 2. j(AA) = Aj(A); 3. f(AB) = f(BA); for all A, BE Fn and all A E F. Prove that there is an element a0 E F such that f (A) = a0 tr A for every A in Fn. (b) If the characteristic of F is 0 and if the fin part (a) satisfies the additional property that f(l) = n, prove that f(A) = tr A for all A E Fn. Note that Problem 3 characterizes the trace function. *4. (a) If the field F has an infinite number of elements, prove that every element in Fn can be written as the sum of regular matrices. (b) IfF has an infinite number of elements and if j, defined on Fn and having its values in F, satisfies 1. f(A + B) = f(A) + f(B); 2. j(AA) = Aj(A); 3. f(BAB- 1 ) = f(A); for every A E F\"' A E F and invertible element B in F\"' prove that f (A) = a0 tr A for a particular a0 E F and all A E Fn. 5. Prove the Jacobson lemma for elements A, BE Fn if n is less than the characteristic of F. 6. (a) If C E F\"' define the mapping de on F\"' by dc(X) = XC- CX for X E Fn. Prove that dc(XY) = (dc(X))Y + X(dc(Y)). (Does this remind you of the derivative?) (b) Using (a), prove that if AB - BA commutes with A, then for any polynomial q(x) E F[x], q(A)B - Bq(A) = q'(A)(AB - BA), where q'(x) is the derivative of q(x). *7. Use part (b) of Problem 6 to give a proof of the Jacobson lemma. (Hint: Let p(x) be the minimal polynomial for A and consider 0 = p(A)B - Bp(A).) 8. (a) If A is a triangular matrix, prove that the entries on the diagonal of A are exactly all the characteristic roots of A. (b) If A is triangular and the elements on its main diagonal are 0, prove that A is nilpotent. 9. For any A, BE Fn and A E F prove that (A')' = A, (A + B)' = A' + B', and (AA)' = AA'. 10. If A is invertible, prove that (A- 1 )' = (A')- 1 . 11. If A is skew-symmetric, prove that the elements on its main diagonal are all 0. Sec. 6.8 Trace and Transpose 321 12. If A and B are symmetric matrices, prove that AB is symmetric if and only if AB = BA. 13. Give an example of an A such that AA' =f. A' A. *14. Show that A and A' are similar. 15. The symmetric elements in Fn form a vector space; find its dimension and exhibit a basis for it. * 16. In F n let S denote the set of symmetric elements; prove that the subring of Fn generated by Sis all of Fn. * 17. If the characteristic ofF is 0 and A E Fn has trace 0 ( tr A = 0) prove that there is a C E Fn such that GAG- 1 has only O's on its main diagonal. *18. IfF is of characteristic 0 and A E Fn has trace 0, prove that there exist B, C E Fn such that A = BC - CB. (Hint: First step, assume, by result of Problem 17, that all the diagonal elements of A are 0.) 19. (a) IfF is of characteristic not 2 and if * is any adjoint on Fm let S = {A E Fn I A* = A} and let K = {A E Fn I A* = -A}. Prove that S + K = Fn. (b) If AEFn and A= B + C where BES and CEK, prove that B and C are unique and determine them. 20. (a) If A, BE S prove that AB + BA E S. (b) If A, BE K prove that AB - BA E K. (c) If AES and BEK prove that AB-BAES and that AB+ BAEK. 21. If</> is an automorphism of the field F we define the mapping ci> on Fn by: If A = (~tij) then <I>(A) = (</>(~tij)). Prove that <I>(A + JJ'j = <I>(A) + <I>(B) and that <I>(AB) = <I>(A)<I>(B) for all A, BE Fn. 22. If * and @ define two adjoints on Fm prove that the mapping l/1 :A ~ (A*)@ for every A E Fn satisfies 1/J(A + B) = 1/J(A) + 1/J(B) and 1/J(AB) = 1/J(A)l/J(B) for every A, BE Fn. 23. If* is any adjoint on Fn and A is a scalar matrix in Fm prove that A* must also be a scalar matrix. *24. Suppose we know the following theorem: If ljJ is an automorphism of Fn (i.e., 1/J maps Fn onto itself in such a way that 1/J(A + B) = 1/J(A) + 1/J(B) and 1/J(AB) = 1/J(A)l/J(B)) such that 1/J(.A) = A for every scalar matrix .A, then there is an element P E Fn such that 1/J(A) = PAP- 1 for every A E Fn. On the basis of this theorem, prove: If* is an adjoint of Fn such that .A* = A for every scalar matrix A then there exists a matrix P E Fn such that A* = PA'P- 1 for every A E Fn. Moreoever, P- 1P' must be a scalar. 25. If P E Fn is such that p- 1 P' =f. 0 is a scalar, prove that the mapping defined by A* = PA'P- 1 is an adjoint on Fn. 322 Linear Transformations Ch. 6 *26. Assuming the theorem about automorphisms stated in Problem 24, prove the following: If* is an adjoint on Fn there is an automorphism c/J of Fofperiod 2 and an element P E Fn such that A* = P(tl>(A))'P- 1 for all A E Fn (for notation, see Problem 21). Moreover, P must satisfy p- 1tl>(P)' is a scalar. Problems 24 and 26 indicate that a general adjoint on Fn is not so far removed from the transpose as one would have guessed at first glance. *27. If t/J is an automorphism of Fn such that t/J(A) = A for all scalars, prove that there is aPE Fn such that t/J(A) = PAP- 1 for every A E Fn. In the remainder of the problems, F will be the field of complex numbers and* the Hermitian adjoint on F n· 28. If A E Fn prove that there are unique Hermitian matrices B and C suchthatA = B + iC (i 2 = -1). 29. Prove that tr AA* > 0 if A i= 0. 30. By directly computing the matrix entries, prove that if A1A1 * + · · · + AkAk * = 0, then A1 = A 2 = · · · = Ak = 0. 31. If A is in Fn and if BAA* = 0, prove that BA = 0. 32. If A in Fn is Hermitian and BAk = 0, prove that BA = 0. 33. If A E Fn is Hermitian and if A, f1 are two distinct (real) characteristic roots of A and if C(A - A) = 0 and D(A - /1) = 0, prove that CD*= DC* = 0. *34. (a) Assuming that all the characteristic roots of the Hermitian matrix A are in the field of complex numbers, combining the results of Problems 32, 33, and the fact that the roots, then, must all be real and the result of the corollary to Theorem 6.6.1, prove that A can be brought to diagonal form; that is, there is a matrix P such that PAP- 1 is diagonal. (b) In part (a) prove that P could be chosen so that PP* = 1. 35. Let vn = {A E Fn I AA* = 1 }. Prove that vn is a group under matrix multiplication. 36. If A commutes with AA* - A* A prove that AA* = A* A. 6.9 Determinants The trace defines an important and useful function from the matrix ring Fn (and from Ap( V)) into F; its properties concern themselves, for the most part, with additive properties of matrices. We now shall introduce the even more important function, known as the determinant, which maps Fn into F. Sec. 6.9 Determinants Its properties are closely tied to the multiplicative properties of matrices. Aside from its effectiveness as a tool in proving theorems, the determinant is valuable in \"practical\" ways. Given a matrix T, in terms of explicit determinants we can construct a concrete polynomial whose roots are the characteristic roots of T; even more, the multiplicity of a root of this poly- nomial corresponds to its multiplicity as a characteristic root of T. In fact, the characteristic polynomial of T, defined earlier, can be exhibited as this explicit, determinantal polynomial. Determinants also play a key role in the solution of systems of linear equations. It is from this direction that we shall motivate their definition. There are many ways to develop the theory of determinants, some very elegant and some deadly and ugly. We have chosen a way that is at neither of these extremes, but which for us has the advantage that we can reach the results needed for our discussion of linear transformations as quickly as possible. In what follows F will be an arbitrary field, Fn the ring of n x n matrices over F, and p<n> the vector space of n-tuples over F. By a matrix we shall tacitly understand an element in Fn. As usual, Greek letters will indicate elements ofF (unless otherwise defined). Consider the system of equations OCuX1 + OC12X2 f3t, OC21X1 + OCz2X2 Pz· We ask: Under what conditions on the ocii can we solve for x 1, x 2 given arbitrary {31 , {32 ? Equivalently, given the matrix when does this map F< 2> onto itself? Proceeding as in high school, we eliminate x 1 between the two equations; the criterion for solvability then turns out to be oc11 oc22 - oc12 oc21 =I= 0. We now try the system of three linear equations OCuxt + OC12X2 + OC13X3 = f3t, OC21 X1 + OCzzXz + 0Cz3X3 = f3z, OC31X1 + OC32X2 + OC33X3 = /33, and again ask for conditions for solvability given arbitrary {31 , {32 , {33 • Eliminating x1 between these two-at-a-time, and then x 2 from the resulting two equations leads us to the criterion for solvability that <Xu OCz20C33 + OCtzOCz30C31 + oc13oc21 OC32 - oc12oc21 OC33 - ocu 0Cz30C32 - OC130CzzOC31 =I= 0. 323 324 Linear Transformations Ch. 6 Using these two as models (and with the hindsight that all this will work) we shall make the broad jump to the general case and shall define the de- terminant of an arbitrary n x n matrix over F. But first a little notation! Let Sn be the symmetric group of degree n; we consider elements in Sn to be acting on the set {1, 2, ... ' n }. For (J E sm a(i) will denote the image of i under a. (We switch notation, writing the permutation as acting from the left rather than, as previously, from the right. We do so to facilitate writing subscripts.) The symbol (- 1 t for (J E sn will mean + 1 if (J is an even permutation and - 1 if a is an odd permutation. DEFINITION If A = (aii) then the determinant of A, written det A, is the element LueSn ( -l)CTa1u(1 )a2u(2) ••• anu(n) in F. We shall at times use the notation for the determinant of the matrix Note that the determinant of a matrix A is the sum (neglecting, for the moment, signs) of all possible products of entries of A, one entry taken from each row and column of A. In general, it is a messy job to expand the determinant of a matrix-after all there are n! terms in the expansion-but for at least one type of matrix we can do this expansion visually, namely, LEMMA 6.9.1 The determinant of a triangular matrix is the product of its entries on the main diagonal. Proof. Being triangular implies two possibilities, namely, either all the elements above the main diagonal are 0 or all the elements below the main diagonal are 0. We prove the result for A of the form 0 and indicate the slight change in argument for the other kind of triangular matrices. Since a 1 i = 0 unless i = 1, in the expansion of det A the only nonzero contribution comes in those terms where a(l) = 1. Thus, since a is a I I Sec. 6.9 Determinants permutation, a(2) =I= I ; however, if a(2) > 2, a 2 a(l) = 0, thus to get a nonzero contribution to det A, a(2) = 2. Continuing in this way, we must have a(i) = i for all i, which is to say, in the expansion of det A the only nonzero term arises when a is the identity element of Sn. Hence the sum of the n! terms reduces to just one term, namely, a11 a22 • • • anm which is the contention of the lemma. If A is lower triangular we start at the opposite end, proving that for a nonzero contribution a(n) = n, then a(n - I) = n - I, etc. Some special cases are of interest: l.If A= C' J is diagonal, det A 2. If J the identity matrix, then det A 1. 3. If the scalar matrix, then det A = .A\". Note also that if a row (or column) of a matrix consists of 0' s then the determinant . is 0, for each term of the expansion of the determinant would be a product in which one element, at least, is 0, hence each term is 0. Given the matrix A = ( aii) in Fn we can consider its first row v1 = (a11 , a 12 , ... , a 1 n) as a vector in p<n); similarly, for its second row, v2 , and the others. We then can consider det A as a function of the n vectors V1, ••• , vn. Many results are most succinctly stated in these terms so we often consider det A = d(v 1 , ••• , vn); in this the notation is always meant to imply that v1 is the first row, v2 the second, and so on, of A. One further remark: Although we are working over a field, we could just easily assume that we are working over a commutative ring, except in obvious places where we divide by elements. This remark will only when we discuss determinants of matrices having polynomial entries, little later in the section. 325 326 Linear Transformations Ch. 6 LEMMA 6.9.2 If A E Fn andy E F then d(v1, ... , vi_ 1, yvi, vi+ 1, .•. , vn) yd(v1, ... ' vi-1' vi, vi+1' ..• ' vn)· Note that the lemma says that if all the elements in one row of A are multiplied by a fixed element y in F then the determinant of A is itself multiplied by y. Proof. Since only the entries in the ith row are changed, the expansion of d(v 1, ... , vi_ 1, yvi, vi+ 1, ... , vn) is L ( -1)0'0Cta(1) ••• oci-1,a(i-1)(yocia(i))oci+1,a(i+1) ••• OCna(n); aESn since this equals y LaESn ( -l)aoc 1a( 1) • • • OCia(i) · · · OCna(n)' it does indeed equal yd (v1, .•• , vn)· LEMMA 6.9.3 d(v1, ... ' vi-1' vi, vi+1' ... ' vn) + d(v1, ... ' vi-1' ui, vi+1' ... ' vn) = d(v1, ... ' vi-1' vi + ui, vi+1' ... ' vn)· Before proving the result, let us see what it says and what it does not say. It does not say that det A + det B = det (A + B); this is false as is mani- fest in the example A = (1 0) 0 0 ' B=(o o) 0 1 ' where det A = det B = 0 while det (A + B) = 1. It does say that if A and Bare matrices equal everywhere but in the ith row then the new matrix obtained from A and B by using all the rows of A except the ith, and using as ith row the sum of the ith row of A and the ith row of B, has a deter- minant equal to det A + det B. If A = G !) and B = G :} then det A -2, det B = I, det G !) = -1 = det A + det B. Proof. If v1 = (ocll, ••• ' OC1n), ••• ' vi = (oci1' ••. ' OCin), ... ' vn :::::: (ocn 1 , ..• , ocnn) and if ui = (f3i1, ... , Pin), then d(v1, · · ·' Vi-1' Ui + Vi, Vi+V · · ·' Vn) = L (- 1 )O'oc1a(1) ••• oci -1 ,a(i -1)( OCia(i) + Pia(i))oci + 1 ,a(i + 1) ••• OCna(n) aESn = L (- 1) a OC1a(l) • • • OCi -1 ,a(i -1 )OCia(i) • • • OCna(n) aESn + L ( -1)0'oc1a(1) 0 •• oci-1,a(i-l)f3ia(i)' .• OCna(n) aESn = d(v1, ... ' vi, ... , vn) + d(v1, ... ' ui, ... ' vn)· Sec. 6.9 Determinants The properties embodied in Lemmas 6.9.1, 6.9.2, and 6.9.3, along with that in the next lemma, can be shown to characterize the determinant function (see Problem 13, end of this section). Thus, the formal property exhibited in the next lemma is basic in the theory of determinants. LEMMA 6.9.4 If two rows of A are equal (that is, v, = vs for r =1 s), then det A = 0. Proof. Let A = (aii) and suppose that for some r, s where r =1 s, a,i = asi for allj. Consider the expansion det A = L ( -1 )ua1u(1) · · · a,u<r> · · · asu<s> · · · anu<n>· acEsn In the expansion we pair the terms as follows: For (J E sn we pair the term ( -1 )O'a1u(1) ... anu(n) with the term ( -1 rO'ahu(1) ... an~u(n) where 't' is the transposition (a(r), a(s)). Since 't' is a transposition and -r2 = 1, this indeed gives us a pairing. However, since aru(r) = asu(r)' by assumption, and aru(r) = aS~O'(S)' we have that aru(r) = as~u(s)• Similarly, asu(s) = 1Xr~u(r)• On the other hand, for i =I r and i =I s, since -ra(i) = a(i), IX;u(i) = aitu(i)• Thus the terms a1u(1) · · · anu(n) and ahu(1) · · · an~u(n) are equal. The first occurs with the sign ( -1 )u and the second with the sign ( -1 )~u in the expansion of det A. Since 't' is a transposition and so an odd permutation, ( -l)~u = - ( -l)u. Therefore in the pairing, the paired terms cancel each other out in the sum, whence det A = 0. (The proof does not depend on the characteristic of F and holds equally well even in the case of characteristic 2.) From the results so far obtained we can determine the effect, on a de- terminant of a given matrix, of a given permutation of its rows. LEMMA 6.9.5 Interchanging two rows of A changes the sign of its determinant. Proof. Since two rows are equal, by Lemma 6.9.4, d(v1 , ••• , V;_ 1 , vi + vi, V;+ 1 , ••• , vi_ 1 , V; + vi, vi+ 1 , ••• , vn) = 0. Using Lemma 6.9.3 several times, we can expand this to obtain d(v1 , ••• , V;_ 1 , V;, ... , vi_ 1 , Vi' ••• , V n) + d ( V 1, ... , Vi _ 1, Vi' ••• , V j _ 1, V ;, • • • , V n) + d ( V 1, ... , Vi_ 1, V ;, •.. , vi_ 1 , V;, ••• , vn) + d(v1 , ••• , V;_ 1 , vi, ... , vi_ 1 , vi, ... , vn) = 0. However, each of the last two terms has in it two equal rows, whence, by Lemma 6.9.4, each is 0. The above relation then reduces to d(v1 , ••• , vi_ 1 , V;, ••• , Vj_ 1 , Vj, ••• , Vn) + d(v1 , ••• , V;_ 1 , Vj, ••• , Vj-1' V;, ••• , Vn) = 0, 'which is precisely the assertion of the lemma. COROLLARY If the matrix B is obtained from A by a permutation of the rows ' of A then det A = ± det B, the sign being + I if the permutation is even, - 1 if the permutation is odd. 327 328 Linear Transformations Ch. 6 We are now in a position to collect pieces to prove the basic algebraic property of the determinant function, namely, that it preserves products. As a homomorphism of the multiplicative structure of Fn into F the de- terminant will acquire certain important characteristics. THEOREM 6.9.1 For A, BE Fm det (AB) = (det A) (det B). Proof. Let A = (rxii) and B = ({3ii); let the rows of B be the vectors u1, u2, ... , un. We introduce then vectors w 1, ..• , wn as follows: W1 = IX11U1 + IX12U2 + · ·\" + IX1nUm W2 = IX21Ul + IX22U2 + ... + IX2nun, Consider d(w 1, .•• , wn); expanding this out and making many uses of Lemmas 6.9.2 and 6.9.3, we obtain d(wl, •.. ' wn) = L IXth IX2i2 •• \"IXnind(uh, uh, ..• ' uiJ· it,h, ... ,in In this multiple sum i 1, ••• , in run independently from 1 ton. However, if any two ir = is then uir = uis whence d(uh, ... , uir' •.. , uis' •.• , uiJ = 0 by Lemma 6.9.4. In other words, the only terms in the sum that may give a nonzero contribution are those for which all of i 1, i 2 , ••• , in are distinct, that is for which the mapping 2 (J is a permutation of 1, 2, ... , n. Also any such permutation IS possible. Finally note that by the corollary to Lemma 6.9.5, when is a permutation, then d(ui 1, Ui2, ... , ud ( -1)u det B. Thus we get d(w 1, ... , wn) = L IXtu(l) • • ·rxna(n)( -1)u det B uESn (detB) L (-1)u1Xlu(l)···rxna(n) , uESn (det B) (det A). We now wish to identify d(w 1, ... , wn) as det (AB). However, since Wl = IXuUl + ... + IXlnun, W2 = IX21U1 + ... + IX2nUm ... ' Wn = CXnt U1 + ... + CXnnUn Sec. 6.9 Determinants we get that d(w 1 , ••• , wn) is det C where the first row of Cis w1 , the second is w 2 , etc. However, if we write out w 1 , in terms of coordinates we obtain W1 = <XuU1 + · · · + atnun = au (f3u, P12' · · ·, P1n) + · ·\" + tX1n(f3nt' · · ·' Pnn) (a11P11 + a12P21 + · · · + a1nPn1' a11P12 + · · · + a1nPn2' · · ·, a11P1n + · · · + a1nPnn) which is the first row of AB. Similarly w2 is the second row of AB, and so for the other rows. Thus we have C = AB. Since det (AB) = det C = d(w 1, ••• , wn) = (det A)(det B), we have proved the theorem. COROLLARY 1 If A is invertible then det A # 0 and det (A- 1 ) (detA)- 1 . Proof Since AA- 1 = I, det ( AA- 1 ) = det 1 = I. Thus by the theorem, 1 = det (AA- 1 ) = (detA)(detA- 1). This relation then states that det A # 0 and det A- 1 = 1/det A. COROLLARY 2 If A is invertible then for all B, det(ABA- 1 ) = detB. Proof. Using the theorem, as applied to (AB)A- 1, we get det ( (AB)A- 1 ) = det (AB) det (A- 1 ) = det A det B det (A- 1 ). Invoking Corollary 1, we reduce this further to det B. Thus det (ABA- 1 ) = det B. Corollary 2 allows us to define the determinant of a linear transformation. For, let TEA( V) and let m1 (T) be the matrix of T in some basis of V. Given another basis, if m2 (T) is the matrix of T in this second basis, then by Theorem 6.3.2, m2 (T) = Cm1 (T)C- 1, hence det (m2 (T)) = det (m 1 (T)) by Corollary 2 above. That is, the matrix of T in any basis has the same determinant. Thus the definition: det T = det m1 (T) is in fact independent of the basis and provides A( V) with a determinant function. In one of the earlier problems, it was the aim of the problem to prove that A', the transpose of A, is similar to A. Were this so (and it is), then A' and A, by Corollary 2, above would have the same determinant. Thus we should not be surprised that we can give a direct proof of this fact. LEMMA 6.9.6 det A = det (A'). Proof. Let A = (aii) and A' = ([3ii); of course, Pii = aji· Now det A = L ( -1 )ua.1u(1) • • • tXnu(n) uESn while 32~ I 330 Linear Transformations Ch. 6 However, the term ( -1 )uau< 1>1 • • • au(n)n is equal to ( -1 ra 1u-1( 1) • • • anu-1(n)· (Prove!) But (J and (J- 1 are of the same parity, that is, if (J is odd, then so is (J- 1 , whereas if (J is even then (J- 1 is even. Thus ( -l)O\"a1u-1(1) ••• anu-1(n) = ( -1 )u-1Gttu-1(1) ••• Gtna-1(n)• Finally as (J runs over Sn then (J-t runs over Sn. Thus det A' = L ( -1 Yatu(l) ••• anu(n) uESn = det A. In light of Lemma 6.9.6, interchanging the rows and columns of a matrix does not change its determinant. But then Lemmas 6.9.2-6.9.5, which held for operations with rows of the matrix, hold equally for the columns of the same matrix. We make immediate use of the remark to derive Cramer's rule for solving a system of linear equations. Given the system of linear equations ctuXt + ... + alnxn = Pt we call A = (aii) the matrix of the system and A = det A the determinant of the system. Suppose that A =/= 0; that is, A= =/= 0. By Lemma 6.9.2 (as modified for columns instead of rows), However, as a consequence ofLemmas 6.9.3, 6.9.4, we can add any multiple of a column to another without cha.pging the determinant (see Problem 5). Add to the ith column of xiA, x1 times the first column, x2 times the second, ... , xi times thejth column (for j =/= i). Thus Sec. 6.9 Determinants 331 au a1' i -1 Pt a1' i + 1 at n x/!i = L\\i, say. a1 n an,i-1 /3n an,i+1 ann Hence, xi= L\\ifL\\. This is THEOREM 6.9.2 (CRAMER's RuLE) If the determinant, L\\, of the system of linear equations is different from 0, then the solution of the system is given by xi = L\\d A, where Ai is the determinant obtained from A by replacing in A the ith column by {31 , {32, ... ' pn' Example The system has determinant hence -5 2 3 -7 1 0 1 x1 = A x1 + 2x2 + 3x3 = - 5, 2x1 + x2 + x3 = - 7, x1 + x2 + x3 = 0, 1 2 3 A= 2 1 1 \"# 0, 1 -5 3 2 -7 0 Xz = A 1 2 -5 2 -7 1 0 x3 = A We can interrelate invertibility of a matrix (or linear transformation) with the value of its determinant. Thus the determinant provides us with a criterion for invertibility. THEOREM 6.9.3 A is invertible if and only ifdet A\"# 0. Proof. If A is invertible, we have seen, in Corollary 1 to Theorem 6.9.1, that det A \"# 0. Suppose, on the other hand, that det A \"# 0 where A = (aii). By Cramer's rule we can solve the system a11x1 + · · · + a1nxn = P1 332 Linear Transformations Ch. 6 for xt, ... , xn given arbitrary flt, ... , fln• Thus, as a linear transformation on p<n>, A' is onto; in fact the vector (flt, ... , fln) is the image under A' of ( ~~ , ... , ~·). Being onto, by Theorem 6.1.4, A' is invertible, hence A is invertible (Prove!). We can see Theorem 6.9.3 from an alternative, and possibly more in- teresting, point of view. Given A E Fn we can embed it in Kn where K is an extension ofF chosen so that in Kn, A can be brought to triangular form. Thus there is aBE Kn such that BAB- 1 = ( A*t here At, ... , An are all the characteristic roots of A, each occurring as often as its multiplicity as a characteristic root of A. Thus det A det (BAB-t) = At A2 ···An by Lemma 6.9.1. However, A is invertible if and only if none of its characteristic roots is 0; but det A =I 0 if and only if At A2 • • • An =I 0, that is to say, if no characteristic root of A is 0. Thus A is invertible if and only if det A =I 0. This alternative argument has some advantages, for in carrying it out we actually proved a subresult interesting in its own right, namely, LEMMA 6.9.7 det A is the product, counting multiplicities, of the characteristic roots of A. DEFINITION Given A E Fm the secular equation of A IS the polynomial det (x - A) in F[x]. Usually what we have called the secular equation of A is called the characteristic polynomial of A. However, we have already defined the characteristic polynomial of A to be the product of its elementary divisors. It is a fact (see Problem 8) that the characteristic polynomial of A equals its secular equation, but since we did not want to develop this explicitly in the text, we have introduced the term secular equation. Let us compute and example. If then X _ A = (X 0) _ (1 2) = (X - 1 -2) ; 0 X 3 0 -3 X Sec. 6.9 Determinants 333 hence det (x - A) secular equation of (x- l)x- (-2)(-3) = x 2 - x- 6. Thus the is x 2 - x - 6. G ~) A few remarks about the secular equation: If A is a root of det (x - A), then det (A. - A) = 0; hence by Theorem 6.9.3, A - A is not invertible. Thus A is a characteristic root of A. Conversely, if A is a characteristic root of A, A - A is not invertible, whence det (). - A) = 0 and so A is a root of det (x - A). Thus the explicit, computable polynomial, the secular equation of A, provides us with a polynomial whose roots are exactly the characteristic roots if A. We want to go one step further and to argue that a given root enters as a root of the secular equation precisely as often as it has multiplicity as a characteristic root of A. For if Ai is the characteristic root of A with multiplicity mi, we can bring A to triangular form so that we have the matrix shown in Figure 6.9.1, where each Ai appears on the diagonal mi 0 0 BAB- 1 = * 0 Figure 6.9.1 times. But as indicated by the matrix in Figure 6.9.2, det (x - A) = det (B(x - A)B- 1) = (x - A1)m1 (x - A2)m 2 • • • (x - A.k)mk, and so each B(x- A)B- 1 = x- BAB- 1 0 0 == * Figure 6.9.2 334 linear Transformations Ch. 6 A.i, whose multiplicity as a characteristic root of A is mi is a root of the poly- nomial det (x - A) of multiplicity exactly mi. We have proved THEOREM 6.9.4 The characteristic roots of A are the roots, with the correct multiplicity, of the secular equation, det (x - A), of A. We finish the section with the significant and historic Cayley-Hamilton theorem. THEOREM 6.9.5 Every A E Fn satisfies its secular equation. Proof. Given any invertible B E K n for any extension K of F, A E F and BAB- 1 satisfy the same polynomials. Also, since det (x - BAB- 1) = det (B(x - A)B- 1 ) = det (x - A), BAB- 1 and A have the same secular equation. If we can show that some BAB- 1 satisfies its secular equation, then it will follow that A does. But we can pick K :::::> F and BE Kn so that BAB- 1 is triangular; in that case we have seen long ago (Theorem 6.4.2) that a triangular matrix satisfies its secular equation. Thus the theorem is proved. Problems 1. If F is the field of complex numbers, evaluate the following determi- nants: 1 2 3 5 6 8 -1 (a) 12 ~ i ~I· 4 3 0 0 (b) 4 5 6. (c) 10 12 16 -2· 7 8 9 1 2 3 4 2. For what characteristics ofF are the following determinants 0: 2 3 0 (a) 3 2 1 0 ? 1 2 4 5 6 3 4 5 (b) 4 5 3 ? 5 3 4 3. If A is a matrix with integer entries such that A- 1 is also a matrix with integer entries, what can the values of det A possibly be? 4. Prove that if you add the multiple of one row to another you do not change the value of the determinant. *5. Given the matrix A = (rxii) let Aii be the matrix obtained from A by removing the ith row and jth column. Let Mii = ( -1)i+i det Aii. Mii is called the cofactor of rxii. Prove that det A = ail Mil + · · · + (XinMin• Sec. 6.9 Determinants 335 6. (a) If A and B are square submatrices, prove that det (~ ~) = (det A)(det B). (b) Generalize part (a) to :} where each Ai is a square submatrix. 7. IfC(f) is the companion matrix ofthe polynomialf(x), prove that the secular equation ofC(f) isf(x). 8. Using Problems 6 and 7, prove that the secular equation of A is its characteristic polynomial. (See Section 6. 7; this proves the remark made earlier that the roots of Pr(x) occur with multiplicities equal to their multiplicities as characteristic roots of T.) 9. Using Problem 8, give an alternative proof of the Cayley-Hamilton theorem. I 0. If F is the field of rational numbers, compute the secular equation, characteristic roots, and their multiplicities, of (! 0 ~)· (b) (~ 2 ~)· (c) (1 :) . 0 0 4 (a) 0 0 2 4 0 4 4 .... II. For each matrix in Problem 10 verify by direct matrix computation that it satisfies its secular equation. *12. If the rank of A is r, prove that there is a square r x r submatrix of A of determinant different from 0, and if r < n, that there Is no (r + 1) x (r + 1) submatrix of A with this property. * 13. Letf be a function on n variables from p<n> to F such that (a) f (v1, ... , vn) = 0 for vi = vi E p<n> for i =1- j. (b) f (v1, ... , avi, ... , vn) = af (v1, ... , vn) for each i, and a E F. (c) f (vv ···,vi + ui, vi+1' · · ·, vn) = f (v1f · ·' vi-1, vi, vi+1' · · ·' vn) + f(v1, • · ·' Vi-1' Ui, Vi+V · · ·' vn)· (d) f(e 1 , ••• , en) = 1, where e1 = (1, 0, ... , 0), e2 = (0, 1, 0, ... , 0), ... ,en= (0,0, ... ,0, 1). Prove that f (v1 , .•. , vn) = det A for any A E Fn, where v1 is the first row of A, v2 the second, etc. 14. Use Problem 13 to prove that det A' = det A. 336 linear Transformations Ch. 6 15. (a) Prove that AB and BA have the same secular (characteristic) equation. (b) Give an example where AB and BA do not have the same minimal polynomial. 16. If A is triangular prove by a direct computation that A satisfies its secular equation. 17. Use Cramer's rule to compute the solutions, in the real field, of the systems (a) x + y + z = 1, 2x + 3y + 4z = 1, X- y- Z = 0. (b) X + y + Z + W = 1, x + 2y + 3z + 4w = 0, x + y + 4z + 5w = 1, x + y + 5z + 6w = 0. 18. (a) Let GL(n, F) be the set of all elements in Fn whose determinant is different from 0. Prove GL(n, F) is a group under matrix multiplication. (b) Let D(n, F) = {A E GL(n, F) I det A = 1 }. Prove that D(n, F) is a normal subgroup of GL(n, F). (c) Prove that GL(n, F)/D(n, F) is isomorphic to the group of non- zero elements ofF under multiplication. 19. If K be an extension field of F, let E(n, K, F) = {A E GL(n, K) I det A E F}. (a) Prove that E(n, K, F) is a normal subgroup of GL(n, K). *(b) Determine GL(n, K)/E(n, K, F). *20. If F is the field of rational numbers, prove that when N is a normal subgroup of D(2, F) then either N = D(2, F) or N consists only of scalar matrices. 6.10 Hermitian, Unitary, and Normal Transformations In our previous considerations about linear transformations, the specific nature of the field F has played a relatively insignificant role. When it did make itself felt it was usually in regard to the presence or absence of charac- teristic roots. Now, for the first time, we shall restrict the field F-generally it will be the field of complex numbers but at times it may be the field of real numbers-and we shall make heavy use of the properties of real and complex numbers. Unless explicitly-stated otherwise, in all rif this section F will denote the field rif complex numbers. We shall also be making extensive and constant use of the notions and results of Section 4.4 about inner product spaces. The reader would be well advised to review and to digest thoroughly that material before proceeding. 1 I Sec. 6.10 Hermitian, Unitary, and Normal Transformations 337 One further remark about the complex numbers: Until now we have managed to avoid using results that were not proved in the book. Now, however, we are forced to deviate from this policy and to call on a basic fact about the field of complex numbers, often known as \"the fundamental theorem of algebra,\" without establishing it ourselves. It displeases us to pull such a basic result out of the air, to state it as a fact, and then to make use of it. Unfortunately, it is essential for what follows and to digress to prove it here would take us too far afield. We hope that the majority of readers will have seen it proved in a course on complex variable theory. FACT 1 A polynomial with codficients which are complex numbers has all its roots in the complex field. Equivalently, Fact 1 can be stated in the form that the only nonconstant irreducible polynomials over the field of complex numbers are those of degree 1. FACT 2 The only irreducible, nonconstant, polynomials over the field of real numbers are either of degree I or of degree 2. The formula for the roots of a quadratic equation allows us to prove easily the equivalence of Facts 1 and 2. The immediate implication, for us, of Fact 1 will be that every linear trn•nrl'nnnnl'wn which we shall consider will have all its characteristic roots in the of complex numbers. In what follows, V will be a finite-dimensional inner-product space over F, the field of complex numbers; the inner product of two elements of .... V 1 be written, as it was before, as (v, w). lf T E A(V) is such that (vT, v) = 0 for all v E V, then Since (vT, v) = 0 for v E V, given u, wE V, ((u + w) T, u + w) = Expanding this out and making use of (uT, u) = (wT, w) = 0, we (uT, w) + (wT, u) = 0 for all u, wE V. (1) Since equation ( 1) holds for arbitrary w in V, it still must hold if we in it w by iw where i 2 = -1; but ('itT, iw) = -i(uT, w) whereas (iw) T, u) = i(wT, u). Substituting these values in (1) and canceling out i - (uT, w) + (wT, u) = 0. (2) Adding (1) and (2) we get (wT, u) = 0 for all u, wE V, whence, in , (wT, wT) = 0. By the defining properties of an inner-product 338 Linear Transformations Ch. 6 space, this forces wT = 0 for all wE V, hence T = 0. (Note: If Vis an inner-product space over the real field, the lemma may be false. For example, let V = {(a, /3) I a, f3 real}, where the inner-product is the dot product. Let T be the linear transformation sending (a, /3) into (- /3, a). A simple check shows that (vT, v) = 0 for all v E V, yet T =/= 0.) DEFINITION The linear transformation T E A(V) is said to be unitary if (uT, vT) = (u, v) for all u, v E V. A unitary transformation is one which preserves all the structure of V, its addition, its multiplication by scalars and its inner product. Note that a unitary transformation preserves length for II vii = -J (v, v) = -J (vT, vT) = llvTII· Is the converse true? The answer is provided us in LEMMA 6.1 0.2 If (vT, vT) = (v, v) for all v E f1 then Tis unitary. Proof. The proof is in the spirit of that of Lemma 6.1 0.1. Let u, v E V: by assumption ((u + v) T, (u + v)T) = (u + v, u + v). Expanding this out and simplifying, we obtain (uT, vT) + (vT, uT) = (u, v) + (v, u), (1) for u, v E V. In ( 1) replace v by iv; computing the necessary parts, this yields - (uT, vT) + (vT, uT) = - (u, v) + (v, u). (2) Adding (1) and (2) results in (uT, vT) = (u, v) for all u, v E V, hence T is unitary. We characterize the property of being unitary in terms of action on a basis of V. THEOREM 6.10.1 The linear transformation Ton Vis unitary if and only if it takes an orthonormal basis of V into an orthonormal basis of V. Proof. Suppose that {vv ... , vn} is an orthonormal basis of V; thus (vi, vi) = 0 for i =/= j while (vi, vi) = 1. We wish to show that if T is unitary, then {v1 T, ... , vnT} is also an orthonormal basis of V. But (viT, viT) = (vi, vi) = 0 for i :f='J and (viT, viT) = (vi, vi) = 1, thus indeed {v1 T, ... , vnT} is an orthonormal basis of V. On the other hand, if T E A(V) is such that both {v1, ... , vn} and {v1 T, ... , vnT} are orthonormal bases of V, if u, wE V then n u = L: a1vi, i= 1 n w = L: f3tvi, i=l Sec. 6.10 Hermitian, Unitary, and Normal Transformations 339 whence by the orthonormality of the v/s, n (u, w) = L r~.J3i. i=1 However, n n uT = L rt.iviT and wT = L f3iviT i= 1 i= 1 whence by the orthonormality of the viT's, (uT, wT) (u, w), proving that T is unitary. Theorem 6.10.1 states that a change of basis from one orthonormal basis to another is accomplished by a unitary linear transformation. LEMMA 6.10.3 If TEA(V) then given any VE V there exists an element w E V, depending on v and T, such that ( u T, v) = ( u, w) for all u E V. This w is uniquely determined by v and T. Proof. To prove the lemma, it is sufficient to exhibit a w E V which .works for all the elements of a basis of V. Let {uu ... , un} be an ortho- normal basis of V; we define n w = L (ui T, v)ui. i=1 'An easy computation shows that (ui, w) = (ui T, v) hence the element w has the desired property. That w is unique can be seen as follows: Suppose that (uT, v) = (u, w1 ) = (u, w 2 ); then (u, w1 - w 2 ) = 0 for all u E~V which forces, on putting u = w1 - w2 , w 1 = w2 • Lemma 6.1 0.3 allows us to make the DEFINITION If TEA(V) then the Hermitian adjoint ofT, written asT*, is defined by (uT, v) = (u, vT*) for all u, v E V. Given v E V we have obtained above an explicit expression for vT* (as and we could use this expression to prove the various desired properties T*. However, we prefer to do it in a \"basis-free\" way. ~ LEMMA 6.1 0.4 If T E A(V) then T* E A(V). Moreover, ·1. (T*)* = T; . (S + T)* = S* + T*; (AS)* = lS*; (ST)* = T*S*; all S, T E A(V) and all A. E F. 340 linear Transformations Ch. 6 Proof. We must first prove that T* is a linear transformation on V. If u, v, ware in V, then (u, (v + w) T*) = (uT, v + w) = (uT, v) + (uT, w) = (u, vT*) + (u, wT*) = (u, vT* + wT*), in consequence of which (v + w)T* = vT* + wT*. Similarly, for A E F, (u, (A.v)T*) = (uT, A.v) = X(uT, v) = X(u, vT*) = (u, A.(vT*)), whence (A.v)T* = A.(vT*). We have thus proved that T* is a linear transformation on V. To see that (T*)* = T notice that (u, v(T*)*) = (uT*, v) = (v, uT*) = (vT, u) = (u, vT) for all u, v E V whence v(T*)* = vT which implies that (T*)* = T. Weleavetheproofsof(S + T)* = S* + T* andof(.A..T)* = 'AT* to the reader. Finally, (u, v(ST)*) = (uST, v) = (uS, vT*) = (u, vT*S*) for all u, v E V; this forces v(ST)* = vT*S* for every v E V which results in (ST) * = T*S*. As a consequence of the lemma the Hermitian adjoint defines an adjoint, in the sense of Section 6.8, on A ( V). The Hermitian adjoint allows us to give an alternative description for unitary transformations in terms of the relation ofT and T*. LEMMA 6.10.5 T E A(V) is unitary if and only ifTT* = 1. Proof. If T is unitary, then for all u, v E V, (u, vTT*) = (uT, vT) (u, v) hence TT* = 1. On the other hand, if TT* = 1, then (u, v) (u, vTT*) = (uT, vT), which implies that Tis unitary. Note that a unitary transformation is nonsingular and its inverse is just its Hermitian adjoint. Note, too, that from TT* = 1 we must have that T* T = 1. We shall soon give an explicit matrix criterion that a linear transformation be unitary. THEOREM 6.10.2 If {v1, ••• , vn} is an orthonormal basis of V and if the matrix of T E A(V) in this basis is (r:tii) then the matrix ofT* in this basis is (f3i), where f3ii = fiji· Proof. Since the matrices of T and T* in this basis are, respectively, (r:tii) and (f3ii), then Now n viT = L r:tiivi and i=l n viT* = L f3iivi. i=l by the orthonormality of the v/s. This proves the theorem. This theorem is very interesting to us in light of what we did earlier in Section 6.8. For the abstract Hermitian adjoint defined on the inner-product Sec. 6.10 Hermitian, Unitary, and Normal Transformations space V, when translated into matrices in an orthonormal basis of V, becomes nothing more than the explicit, concrete Hermitian adjoint we defined there for matrices. Using the matrix representation in an orthonormal basis, we claim that T E A(V) is unitary if and only if, whenever (r:xJ is the matrix ofT in this orthonormal basis, then 0 for j =1- k while In terms of dot products on complex vector spaces, it says that the rows of the matrix of T form an orthonormal set of vectors in p<n> under the dot product. DEFINITION T E A(V) is called self-adjoint or Hermitian if T* T. If T* == - Twe call skew-Hermitian. Given any S E A(V), s + s * . (s - s *) s = + z ' 2 2i and since (S + S*)/2 and (S - S*)f2i are Hermitian, S = A + iB where both A and Bare Hermitian. In Section 6.8, using matrix calculations, we proved that any complex characteristic root of a Hermitian matrix is real; in light of Fact 1, this can be changed to read: Every characteristic root of a Hermitian matrix is real. We now re-prove this from the more uniform point of view of an inner- product space. THEOREM 6.1 0.3 If T E A ( V) is Hermitian, then all its characteristic roots are real. Proof. Let A be a characteristic root of T; thus there is a v =1- 0 in V such that vT = AV. We compute: A(v, v) = (Av, v) = (vT, v) = (v, vT*) = (v, vT) = (v, Av) = A(v, v); since (v, v) =1- 0 we are left with A = A hence A is real. \\\\ We want to describe canonical forms for unitary, Hermitian: and even ' more general types of linear transformations which will be even simpler .. than the Jordan form. This accounts for the next few lemmas which, although of independent interest, are for the most part somewhat technical in nature. 341 342 Linear Transformations Ch. 6 LEMMA 6.10.6 If S E A(V) and ifvSS* = 0, then vS = 0. Proof. Consider (vSS*, v); since vSS* = 0, 0 = (vSS*, v) = (vS, v(S*)*) = (vS, vS) by Lemma 6.10.4. In an inner-product space, this implies that vS = 0. COROLLARY If Tis Hermitian and vTk = Ofor k ;;::: 1 then vT = 0. Proof. We show that if vT 2m = 0 then vT = 0; for if S = T 2m-', then S* = S and SS* = T 2m, whence (vSS*, v) = 0 implies that 0 = vS = vT 2 m- 1 • Continuing down in this way, we obtain vT = 0. If vTk = 0, then vT 2 m = 0 for 2m > k, hence vT = 0. We introduce a class of linear transformations which contains, as special cases, the unitary, Hermitian and skew-Hermitian transformations. DEFINITION T E A(V) is said to be normal if TT* = T*T. Instead of proving the theorems to follow for unitary and Hermitian transformations separately, we shall, instead, prove them for normal linear transformations and derive, as corollaries, the desired results for the unitary and Hermitian ones. LEMMA 6.10.7 If N is a normal linear transformation and if vN = 0 for v E V, then vN* = 0. Proof. Consider (vN*, vN*); by definition, (vN*, vN*) = (vN* N, v) = (vNN*, v), since NN* = N* N. However, vN = 0, whence, certainly, vNN* = 0. In this way we obtain that (vN*, vN*) = 0, forcing vN* = 0. COROLLARY 1 If A is a characteristic root of the normal transformation N and if vN = A.v then vN* = Av. Proof. Since Nis normal, NN* = N* N, therefore, (N- A.)(N- A.)* = (N- A.)(N*- A:) = NN*- A.N* - A:N + A.A: = N*N- A.N*- XN + A.X = (N* - X)(N- A.) = (N- A.)*(N- A.), that is to say, N- A. is normal. Since v(N- A) = 0 by the normality of N- A, from the lemma, v(N - A.)* = 0, hence vN* = Xv. The corollary states the interesting fact that if). is a characteristic root of the normal transformation N not only is X a characteristic root of N* but any characteristic vector of N belo'nging to ). is a characteristic vector of N* belonging to X and vice versa. COROLLARY 2 If T is unitary and if A. is a characteristic root of T, then IA.I = I. Sec. 6.10 Hermitian, Unitary, and Normal Transformations 343 Proof. Since Tis unitary it is normal. Let A be a characteristic root of T and suppose that v T = AV with v # 0 in V. By Corollary 1, vT* = .A:v, thus v = vTT* = AvT* = AAV smce TT* = 1. Thus we get AA = 1, which, of course, says that IAI = 1. We pause to see where we are going. Our immediate goal is to prove that a normal transformation N can be brought to diagonal form by a unitary one. If A1, .•• , Ak are the distinct characteristic roots of V, using Theorem 6.6.1 we can decompose v as v = v1 EB ... EB vk, where for viE vi, vi(N - Ait; = 0. Accordingly, we want to study two things, namely, the relation of vectors lying in different V/s and the very nature of each Vi. When these have been determined, we will be able to assemble them to prove the desired theorem. LEMMA 6.1 0.8 lf N is normal and if vNk = 0, then vN = 0. Proof. Let S = NN*; S is Hermitian, and by the normality of N, vSk = v(NN*)k = vNk(N*)k = 0. By the corollary to Lemma 6.10.6, we deduce that vS = 0, that is to say, vNN* = 0. Invoking Lemma 6.10.6 itself yields vN = 0. COROLLARY lf N zs normal and if for A E F, v(N - A)k = 0, then vN = AV. Proof. From the normality of Nit follows that N - A is normal, whence by applying the lemma just proved to N- A we obtain the corollary. In line with the discussion just preceding the last lemma, this corollary shows that every vector in Vi is a characteristic vector of N belonging to the charac- teristic root Ai. We have determined the nature of Vi; now we proceed to investigate the interrelation between two distinct V/s. LEMMA 6.1 0.9 Let N be a normal transformation and suppose that A and J..t are two distinct characteristic roots of N. lf v, w are in V and are such that vN = AV, wN = f1W, then (v, w) = 0. Proof. We compute (vN, w) in two different ways. As a consequence of vN = AV, (vN, w) = (Av, w) = A(v, w). From wN = f1W, using Lemma 6.10. 7 we obtain that wN* = jiw, whence (vN, w) = (v, wN*) = (v, jiw) = p,(v, w). Comparing the two computations gives us A(v, w) = Jl(v, w) and ' since A # p,, this results in (v, w) = 0. ,,~ All the background work has been done to enable us to prove the basic and lovely 344 Linear Transformations Ch. 6 THEOREM 6.1 0.4 If N is a normal linear transformation on V, then there exists an orthonormal basis, consisting of characteristic vectors of N, in which the matrix of N is diagonal. Equivalently, if N is a normal matrix there exists a unitary matrix U such that UNU- 1 ( = UNU*) is diagonal. Proof. We fill in the informal sketch we have made of the proof prior to proving Lemma 6.10.8. Let N be normal and let ). 1 , .•. , Ak be the distinct characteristic roots of N. By the corollary to Theorem 6.6.1 we can decompose V = v1 EB ... EB vk where every viE vi is annihilated by (N- Ait1• By the corollary to Lemma 6.10.8, Vi consists only of characteristic vectors of N belonging to the characteristic root Ai. The inner product of V induces an inner product on Vi; by Theorem 4.4.2 we can find a basis of Vi orthonormal relative to this inner product. By Lemma 6.10.9 elements lying in distinct V/s are orthogonal. Thus putting together the orthonormal bases of the V/s provides us with an orthonormal basis of V. This basis consists of characteristic vectors of N, hence in this basis the matrix of N is diagonal. We do not prove the matrix equivalent, leaving it as a problem; we only point out that two facts are needed: 1. A change of basis from one orthonormal basis to another is accomplished by a unitary transformation (Theorem 6.1 0.1). 2. In a change of basis the matrix of a linear transformation is changed by conjugating by the matrix of the change of basis (Theorem 6.3.2). Both corollaries to follow are very special cases of Theorem 6.1 0.4, but since each is so important in its own right we list them as corollaries in order to emphasize them. COROLLARY 1 If T is a unitary transformation, then there is an orthonormal basis in which the matrix of T is diagonal; equivalently, if T is a unitary matrix, then there is a unitary matrix U such that uru- 1 ( = UTU*) is diagonal. COROLLARY 2 If Tis a Hermitian linear transformation, then there exists an orthonormal basis in which the matrix ofT is diagonal; equivalently, if Tis a Hermitian matrix, then there exists a unitary matrix U such that UT U- 1 ( = UT U*) zs diagonal. The theorem proved is the basic....result for normal transformations, for it sharply characterizes them as precisely those transformations which can be brought to diagonal form by unitary ones. It also shows that the distinc- tion between normal, Hermitian, and unitary transformations is merely a distinction caused by the nature of their characteristic roots. This is made precise in ~ I Sec. 6.10 Hermitian, Unitary, and Normal Transformations 345 LEMMA 6.1 0.1 0 The normal transformation N is I. Hermitian if and only if its characteristic roots are real. 2. Unitary if and only if its characteristic roots are all qf absolute value I. Proof. We argue using matrices. If N is Hermitian, then it is normal and all its characteristic roots are real. If N is normal and has only real charac- teristic roots, then for some unitary matrix U, UNU- 1 = UNU* = D, where D is a diagonal matrix with real entries on the diagonal. Thus D* = D; since D* = (UNU*)* = UN*U*, the relation D* = D implies UN* U* = UNU*, and since U is invertible we obtain N* = N. Thus N is Hermitian. We leave the proofofthe part about unitary transformations to the reader. If A is any linear transformation on V, then tr (AA*) can be computed by using the matrix representation of A in any basis of V. We pick an orthonormal basis of V; in this basis, if the matrix of A is (rxii) then that of A* is (flii) where flii = fiji· A simple computation then shows that tr (AA*) = Li,i lrxiil 2 and this is 0 if and only if each rxii = 0, that is, if and only if A = 0. In a word, tr (AA*) = 0 if and only if A = 0. This is a useful criterion for showing that a given linear transformation is 0. This is illustrated in LEMMA 6.10.11 If N is normal and AN = NA, then AN* = N* A. Proof. We want to show that X = AN* - N* A is 0; what we shall do is prove that tr XX* = 0, and deduce from this that X = 0. Since N commutes with A and with N*, it must commute with AN*..,.- N*A, thus XX*= (AN*- N*A)(NA*- A*N) =(AN*- N*A)NA*- (AN* - N*A)A*N = N{(AN* - N*A)A*} - {(AN* - N*A)A*}N. Being of the form NB - BN, the trace of XX* is 0. Thus X = 0, and AN* = N*A. We have just seen that N* commutes with all the linear transformations that commute with N, when N is normal; this is enough to force N* to be a polynomial expression in N. However, this can be shown directly as a consequence of Theorem 6.1 0.4 (see Problem 14). The linear transformation Tis Hermitian if and only if (vT, v) is real for every v E V. (See Problem 19.) Of special interest are those Hermitian linear transformations for which ( v T, v) ~ 0 for all v E V. We call these nonnegative linear transformations and denote the fact that a linear trays-, ~ formation is nonnegative by writing T ~ 0. If T ~ 0 and in addition (vT, v) > 0 for v =I= 0 then we caU T positive (or positive definite) and write T > 0. We wish to distinguish these linear transformations by their charac- teristic roots. 346 linear Transformations Ch. 6 LEMMA 6.10.12 The Hermitian linear transformation T is nonnegative (positive) if and only if all of its characteristic roots are nonnegative (positive). Proof. Suppose that T ~ 0; if A is a characteristic root of T, then vT = Av for some v =/= 0. Thus 0 ~ (vT, v) = (Av ,v) = A(v, v); since (v, v) > 0 we deduce that A ~ 0. Conversely, if Tis Hermitian with nonnegative characteristic roots, then we can find an orthonormal basis {v1 , ..• , vn} consisting of characteristic vectors of T. For each vi, vi T = Aivi, where Ai ~ 0. Given v E V, v = Lrtivi hence vT = LrtiviT = LAirtivi. But (vT, v) = (LAirtivi, Lrtivi) = LAirtiiXi by the orthonormality of the v/s. Since Ai ~ 0 and rtiiXi ~ 0, we get that (vT, v) ~ 0 hence T ~ 0. The corresponding \"positive\" results are left as an exercise. LEMMA 6.10.13 T ~ 0 if and only if T = AA* for some A. Proof. We first show that AA* ~ 0. Given v E V, (vAA*, v) = (vA, vA) ~ 0, hence AA* ~ 0. On the other hand, if T ~ 0 we can find a unitary matrix U such that UTU* = ( Al where each Ai is a characteristic root ofT, hence each Ai ~ 0. Let s =(b. since each Ai ~ 0, each .J).i is real, whence S is Hermitian. Therefore, U*SU is Hermiti_an; but We have represented Tin the form AA*, where A = U*SU. Notice that we have actually proved a little more; namely, if in construct- ing S above, we had chosen the nonnegative ,f\"i_i for each Ai, then S, and U*SU, would have been nonnegative. Thus T ~ 0 is the square of a non- negative linear transformation; that is, every T ~ 0 has a nonnegative square root. This nonnegative square root can be shown to be unique (see Problem 24). We close this section with a discussion ofunitary and Hermitian matrices over the real field. In this case, the unitary matrices are called orthogonal, and ~ I Sec. 6.10 Hermitian, Unitary, and Normal Transformations satisfy QQ' = 1. The Hermitian ones are just symmetric, in this case. We claim that a real .rymmetric matrix can be brought to diagonal form by an orthogonal matrix. Let A be a real symmetric matrix. We can consider A as acting on a real inner-product space V. Considered as a complex matrix, A is Hermitian and thus all its characteristic roots are real. If these are li, ... , 'Ak then V can be decomposed as V = V1 E8 · · · E8 Vk where vi(A - 'Ai)ni = 0 for viE vi. As in the proof of Lemma 6.10.8 this forces viA = 'Aivi. Using exactly the same proof as was used in Lemma 6.1 0.9, we show that for viE vi, vj E vj with i =f. J, (vi, vj) = 0. Thus we can find an orthonormal basis of V consisting of characteristic vectors of A. The change of basis, from the orthonormal basis { ( 1, 0, ... , 0), (0, 1, 0, ... , 0), ... , (0, ... , 0, 1)} to this new basis is accomplished by a real, unitary matrix, that is, by an orthogonal one. Thus A can be brought to diagonal form by an orthogonal matrix, proving our contention. To determine canonical forms for the real orthogonal matrices over the real field is a little more complicated, both in its answer and its execution. We proceed to this now; but first we make a general remark about all unitary transformations. If W is a subspace of V invariant under the unitary transformation T, is it true that W', the orthogonal complement of W, is also invariant under T? Let wE W and x E W'; thus (wT, xT) = (w, x) = 0; since W is invariant under T and Tis regular, WT = W, whence xT, for x E W', is orthogonal to all of W. Thus indeed ( W') T c W'. Recall that V = WEB W'. Let Q be a real orthogonal matrix; thus T = Q + Q -t = Q + Q' is symmetric, hence has real characteristic roots. If these are 'A1 , ..• , Ai, then v can be decomposed as v = v1 E8 ••• EB vk, where vi E v implies viT = 'Aivi. The V/s are mutually orthogonal. We claim each Vi is invariant under Q. (Prove!) Thus to discuss the action of Q on V, it is enough to describe it on each vi. On Vi, since 'Aivi = viT = vi(Q + Q - 1), multiplying by Q yields vi( Q 2 - 'AiQ + I) = 0. Two special cases present themselves, namely .· li = 2 and 'Ai = -2 (which may, of course, not occur), for then . vi( Q ± 1) 2 = 0 leading to vi( Q ± 1) = 0. On these spaces Q acts as 1 ,or as -1. If 'Ai =!= 2, -2, then Q has no characteristic vectors on Vi, hence for v =I= 0 E vi, v, vQ are linearly independent. The subspace they generate, W, is invariant under Q, since vQ 2 = 'AivQ - v. Now Vi = W E9 W' ~::with W' invariant under Q. Thus we can get Vi as a direct sum of two-. dimensional mutually orthogonal subspaces invariant under Q. To find icanonical forms of Q on Vi (hence on V), we must merely settle the question for 2 x 2 real orthogonal matrices. 347 348 linear Transformations Ch. 6 Let Q be a real 2 x 2 orthogonal matrix satisfying Q 2 - A,Q + 1 = 0; suppose that Q = (~ !} The orthogonality of Q implies 1 . ' 1. ' ay + {3b = 0; since Q 2 - A,Q + 1 = 0, the determinant of Q is 1, hence ab - f3y = 1. (1) (2) (3) (4) We claim that equations (1)-(4) imply that a= b, f3 = -y. Since a 2 + {32 = 1, lal ~ 1, whence we can write a = cos (J for some real angle fJ; in these terms f3 = sin fJ. Therefore, the matrix Q looks like ( cos (J -sin (J sin(})· cos (J All the spaces used in all our decompositions were mutually orthogonal, thus by picking orthogonal bases of each of these we obtain an orthonormal basis of V. In this basis the matrix of Q is as shown in Figure 6.1 0.1. D F.l ~ Fig~re 6.10.1 cos (Jr sin (Jr -sin (Jr cos (Jr Since we have gone from one orthonormal basis to another, and since this is accomplished by an orthogonal matrix, given a real orthogonal matrix Q we can find an orthogonal matrix T such that TQT- 1 ( = TQT*) is of the form just described. Sec. 6.10 Hermitian, Unitary, and Normal Transformations 349 Problems I. Determine which of the following matrices are unitary, Hermitian, normal. r , l} (~ 0 0 ~)· (a) I 0 (b) (~ ~). (c) 0 1 I 0 0 I 0 0 3 0 0 0 I 1 (d) (2 2 i). (e) .J2 .J2 1 I 0 .J2 .J2 2. For those matrices in Problem 1 which are normal, find their charac- teristic roots and bring them to diagonal form by a unitary matrix. 3. If T is unitary, just using the definition (vT, uT) = (v, u), prove that Tis nonsingular. 4. If Q is a real orthogonal matrix, prove that det Q = ± 1. 5. If Q is a real symmetric matrix satisfying Q k = I for k ;:::: I, prove that Q 2 = 1. 6. Complete the proof of Lemma 6.10.4 by showing that (S + T)* = S* + T* and (J.T)* = J..T*. 7. Prove the properties of* in Lemma 6.I 0.4 by making use of the explicit form of w = vT* given in the proof of Lemma 6.I0.3. .,.. 8. If T is skew-Hermitian, prove that all of its characteristic roots are pure imaginaries. 9. If T is a real, skew-symmetric n x n matrix, prove that if n is odd, then det T = 0. IO. By a direct matrix calculation, prove that a real, 2 x 2 symmetric matrix can be brought to diagonal form by an orthogonal one. II. Complete the proof outlined for the matrix-equivalent part of Theorem 6.10.4. I2. Prove that a normal transformation is unitary if and only if the charac- teristic roots are all of absolute value 1. I3. If N1 , ... , Nk is a finite number of commuting normal transformations, prove that there exists a unitary transformation T such that all of T NiT- 1 are diagonal. 350 Linear Transformations Ch. 6 14. If N is normal, prove that N* = p(N) for some polynomial p(x). 15. If N is normal and if AN = 0, prove that AN* = 0. 16. Prove that A is normal if and only if A commutes with AA*. 17. If N is normal prove that N = LAiEi where E/ = Ei, Ei* = Ei, and the .A/s are the characteristic roots of N. (This is called the spectral resolution of N.) 18. If N is a normal transformation on V and iff (x) and g(x) are two relatively prime polynomials with real coefficients, prove that if vf (N) = 0 and wg(N) = 0, for v, w in V, then (v, w) = 0. 19. Prove that a linear transformation T on V is Hermitian if and only if (vT, v) is real for all v E V. 20. Prove that T > 0 if and only if T is Hermitian and has all its charac- teristic roots positive. 21. If A ~ 0 and (vA, v) = 0, prove that vA = 0. 22. (a) If A ~ 0 and A 2 commutes with the Hermitian transformation B then A commutes with B. (b) Prove part (a) even if B is not Hermitian. 23. If A ~ 0 and B ~ 0 and AB = BA, prove that AB ~ 0. 24. Prove that if A ~ 0 then A has a unique nonnegative square root. 25. Let A = (aii) be a real, symmetric n x n matrix. Let (a) If A > 0, prove that As > 0 for s = 1, 2, ... , n. (b) If A > 0 prove that det As > 0 for s = 1, 2, ... , n. (c) Ifdet As> 0 for s = 1, 2, ... , n, prove that A > 0. (d) If A ~ 0 prove that As ~ 0 for s = 1, 2, ... , n. (e) If A ~ 0 prove that det As ~ 0 for s = 1, 2, ... , n. (f) Give an example of an A such that det As ~ 0 for all s = 1, 2, ... , n yet A is not nonnegative. 26. Prove that any complex matrix can be brought to triangular form by a unitary matrix. 6.11 Real Quadratic Forms We close the chapter with a brief discussion of quadratic forms over the field of real numbers. Let V be a real, inner-product space and suppose that A is a (real) sym- Sec. 6.11 Real Quadratic Forms metric linear transformation on V. The real-valued function Q (v) defined on V by Q (v) = (vA, v) is called the quadratic form associated with A. If we consider, as we may without loss of generality, that A is a real, n x n symmetric matrix (aii) acting on p<n> and that the inner product for (c51 , ••• , ()n) and (y1 , •.• , Yn) in p(n) is the real number c51 y1 + c52 y2 + · · · + bnYn, for an arbitrary vector v = (xv ... , xn) in p<n> a simple calcula- tion shows that Q (v) = (vA, v) = a11 x1 2 + · · · + annxn 2 + 2 L aijxixi. i<j On the other hand, given any quadratic function in n-variables with real coefficients 'Yii' we clearly can realize it as the quadratic form associated with the real symmetric matrix C = (Yii). In real n-dimensional Euclidean space such quadratic functions serve to define the quadratic surfaces. For instance, in the real plane, the form ax 2 + pxy + yy 2 gives rise to a conic section (possibly with its major axis tilted). It is not too unnatural to expect that the geometric properties of this conic section should be intimately related with the symmetric matrix P/2) y ' with which its quadratic form is associated. Let us recall that in elementary analytic geometry one proves that by a suitable rotation of axes the equation ax 2 + pxy + yy 2 can, in the ii.\"ew coordinate system, assume the form a1 (x') 2 + y1 (y') 2 • Recall that rx1 + y1 = a + y and ay - P 2 /4 = a1 y1 • Thus a1 , y1 are the charac- teristic roots of the matrix P/2). ')' ' the rotation of axes is just a change of basis by an orthogonal transformation, and what we did in the geometry was merely to bring the symmetric matrix to its diagonal form by an orthogonal matrix. The nature of ax 2 + pxy + yy 2 as a conic was basically determined by the size and sign of its charac- teristic roots a1 , y1 • A similar discussion can be carried out to classify quadric surfaces in 3-space, and, indeed quadric surfaces in n-space. What essentially deter- mines the geometric nature of the quadric surface associated with rxuxt 2 + ... + annxn 2 + 2 L aijxixj i<j 351 352 linear Transformations Ch. 6 is the size and sign of the characteristic roots of the matrix (r:t.ii). If we were not interested in the relative flatness of the quadric surface (e.g., if we consider an ellipse as a flattened circle), then we could ignore the size of the nonzero characteristic roots and the determining factor for the shape of the quadric surface would be the number of 0 characteristic roots and the num- ber of positive (and negative) ones. These things motivate, and at the same time will be clarified in, the discussion that follows, which culminates in Sylvester's law cif inertia. Let A be a real symmetric matrix and let us consider its associated quadratic form Q (v) = (vA, v). If Tis any nonsingular real linear trans- formation, given v E p<n>, v = wT for some wE p<n>, whence (vA, v) = (wTA, wT) = (wTAT', w). Thus A and TAT' effectively define the same quadratic form. This prompts the DEFINITION Two real symmetric matrices A and B are congruent if there is a nonsingular real matrix T such that B = TAT'. LEMMA 6.11.1 Congruence is an equivalence relation. Proof. Let us write, when A is congruent to B, A ~ B. I. A~ A for A = IAI'. 2. If A ~ B then B = TAT' where T is nonsingular, hence A = SBS' where S = T- 1 . Thus B ~A. 3. If A~ B and B ~ C then B = TAT' while C = RBR', hence C = RTAT'R' = (RT)A(RT)', and so A ~ C. Since the relation satisfies the defining conditions for an equivalence relation, the lemma is proved. The principal theorem concerning congruence IS its characterization, contained in Sylvester's law. THEOREM 6.11.1 Given the real symmetric matrix A there zs an invertible matrix T such that TAT' where Ir and 18 are respectively the r x r and s x s unit matrices and where Ot is the t x t zero-matrix. The integers r i- s, which is the rank cif A, and r - s, which is the signature cif A, characterize the congruence class cif A. That is, two real symmetric matrices are congruent if and only if they have the same rank and signature. Proof. Since A is real symmetric its characteristic roots are all real; let .A1 , ... , Ar be its positive characteristic roots, - Ar+t' ... , - Ar+s its Sec. 6.11 Real Quadratic Forms negative ones. By the discussion at the end of Section 6.10 we can find a real orthogonal matrix G such that A., GAG- 1 = GAG' = where t = n - r - s. Let D be the real diagonal matrix shown in Figure 6.11.1. D= 1 .J A., Figure 6.11.1 A simple computation shows that DGAG'D' = ( I, I, Thus there is a matrix of the required form in the congruence class of A. Our task is now to show that this is the only matrix in the congruence class of A of this form, or, equivalently, that ( I, L= ( I,, ) and M = -Is' o,, are congruent only if r = r', s = s', and t = t'. Suppose that M = TLT' where T is invertible. By Lemma 6.1.3 the r~nk of M equals that of L; since the rank of M is n - t' while that of L is n - t we get t = t'. Suppose that r < r'; since n = r + s + t = r' + s' + t', and since t = t', we must have s > s'. Let U be the subspace of p<n> of all vectors 353 354 Linear Transformations Ch. 6 having the first r and last t coordinates 0; U is s-dimensional and for u =I= 0 in U, (uL, u) < 0. Let W be the subspace of p<n> for which the r' + 1, ... , r' + s' com- ponents are all 0; on W, (wM, w) ~ 0 for any wE W. Since Tis invertible, and since W is (n - s')-dimensional, WT is (n - s')-dimensional. For wE W, (wM, w) ~ 0; hence (wTLT', w) ~ 0; that is, (wTL, wT) ~ 0. Therefore, on WT, (wTL, wT) ~ 0 for all elements. Now dim (WT) + dim U = (n - s') + r = n + s - s' > n; thus by the corollary to Lemma 4.2.6, WT n U =1= 0. This, however, is nonsense, for if x =I= 0 E WT n U, on one hand, being in U, (xL, x) < 0, while on the other, being in WT, (xL, x) ~ 0. Thus r = r' and so s = s'. The rank, r + s, and signature, r - s, of course, determine r, s and so t = (n - r - s), whence they determine the congruence class. Problems 1. Determine the rank and signature of the following real quadratic forms: (a) x12 + 2x1x2 + x22· (b) x12 + x 1x 2 + 2x1x3 + 2x22 + 4x2x3 + 2x/. 2. If A is a symmetric matrix with complex entries, prove we can find a complex invertible matrix B such that BAB' = e· o) and that r, the rank of A, determines the congruence class of A relative to complex congruence. 3. IfF is a field of characteristic different from 2, given A E F\"' prove that there exists a BE Fn such that BAB' is diagonal. 4. Prove the result of Problem 3 is false if the characteristic ofF is 2. 5. How many congruence classes are there of n x n real symmetric matrices. Supplementary Reading liALMOS, PAuL R., Finite-Dimensional Vector Spaces, 2nd ed. Princeton, N.J.: D. Van Nostrand Company, 1958. 7 Selected Topics In this final chapter we have set ourselves two objectives. Our first is to present some mathematical results which cut deeper than most of the material up to now, results which are more sophisticated, and are a little apart from the general development which we have followed. Our second goal is to pick results of this kind whose discussion, in addition, makes vital use of a large cross section of the ideas and theorems expounded earlier in the book. To this end we have decided on three items to serve as the focal points of this chapter. The first of these is a celebrated theorem proved by Wedderburr+.in 1905 (\"A Theorem on Finite Algebras,\" Transactions qf the American Mathematical Society, Vol. 6 (1905), pages 349-352) which asserts that a division ring which has only a finite number of elements must be a commutative field. We shall give two proofs ofthis theorem, differing totally from each other. The first one will closely follow Wedderburn's original proof and will use a counting argument; it will lean heavily on results we developed in the chapter on group theory. The second one will use a mixture of group-theoretic and field-theoretic arguments, and will draw incisively on the material we developed in both these directions. The second proof has the distinct advantage that in the course of executing the proof certain side-results will fall out which will enable us to proceed to the proof, in the division ring case, of a beautiful theorem due to Jacobson (\"Structure Theory for Algebraic Algebras of Bounded Degree,\" Annals qf Mathematics, Vol. 46 (1945), pages 695-707) which is a far-reaching generalization ofWedderburn's theorem. 355 356 Selected Topics Ch. 7 Our second high spot is a theorem due to Frobenius (\"Uber lineare Substitutionen und bilineare Formen,\" Journal Jiir die Reine und Angewandte Mathematik, Vol. 84 (1877), especially pages 59-63) which states that the only division rings algebraic over the field of all real numbers are the field of real numbers, the field of complex numbers, and the division ring of real quaternions. The theorem points out a unique role for the quaternions, and makes it somewhat amazing that Hamilton should have discovered them in his somewhat ad hoc manner. Our proof of the Frobenius theorem, now quite elementary, is a variation of an approach laid out by Dickson and Albert; it will involve the theory of polynomials and fields. Our third goal is the theorem that every positive integer can be represented as the sum of four squares. This famous result apparently was first con- jectured by the early Greek mathematician Diophantos. Fermat grappled unsuccessfully with it and sadly announced his failure to solve it (in a paper where he did, however, solve the two-square theorem which we proved in Section 3.8). Euler made substantial inroads on the problem; basing his work on that of Euler, Lagrange in 1770 finally gave the first complete proof. Our approach will be entirely different from that of Lagrange. It is rooted in the work of Adolf Hurwitz and will involve a generalization of Euclidean rings. Using our ring-theoretic techniques on a certain ring of quaternions, the Lagrange theorem will drop out as a consequence. En route to establishing these theorems many ideas and results, interesting in their own right, will crop up. This is characteristic of a good theorem- its proof invariably leads to side results of almost equal interest. 7.1 Finite Fields Before we can enter into a discussion of Wedderburn's theorem and finite division rings, it is essential that we investigate the nature of fields having only a finite number of elements. Such fields are called finite fields. Finite fields do exist, for the ring ]p of integers modulo any prime p, provides us with an example of such. In this section we shall determine all possible finite fields and many of the important properties which they possess. We begin with LEMMA 7.1 .1 Let F be a finite field with q elements and suppose that F c K where K is also a finite field. Then K has qn elements where n = [ K :F]. ' Proof. K is a vector space over F and since K is finite it is certainly finite- dimensional as a vector space over F. Suppose that [K :F] = n; then K has a basis of n elements over F. Let such a basis be v1 , v2 , . .• , vn- Then every element in K has a unique representation in the form cx1 v1 + a2 v2 + · · · + anvn where cx1 , cx2 , ... , an are all in F. Thus the number of Sec. 7.1 Finite Fields elements in K is the number of a 1v1 + a2 v2 + · · · + anvn as the a 1 , a 2 , ••• , an range over F. Since each coefficient can have q values K must clearly have qn elements. COROLLARY 1 Let F be afinitefield; then F has pm elements where the prime number p is the characteristic cif F. Proof. Since F has a finite number of elements, by Corollary 2 to Theorem 2.4.1, f 1 = 0 where f is the number of elements in F. Thus F has characteristic p for some prime number p. Therefore F contains a field F0 isomorphic to ]p· Since F0 hasp elements, F has pm elements where m = [F:F0 ], by Lemma 7.1.1. COROLLARY 2 If the finite field F has pm elements then every a E F satisfies aPm = a. Proof. If a = 0 the assertion of the corollary is trivially true. On the other hand, the nonzero elements ofF form a group under multi- plication of order pm - 1 thus by Corollary 2 to Theorem 2.4.1, apm-l = 1 for all a # 0 in F. Multiplying this relation by a we obtain that aPm = a. From this last corollary we can easily pass to LEMMA 7 .1.2 If the finite field F has pm elements then the polynomial xPm - x in F[x] factors in F[x] as xPm - x = IT.teF (x - A.). Proof. By Lemma 5.3.2 the polynomial xPm - x has at most pPm roots in F. However, by Corollary 2 to Lemma 7 .1.1 we know pm such roots, namely all the elements of F. By the corollary to Lemma 5.3.1 we\"'\"\"can conclude that xPm - x = IT.teF (x - A.). COROLLARY If the field F has pm elements then F is the splitting field cif the polynomial xPm - x. Proof By Lemma 7.1.2, xPm - x certainly splits in F. However, it cannot split in any smaller field for that field would have to have all the roots of this polynomial and so would have to have at least pm elements. Thus F is the splitting field of xPm - x. As we have seen in Chapter 5 (Theorem 5.3.4) any two splitting fields over a given field of a given polynomial are isomorphic. In light of the corollary to Lemma 7 .1.2 we can state LEMMA 7.1.3 Any two finite fields having the same number cif elements are isomorphic. 357 58 Selected Topics Ch. 7 Proof. If these fields have pm elements, by the above corollary they are both splitting fields of the polynomial xPm - x, over ]p whence they are isomorphic. Thus for any integer m and any prime number p there is, up to iso- morphism, at most one field having pm elements. The purpose of the next lemma is to demonstrate that for any prime number p and any integer m there is a field having pm elements. When this is done we shall know that there is exactly one field having pm elements where p is an arbitrary prime and man arbitrary integer. LEMMA 7.1.4 For every prime number p and every positive integer m there exists a field having pm elements. Proof. Consider the polynomial xPm - x in ]p[x], the ring of polynomials in x over ]p, the field of integers mod p. Let K be the splitting field of this polynomial. In K let F = {a E K I aPm = a}. The elements ofF are thus the roots of xPm - x, which by Corollary 2 to Lemma 5.5.2 are distinct; whence F has pm elements. We now claim that F is a field. If a, bE F then aPm = a, bPm = b and so (ab)Pm = aPmbpm = ab; thus abE F. Also since the characteristic is p, (a ± b)Pm = aPm ± bPm = a ± b, hence a ± b E F. Consequently F is a subfield of K and so is a field. Having exhibited the field F having pm elements we have proved Lemma 7.1.4. Combining Lemmas 7.1.3 and 7.1.4 we have T H E 0 REM 7.1 .1 For every prime number p and every positive integer m there is a unique field having pm elements. We now return to group theory for a moment. The group-theoretic result we seek will determine the structure of any finite multiplicative subgroup of the group of nonzero elements of any field, and, in particular, it will determine the multiplicative structure of any finite field. LEMMA 7.1.5 Let G be a finite abelian group enJoying the property that the relation xn = e is satisfied by at most n elements of G, for every integer n. Then G is a cyclic group. Proof. If the order of G is a power of some prime number q then the result is very easy. For suppose that aEGis an element whose order is as large as possible; its order must be qr for some integer r. The elements e, a, a2 , •.• , aqr- 1 give us { distinct solutions of the equation xqr = e, which, by our hypothesis, implies that these are all the solutions of this equation. Now if bEG its order is t where s :::; r, hence bq\" = (bqs)qr-s = e. Sec. 7.1 Finite Fields By the observation made above this forces b = ai for some i, and so G is cyclic. The general finite abelian group G can be realized as G = Sq 1Sq2 • •• , Sqk where the q; are the distinct prime divisors of o(G) and where the Sq; are the Sylow subgroups of G. Moreover, every element g e G can be written in a unique way as g = StSz, ... 'sk where si E sq; (see Section 2. 7). Any solution of xn = e in Sq; is one of xn = e in G so that each Sq; inherits the hypothesis we have imposed on G. By the remarks of the first paragraph of the proof, each Sq; is a cyclic group; let ai be a generator of Sq;· We claim that c = a 1 a2 , ••• , ak is a cyclic generator of G. To verify this all we must do is prove that o(G) divides m, the order of c. Since em = e, we have that a 1 ma2 m · · · ak m = e. By the uniqueness of representation of an element of G as a product of elements in the Sq;' we conclude that each at = e. Thus o(SqJ I m for every i. Thus o(G) = o(SqJo(Sq 2 ) • • • o(Sqk) I m. However, m I o(G) and so o(G) = m. This proves that G is cyclic. Lemma 7.1.5 has as an important consequence LEMMA 7.1.6 Let K be afield and let G be afinite subgroup of the multiplicative group of nonzero elements of K. Then G is a cyclic group. Proof Since K is a field, any polynomial of degree n in K[ x] has at most n roots in K. Thus in particular, for any integer n, the polynomial xn - 1 has at most n roots in K, and all the more so, at most n roots in G. The hypothesis of Lemma 7.1.5 is satisfied, so G is cyclic. Even though the situation of a finite field is merely a special case of Lemma 7.1.6, it is of such widespread interest that we single it out as .... THEOREM 7.1.2 The multiplicative group of nonzero elements of a finite field is cyclic. Proof. Let F be a finite field. By merely applying Lemma 7.1.6 with F = K and G = the group of nonzero elements ofF, the result drops out. We conclude this section by using a counting argument to prove the existence of solutions of certain equations in a finite field. We shall need the result in one proof of the Wedderburn theorem. LEMMA 7.1.7 IfF is a finite field and a =I= 0, {3 =I= 0 are two elements ofF then we can find elements a and b in F such that 1 + aa 2 + {3b 2 = 0. Proof. If the characteristic of F is 2, F has 2n elements and every element x in F satisfies x 2 n = x. Thus every element in F is a square. In particular cx- 1 = a2 for some a e F. Using this a and b = 0, we have 359 360 Selected Topics Ch. 7 1 + aa 2 + f3b 2 = 1 + ar:J.- 1 + 0 = 1 + 1 = 0, the last equality being a consequence of the fact that the characteristic ofF is 2. If the characteristic of F is an odd prime p, F has pn elements. Let Wcx = {1 + ax 2 I x E F}. How many elements are there in Wa? We must check how often 1 + ax 2 = 1 + t:~.y 2 • But this relation forces ax 2 = ay 2 and so, since a =P 0, x 2 = y 2 . Finally this leads to x = ±y. Thus for x =P 0 we get from each pair x and - x one element in Wcx, and for x = 0 we get 1 E Wcx. Thus Wcx has 1 + (pn - 1) /2 = (pn + 1) /2 elements. Similarly Wp = {- f3x 2 I x E F} has (pn + 1) /2 elements. Since each of Wcx and Wp has more than half the elements of F they must have a non- empty intersection. Let c E Wcx n Wp. Since c E Wcx, c = 1 + aa 2 for some a E F; since c E Wp, c = - f3b 2 for some bE F. Therefore 1 + aa 2 = - f3b 2 , which, on transposing yields the desired result 1 + aa 2 + f3b 2 = 0. Problems 1. By Theorem 7.1.2 the nonzero elements of ]p form a cyclic group under multiplication. Any generator of this group is called a primitive root of p. (a) Find primitive roots of: 17, 23, 31. (b) How many primitive roots does a prime p have? 2. Using Theorem 7.1.2 prove that x 2 = -1 modp is solvable if and only if the odd prime p is of the form 4n + 1. 3. If a is an integer not divisible by the odd prime p, prove that x 2 = a mod p is solvable for some integer x if and only if a<P- 1>12 = 1 mod p. (This is called the Euler criterion that a be a quadratic residue mod p.) 4. Using the result of Problem 3 determine if: (a) 3 is a square mod 17. (b) 10 is a square mod 13. 5. If the field F has pn elements prove that the automorphisms ofF form a cyclic group of order n. 6. IfF is a finite field, by the quaternions over F we shall mean the set of all a0 + a 1 i + a2 j + a 3k where a0 , a 1 , a 2 , a 3 E F and where addition and multiplication are carried out as in the real quaternions (i.e., i2 = j 2 = k 2 = ijk = -1, etc.). Prove that the quaternions over a finite field do not form a division ring. 7.2 Wedderburn's Theorem ort Finite Division Rings In 1905 Wedderburn proved the theorem, now considered a classic, that a finite division ring must be a commutative field. This result has caught the imagination of most mathematicians because it is so unexpected, interrelating two seemingly unrelated things, namely the number of elements in a certain Sec. 7.2 Wedderburn's Theorem on Finite Division Rings 361 algebraic system and the multiplication of that system. Aside from its intrinsic beauty the result has been very important and useful since it arises in so many contexts. To cite just one instance, the only known proof of the purely geometric fact that in a finite geometry the Desargues configuration implies that of Pappus (for the definition of these terms look in any good book on projective geometry) is to reduce the geometric problem to an algebraic one, and this algebraic question is then answered by invoking the Wedderburn theorem. For algebraists the Wedderburn theorem has served as a jumping-off point for a large area of research, in the 1940s and 1950s, concerned with the commutativity of rings. THEOREM 7.2.1 (WEDDERBURN) A finite division rzng zs necessarily a commutative field. First Proof. Let K be a finite division ring and let Z = {z E K I zx = xz for all x E K} be its center. If Z has q elements then, as in the proof of Lemma 7.1.1, it follows that K has qn elements. Our aim is to prove that Z = K, or, equivalently, that n = 1. If a E K let N(a) = {x E K I xa =ax}. N(a) clearly contains Z, and, as a simple check reveals, N(a) is a subdivision ring of K. Thus N(a) contains ~(a) elements for some integer n(a). We claim that n(a) In. For, the nonzero elements of N(a) form a subgroup of order qn(a) - 1 of the group of nonzero elements, under multiplication, of K which has qn - 1 elements. By Lagrange's theorem (Theorem 2.4.1) qn<a> - 1 is a divisor ·of~ - 1; but this forces n(a) to be a divisor of n (see Problem 1 at the end of this section). In the group of nonzero elements of K we have the conjugacy rel~~ion used in Chapter 2, namely a is a conjugate of b if a = x- 1bx for some x =f. 0 inK. By Theorem 2.11.1 the number of elements in K conjugate to a is the index of the normalizer of a in the group of nonzero elements of K. Therefore the number of conjugates of a in K is (qn - 1)/(qn(a) - 1). Now a E Z if and only if n(a) = n, thus by the class equation (see the corollary to Theorem 2.11.1) qn - 1 q-1+\"\"' L.J n(a) 1 n(a) In q - n(a)*n (1) where the sum is carried out over one a in each conjugate class for a's not in the c~nter. The problem has been reduced to proving that no equation such as (I) can hold in the integers. Up to this point we have followed the proof in Wedderburn's original paper quite closely. He went on to rule out the possibility of equation ( 1) by making use of the following number-theoretic 362 Selected Topics Ch. 7 result due to Birkhoff and Vandiver: for n > 1 there exists a prime number which is a divisor of qn - 1 but is not a divisor of any qm - 1 where m is a proper divisor of n, with the exceptions of 26 - 1 = 63 whose prime factors already occur as divisors of 2 2 - 1 and 2 3 - 1, and n = 2, and q a prime of the form 2k - 1. If we grant this result, how would we finish the proof? This prime number would be a divisor of the left-hand side of ( 1) and also a divisor of each term in the sum occurring on the right-hand side since it divides t - 1 but not f(a) - I ; thus this prime would then divide q - 1 giving us a contradiction. The case 2 6 - 1 still would need ruling out but that is simple. In case n = 2, the other possibility not covered by the above argument, there can be no subfield between Z and K and this forces Z = K. (Prove!-See Problem 2.) However, we do not want to invoke the result of Birkhoff and Vandiver without proving it, and its proof would be too large a digression here. So we look for another artifice. Our aim is to find an integer which divides (qn - I)J(qn(a) - I), for all divisors n(a) of n except n(a) = n, but does not divide q - I. Once this is done, equation ( 1) will be impossible unless n = I and, therefore, Wedderburn's theorem will have been proved. The means to this end is the theory of cyclotomic polynomials. (These have been mentioned in the problems at the end of Section 5.6.) Consider the polynomial xn - 1 considered as an element of C [ x] where Cis the field of complex numbers. In C[x] xn - I = II (x - A), (2) where this product is taken over all A satisfying An = 1. A complex number e is said to be a primitive nth root of unity if en = but em # I for any positive integer m < n. The complex numbers satis- fying xn = I form a finite subgroup, under multiplication, of the complex numbers, so by Theorem 7 .I.2 this group is cyclic. Any cyclic generator of this group must then be a primitive nth root of unity, so we know that such primitive roots exist. (Alternatively, e = e2 1ti/n yields us a primitive nth root of unity.) Let <I>n(x) = II (x - e) where this product is taken over all the primitive nth roots of unity. This polynomial is called a cyclotomic polynomial. We list the first few cyclotomic polynomials: <1>1 (x) = x - 1, <1>2 (x) = x + 1, <I>3 (x) = x 2 + x + I, <I>4 (x) = x 2 + I, <I>5 (x) = x 4 + x 3 + x 2 + x + 1, <I>6 (x) = x 2 - x + 1. Notice that these are all monic polynomials with integer coefficients. Our first aim is to prove that in general <I>n(x) is a monic polynomial with integer coefficients. We regroup the factored form of xn - I as given in (2), and obtain xn - I = II <I>ix). (3) din Sec. 7.2 Wedderburn's Theorem on Finite Division Rings 363 By induction we assume that <I>d(x) is a monic polynomial with integer coefficients for d I n, d =I= n. Thus xn - 1 = <I>n(x) g(x) where g(x) is a monic polynomial with integer coefficients. Therefore, which, on actual division (or by comparing coefficients), tells us that <I>n(x) is a monic polynomial with integer coefficients. We now claim that for any divisor d of n, where d =1- n, in the sense that the quotient is a polynomial with integer coefficients. To see this, first note that xd - 1 = rr <I>k(x), kid and since every divisor of d is also a divisor of n, by regouping terms on the right-hand side of (3) we obtain xd - 1 on the right-hand side; also since d < n, xd - 1 does not involve <I>n(x). Therefore, xn - 1 = <I>n(x) (xd - 1) f (x) where f (x) rr <I>k(x) has integer coefficients, and so kin k)'d in the sense that the quotient is a polynomial with integer coefficients. This establishes our claim. For any integer t, <I>n(t) is an integer and from the above as an integer divides (t n - 1 ) / ( td - 1 ) . In particular, returning to equation ( 1 ) , I qn - 1 <I>n(q) qn(a) - 1 and <I>n(q) I (t - 1); thus by (1), <I>n(q) I (q - 1). We claim, however, that if n > 1 then I<I>n(q) I > q - 1. For <I>n(q) = rr (q - ()) where () runs over all primitive nth roots of unity and lq - 01 > q - 1 for all 0 =I= 1 a root of unity (Prove!) whence I<I>n(q) I = fllq - 01 > q - 1. Clearly, ' then <I>n(q) cannot divide q - 1, leading us to a contradiction. We must, therefore, assume that n = 1, forcing the truth of the Wedderburn theorem. Second Proof. Before explicitly examining finite division rings again, we prove some preliminary lemmas. 364 Selected Topics Ch. 7 LEMMA 7 .2.1 Let R be a ring and let a E R. Let Ta be the mapping of R into itself defined by xTa = xa - ax. Then m(m - 1) xTa m = xam - maxam- 1 + a2xam- 2 2 m(m- 1)(m- 2) a3xam-3 + .... 3! Proof. What is xTa 2 ? xTa 2 = (xTa) Ta = (xa -ax) Ta = (xa -ax) a - a(xa - ax) = xa 2 - 2axa + a2x. What about xTa 3 ? xTa 3 = (xTa 2 ) Ta = (xa 2 - 2axa + a2x)a - a(xa 2 - 2axa + a2 x) = xa 3 - 3axa 2 + 3a 2xa - a3x. Continuing in this way, or by the use of induction, we get the result of Lemma 7.2.1. COROLLARY If R is a ring in which px = Ofor all x E R, where pis a prime number, then xT/m = xaPm - aPmx. Proof. By the formula of Lemma 7.2.1, if p = 2, xTa 2 = xa 2 - a2x, since 2axa = 0. Thus, xTa 4 = (xa 2 - a 2x)a 2 - a 2 (xa 2 - a 2 x) = xa 4 - a4 x, and so on for xTa 2 m. Ifp is an odd prime, again by the formula ofLemma 7.2.1, T p p p 1 p(p - 1) 2 p- 2 p x a = xa - paxa - + 2 a xa + · · · - ax, and since l p(p - 1) ... (p - i + 1) p ., z. for i < p, all the middle terms drop out and we are left with x T/ xaP - aPx = xTaP· Now xT/ 2 = x( TaP)P = xTaP2, and so on for the higher powers of p. LEMMA 7.2.2 Let D be a division ring of characteristic p > 0 with center Z, and let P = {0, 1, 2, ... , (p - 1)} be the subfield of Z isomorphic to ]p· Suppose that a E D, a ~ Z is such that aP\" = a for some n ~ 1. Then there exists an x E D such that 1. xax- 1 ::j:. a. 2. xax- 1 E P (a) the field obtained by f-djoining a to P. Proof. Define the mapping Ta of D into itself by yTa = ya - ay for every y ED. P (a) is a finite field, since a is algebraic over P and has, say, pm elements. These all satisfy uPm = u. By the corollary to Lemma 7.2.1, yT/m = yaPm - aPmy = ya - ay = yTa, and so TaPm = Ta. Sec. 7.2 Wedderburn's Theorem on Finite Division Rings 365 Now, if A E P(a), (Ax)Ta = (Ax)a - a(Ax) = AXa - Aax = A(xa - ax) = A(xTa), since A commutes with a. Thus the mapping AI of D into itself defined by Al:y--+ 1f' commutes with Ta for every A E P(a). Now the polynomial uPm - u = IT ( u - A) ..1.EP(a) by Lemma 7.2.1. Since Ta commutes with AI for every A E P(a), and smce T/m = Ta, we have that 0 = T/m - Ta = IT (Ta - Af) . ..1.EP(a} If for every A =1- 0 in P(a), Ta - AI annihilates no nonzero element in D (ify(Ta - AI) = 0 impliesy = 0), since Ta(Ta - A11) · · · (Ta - AJ) = 0, where A1 , ... , Ak are the nonzero elements of P(a), we would get Ta = 0. That is, 0 = yTa = ya - ay for every y ED forcing a E Z con- trary to hypothesis. Thus there is a A =1- 0 in P (a) and an x =1- 0 in D such that x(Ta - AI) = 0. Writing this out explicitly, xa - ax - AX = 0; hence, xax- 1 = a + A is in P(a) and is not equal to a since A =1- 0. This proves the lemma. COROLLARY In Lemma 7.2.2, xax- 1 ai =1- a for some integer i. Proof. Let a be of order s; then in the field P (a) all the roots of the polynomial us - 1 are 1, a, a 2 , ••• , as-l since these are all distinct roots and they are s in number. Since (xax- 1)s = xasx- 1 = 1, and since xax- 1 E P(a), xax- 1 is a root in P(a) of us - 1, hence xax- 1 = ai. We now have all the pieces that we need to carry out our second proof of Wedderburn's theorem. Let D be a finite division ring and let Z be its center. By induction we may assume that any division ring having fewer elements than D is a commutative field. We first remark that if a, bED are such that bta = abt but ba =1- ab, then bt E Z. For, consider N(bt) = {xED I btx = xbt}. N(bt) is a sub- division ring of D; if it were not D, by our induction hypothesis, it would be commutative. However, both a and b are in N ( b 1 ) and these do not commute; consequently, N(b1) is not commutative so must be all of D. Thus bt E Z. Every nonzero element in D has finite order, so some positive power of it falls in Z. Given w E D let the order if w relative to Z be the smallest positive integer m(w) such that wm(w) E Z. Pick an element a in D but not in Z having minimal possible order relative to Z, and let this order be r. We claim that r is a prime number, for if r = r1 r2 with 1 < r1 < r then ar 1 is not in Z. Yet ( ar 1 yz = ar E Z, implying that ar1 has an order relative to Z smaller than that of a. 366 Selected Topics Ch. 7 By the corollary to Lemma 7.2.2 there is an xED such that xax- 1 = ai =I= a; thus x 2ax- 2 = x(xax- 1 )x- 1 = xaix- 1 = (xax- 1 )i = (ai)i = ai 2 • Similarly, we get x'- 1ax-<r- 1 ) = air- 1 • However, r is a prime number, thus by the 1i ttle Fermat theorem (corollary to Theorem 2.4 .1), i'- 1 = 1 + Uor, hence air- 1 = a 1 +uor = aauor = Aa where A = auor E z. Thus x'- 1a = ).ax'- 1 . Since x ¢ Z, by the minimal nature of r, x'- 1 cannot be in Z. By the remark of the earlier paragraph, since xa =1= ax, x'- 1 a =I= ax- 1 and so A =I= 1. Let b = x'- 1 ; thus bab- 1 = Aa; consequently, ).'a' = (bah- 1 r = ba' b- 1 = a' since a' E Z. This relation forces ;_r = 1. We claim that if y ED then whenever y' = 1, then y = ;_i for some i, for in the field Z (y) there are at most r roots of the polynomial u' - 1; the elements 1, A, A 2 , .•• , A'- 1 in Z are all distinct since A is of the prime order r and they already account for r roots of u' - 1 in Z (y), in con- sequence ofwhichy = Ai. Since X = 1, b' = A'h' = (Ah)' = (a- 1ba)' = a- 1b'a from which we get ab' = b' a. Since a commutes with b' but does not commute with b, by the remark made earlier, b' must be in Z. By Theorem 7.1.2 the multi- plicative group of nonzero elements of Z is cyclic; let y E Z by a generator. Thus a' = yi, b' = yk; if J = sr then a' = ys', whence (afysy = 1; this would imply that afi = Ai, leading to a E Z, contrary to a¢ Z. Hence, r -t J; similarly r -t k. Let a1 = ak and b1 = hi; a direct computation from ba = Aah leads to a1 b1 = J1h1 a1 where J1 = A- ik E Z. Since the prime number r which is the order of A does not divide J or k, Aik =1= 1 hence J1 =I= 1. Note that 11' = 1. Let us see where we are. We have produced two elements a1 , b1 such that 1. a1' = h1' = a E Z. 2. a 1 h1 = J1h1 a 1 with J1 =I= 1 in Z. 3. Jl' = 1. We compute (a 1 - 1h1)'; (a 1 - 1b1 ) 2 = a1 - 1b 1a1 - 1b 1 = a1 - 1 (b 1a 1 - 1 )h1 a 1 - 1 (J1a1 - 1h1)h1 = J1a1 - 2 h1 2 • Ifwe compute (a 1 - 1b1 ) 3 we find it equal to J1 1 + 2a1 - 3 h1 3 . Continuing, we obtain (a 1 - 1 b 1)' = J1 1 + 2 + · · · +(r- 1>a 1 -rb 1' = 111 + 2 +···+(r- 1 ) = Jlr(r- 1 >1 2 • If r is an odd prime, since 11' = 1, we get Jlr(r- 1> 12 = 1, whence (a 1 - 1h1)' = 1. Being a solution of y' = 1, a1 - 1h1 = Ai so that b1 = Aia1 ; but then J1h1a1 = a1h1 = h1a1, contra- dicting J1 =I= 1. Thus if r is an odd prime number, the theorem is proved. We must now rule out the case r = 2. In that special situation we have two elements a 1 , h1 ED such that -a1 2 = b1 2 = a E Z, a1 b1 = J1h1 a 1 where 112 = 1 and J1 =I= 1. Thus J1 = -1 and a 1b1 = -b 1a 1 =I= b1a1 ; in conse- quence, the characteristic of Dis not 2. By Lemma 7 .1. 7 we can find elements C, 11 E Z such that 1 + C 2 - a17 2 = 0. Consider (a 1 + Cb1 + 17a1h1 ) 2 ; on computing this out we find that (a 1 + Cb1 + 17a1b1 ) 2 = a(1 + C 2 - a17 2 ) = 0. Being in a division ring this yields that a1 + Cb1 + 17a1 b1 = 0; thus 0 #- Sec. 7.2 Wedderburn's Theorem on Finite Division Rings 367 2a1 2 = at (at + Cbt + 11a1 bl) + (at + Cbt + 11a1 h1)a1 = 0. This contra- diction finishes the proof and Wedderburn's theorem is established. This second proof has some advantages in that we can use parts of it to proceed to a remarkable result due to Jacobson, namely, THEOREM 7.2.2 (JACOBSoN) Let D be a division ring such that for every a ED there exists a positive integer n(a) > 1, depending on a, such that an(a) = a. Then D is a commutative field. Proof. If a =f:. 0 is in D then an = a and (2a)m = 2a for some integers n, m > 1. Let s = (n - 1)(m - 1) + 1; s > 1 and a simple calculation shows that a 5 = a and (2a) 5 = 2a. But (2aY = 2 5a 5 = 2 5a, whence 2 5a = 2a from which we get (25 - 2)a = 0. Thus D has characteristic p > 0. If P c Z is the field having p elements (isomorphic to ]p), since a is algebraic over P, P(a) has a finite number of elements, in fact, ph ele- ments for some integer h. Thus, since a E P (a), aPh = a. Therefore, if a¢ Z all the conditions of Lemma 7.2.2 are satisfied, hence there exists a bED such that bah- 1 = all =f:. a. (1) By the same argument, bPk = b for some integer k > 1. Let W is finite and is closed under addition. By virtue of ( 1) it is also closed under multiplication. (Verify!) Thus W is a finite ring, and being a ~ub­ ring of the division ring D, it itself must be a division ring (Problem 3). Thus W is a finite division ring; by Wedderburn's theorem it is commutative. But a and b are both in W; therefore, ab = ba contrary to allb = ba. This proves the theorem. Jacobson's theorem actually holds for arry ring R satisfying an(a) = a for every a E R, not just for division rings. The transition from the division ring case to the general case, while not difficult, involves the axiom of choice, and to discuss it would take us too far afield. Problems 1. If t > 1 is an integer and (tm - 1) I (tn - 1 ), prove that m I n. 2. If D is a division ring, prove that its dimension (as a vector space) over its center cannot be 2. 3. Show that any finite subring of a division ring is a division ring. r-, -- 368 Selected Topics Ch. 7 4. (a) Let D be a division ring of characteristic p =I= 0 and let G be a finite subgroup of the group of nonzero elements of D under multiplication. Prove that G is abelian. (Hint: consider the sub- set {xED I x = LAigi, Ai E P, gi E G}.) (b) In part (a) prove that G is actually cyclic. *5. (a) If R is a finite ring in which xn = x, for all x E R where n > 1 prove that R is commutative. (b) If R is a finite ring in which x 2 = 0 implies that x = 0, prove that R is commutative. *6. Let D be a division ring and suppose that a ED only has a finite number of conjugates (i.e., only a finite number of distinct x- 1ax). Prove that a has only one conjugate and must be in the center of D. 7. Use the result of Problem 6 to prove that if a polynomial of degree n having coefficients in the center of a division ring has n + 1 roots in the division ring then it has an infinite number of roots in that division ring. *8. Let D be a division ring and K a subdivision ring of D such that xKx- 1 c K for every x =I= 0 in D. Prove that either K c Z, the center of D or K = D. (This result is known as the Brauer-Cartan-Hua theorem.) *9. Let D be a division ring and K a subdivision ring of D. Suppose that the group of nonzero elements of K is a subgroup of finite index in the group (under multiplication) of nonzero elements of D. Prove that either Dis finite or K = D. 10. If e =I= 1 is a root of unity and if q is a positive integer, prove that lq - 81 > q - 1. 7.3 A Theorem of Frobenius In 1877 Frobenius classified all division rings having the field of real numbers in their center and satisfying, in addition, one other condition to be described below. The aim of this section is to present this result of Frobenius. In Chapter 6 we brought attention to two important facts about the field of complex numbers. We recall them here: FACT 1 Every polynomial of degree n over the field of complex numbers has all its n roots in the field of complex numbers. FACT 2 The only irreducible polynomials over the field of real numbers are of degree 1 or 2. DEFINITION A division algebra Dis said to be algebraic over afield F if 1. F is contained in the center of D; 2. every a ED satisfies a nontrivial polynomial with coefficients in F. Sec. 7.3 A Theorem of Frobenius 369 If D, as a vector space, is finite-dimensional over the field F which is contained in its center, it can easily be shown that Dis algebraic over F (see Problem 1, end ofthis section). However, it can happen that D is algebraic over F yet is not finite-dimensional over F. We start our investigation of division rings algebraic over the real field by first finding those algebraic over the complex field. LEMMA 7.3.1 Let C be thefield of complex numbers and suppose that the division ring D is algebraic over C. Then D = C. Proof. Suppose that a ED. Since D is algebraic over C, an + cx1an- 1 + · · · + cxn_ 1a + an = 0 for some a1 , cx2 , ... , an in C. Now the polynomial p(x) = xn + cx1xn- 1 + · · · + an_ 1x + an in C[x], by Fact 1, can be factored, in C [ x], into a product of linear factors; that is, p(x) = (x - A1) (x - A2 ) • • • (x - An), where A1 , A2 , ••• , An are all in C. Since Cis in the center of D, every element of C commutes with a, hence p(a) = (a - A1 ) (a - A2 ) ···(a - An)· But, by assumption, p(a) = 0, thus (a - A1) (a - A2 ) ···(a - An) = 0. Since a product in a division ring is zero only if one of the terms of the product is zero, we conclude that a - Ak = 0 for some k, hence a = Ak, from which we get that a E C. Therefore, every element of Dis in C; since C c D, we obtain D = C. We are now in a position to prove the classic result ofFrobenius, namely, THEOREM 7.3.1 (FROBENIUs) Let D be a division ring algebraic over F, the field of real numbers. Then D is isomorphic to one of: the field of real numbers, the field of complex numbers, or the division ring of real quaternions. Proof. The proof consists of three parts. In the first, and easiest, we dispose of the commutative case; in the second, assuming that D is not commutative, we construct a replica of the real quaternions in D; in the third part we show that this replica of the quaternions fills out all of D. Suppose that D =P F and that a is in D but not in F. By our assumptions, a satisfies some polynomial over F, hence some irreducible polynomial over F. In consequence of Fact 2, a satisfies either a linear or quadratic equation over F. If this equation is linear, a must be in F contrary to assumption. So we may suppose that a2 - 2cxa + f3 = 0 where a, f3 E F. Thus (a- a) 2 = cx2 - {3; we claim that oc2 - f3 < 0 for, otherwise, it would have a real square root b and we would have a - ex = ± b and so a would be in F. Since a 2 - f3 < 0 it can be written as - y 2 where y E F. Con- sequently (a- a) 2 = -y 2 , whence [(a- a)Jy] 2 = -1. Thus if a ED, a ~ F we can find real a, y such that [ (a - a)/ y J 2 = - 1. If D is commutative, pick a ED, a~ F and let i = (a - a)Jy where a, y in Fare chosen so as to make i 2 = -1. Therefore D contains F(i), a field isomorphic to the field of complex numbers. Since D is commutative and l70 Selected Topics Ch. 7 algebraic over Fit is, all the more so, algebraic over F(i). By Lemma 7.3.1 we conclude that D = F(i). Thus if Dis commutative it is either For F(i). Assume, then, that Dis not commutative. We claim that the center of D must be exactly F. If not, there is an a in the center, a not in F. But then for some rt, y E F, [(a- rt)/y]z = -1 so that the center contains a field isomorphic to the complex numbers. However, by Lemma 7 .3.1 if the complex numbers (or an isomorph of them) were in the center of D then D = C forcing D to be commutative. Hence F is the center of D. Let a ED, a¢ F; for some rt, y E F, i = (a - rt)/y satisfies iz = -1. Since i ¢ F, i is not in the center of F. Therefore there is an element h E D such that c = hi- ih =/= 0. We compute ic + ci; ic + ci = i(hi - ih) + (hi - ih)i = ihi - izh + hi 2 - ihi = 0 since i 2 = -1. Thus ic = -ci; from this we get icz = -c(ic) = -c( -ci) = czi, and so cz commutes with i. Now c satisfies some quadratic equation over F, c2 + AC + 11 = 0. Since cz and 11 commute with i, AC must commute with i; that is, Aci = iAc = ).ic = - ).ci, hence 2Aci = 0, and since 2ci =/= 0 we have that A = 0. Thus cz = -11; since c ¢ F (for ci = - ic =/= ic) we can say, as we have before, that 11 is positive and so 11 = v 2 where v E F. Therefore cz = - v2 ; letj = cfv. Thenj satisfies z 1. j 2 = :__ = - 1. vz 2 . . .. c . . c ci + ic 0 • JZ + ZJ = - Z + Z - = --- = . v v v Let k = ij. The i,j, k we have constructed behave like those for the qua- ternions, whence T = {rt0 + rt1 i + rtz} + rt3k I rt0 , lit, liz, rt 3 E F} forms a subdivision ring of D isomorphic to the real quaternions. We have produced a replica, T, of the division ring of real quaternions in D! Our last objective is to demonstrate that T = D. If rED satisfies rz = -1 let N(r) = {xED I xr = rx}. N(r) is a sub- division ring of D; moreover r, and so all rt0 + lit r, rt0 , lit E F, are in the center of N(r). By Lemma 7.3.1 it follows that N(r) = {rt0 + rt 1r I rt0 , litE F}. Thus if xr = rx then x = rt0 + rt 1r for some rt0 , lit in F. Suppose that u ED, u ¢F. For some rt, f3 E F, w = (u - rt) f f3 satisfies w 2 = -1. We claim that wi + iw commutes with both i and w; for i(wi + iw) = iwi + izw = iwi + wiz = (iw + wi)i since iz = -1. Similarly w(wi + iw) = (wi + iw)w. By the remark of the preceding paragraph, wi + iw = rtb + rt~ i = a0 + rt 1 w. If w ¢ T this last relation forces lit = 0 (for otherwise we could solve for w in terms of i). Thus wi + iw = rt0 E F. Similarly wj + jw = {30 E F and wk + kw = Yo E F. Let z = w + rio i + f3o J + Yo k. 2 2 2 Sec. 7.4 Integral Ouaternions and the Four-Square Theorem 371 Then zi + zz = wz + zw + exo (i 2 + i 2 ) + f3o (ji + ij) + Yo (ki + ik) 2 2 2 = ex0 ex0 = 0; similarly zj + jz = 0 and zk + kz = 0. We claim these relations force z to be 0. For 0 = zk + kz = zij + ijz = (zi + iz) j + i(jz - zj) = i(jz - zj) since zi + iz = 0. However i ::/= 0, and since we are in a division ring, it follows that jz - zj = 0. But jz + zj = 0. Thus 2jz = 0, and since 2j ::/= 0 we have that z = 0. Going back to the expression for z we get hence w E T, contradicting w ¢= T. Thus, indeed, w E T. Since w = (u - ex)/ {3, u = f3w + ex and so u E T. We have proved that any element in D is in T. Since T c D we conclude that D = T; because Tis iso- morphic to the real quaternions we now get that D is isomorphic to the division ring of real quaternions. This, however, is just the statement of the theorem. Problems I. If the division ring D is finite-dimensional, as a vector space, over the field F contained in the center of D, prove that D is algebraic over F. 2. Give an example of a field K algebraic over another field F but not finite-dimensional over F. ..,. 3. If A is a ring algebraic over a field F and A has no zero divisors prove that A is a division ring. 7.4 Integral Ouaternions and the Follt-Square Theorem In Chapter 3 we considered a certain special class of integral domains called Euclidean rings. When the results about this class of rings were applied to the ring of Gaussian integers, we obtained, as a consequence, the famous result of Fermat that every prime number of the form 4n + I is the sum of two squares. We shall now consider a particular subring of the quaternions which, in all ways except for its lack of commutativity, will look like a Euclidean ring. Because of this it will be possible to explicitly characterize all its left-ideals. This characterization of the left-ideals will lead us quickly to a proof of the classic theorem of Lagrange that every positive integer is a sum of four squares. 372 Selected Topics Ch. 7 Let Q be the division ring of real quaternions. In Q we now proceed to introduce an adjoint operation, *, by making the DEFINITION For x = oc0 + octi + oc2j + oc3k in Q the adjoint of x, de- noted by x*, is defined by x* = oc0 - OCt i - oc2 j - oc3k. LEMMA 7 .4.1 The adjoint in Q satisfies 1. x** = x; 2. (bx + yy)* = bx* + yy*; 3. (xy)* =y*x*; for all x,y in Q and all real b andy. Proof. If x = oc0 + oct i + oc2 j + oc3k then x* = oc0 - oct i - oc2 j - oc3k, whence x** = (x*)* = oc0 + octi + oc2 j + oc3k, proving part 1. Let x = oc0 + OCt i + oc2 j + oc3k andy = {30 + f3t i + {32 j + {33k be in Q and let b and y be arbitrary real numbers. Thus bx + yy = (boc0 + y/30 ) + (boct + Yf3t)i + (boc2 + yf32 )j + (boc3 +-y{33)k; therefore by the definition of the *, (bx + yy)* = (bCJCo + y/30 ) - (boct + Yf3t)i - (boc2 + yf32 )j- (boc3 + yf33 )k = b(oc0 - octi - oc2j- oc3k) + y(/30 - f3ti - {32 j- {33k) = bx* + yy*. This, of course, proves part 2. In light of part 2, to prove 3 it is enough to do so for a basis of Q over the reals. We prove it for the particular basis 1, i,j, k. Now iJ = k, hence (ij)* = k* = -k = ji = ( -j)( -i) = j*i*. Similarly (ik)* = k*i*, (Jk)* = k*j*. Also (i 2 )* = ( -1)* = -1 = (i*) 2 , and similarly for j and k. Since part 3 is true for the basis elements and part 2 holds, 3 is true for all linear combinations of the basis elements with real coefficients, hence 3 holds for arbitrary x andy in Q. DEFINITION If x E Q then the norm of x, denoted by N(x), Is defined by N(x) = xx*. Note that if x = oc0 + octi + oc2 j + oc3k then N(x) = xx* = (oc0 + octi + oc2 j + oc3k) ( oc0 - oc1 i - oc2 j - oc3k) = oc0 2 + oct 2 + oc2 2 + oc3 2 ; therefore N(O) = 0 and N(x) is a positive real number for x =/= 0 in Q. In particular, for any real number ex, N(oc) = oc2 • Ifx =1= 0 note that x-t = [1/N(x)]x*. LEMMA 7.4.2 For all x,y E Q, N(xy) = N(x)N(y). ' . Proof. By the very definition of norm, N(xy) = (xy)(xy)*; by part 3 of Lemma 7.4.1, (xy)* = y*x* and so N(xy) = xyy*x*. However, yy* = N(y) is a real number, and thereby it is in the center of Q; in particular it must commute with x*. Consequently N(xy) = x(yy*)x* = (xx*)(yy*) = N(x)N(y). Sec. 7.4 Integral Quaternions and the Four-Square Theorem 373 As an immediate consequence of Lemma 7 .4.2 we obtain LEMMA 7.4.3 (LAGRANGE IDENTITY) If a0 , a 1, a2, a 3 and P0 , P1 , P2, P3 are real numbers then (a0 2 + a 1 2 + a2 2 + a 3 2 ) (Po 2 + P1 2 + P2 2 + P3 2 ) (e<oPo - a1P1 - a2P2 - a3P3) 2 + (aoPt + at Po + a2P3 - a3P2) 2 + (C<oP2 - atP3 + a2Po + a3P1) 2 + (aoP3 + a1P2 - a2P1 + a3Po) 2 • Proof. Of course there is one obvious proof of this result, namely, multiply everything out and compare terms. However, an easier way both to reconstruct the result at will and, at the same time, to prove it, is to notice that the left-hand side is N(x)N(y) while the right-hand side is N(xy) where x = a0 + a 1i + a2j + a3k and y =Po + P1i + P2j + P3k. By Lemma 7.4.2, N(x)N(y) = N(xy), ergo the Lagrange identity. The Lagrange identity says that the sum of four squares times the sum of four squares is again, in a very specific way, the sum of four squares. A very striking result of Adolf Hurwitz says that if the sum of n squares times the sum of n squares is again a sum of n squares, where this last sum has terms computed bilinearly from the other two sums, then n = 1, 2, 4, or 8. There is, in fact, an identity for the product of sums of eight squares but it is too long and cumbersome to write down here. Now is the appropriate time to introduce the Hurwitz ring of integral quaternions. Let ' = !(1 + i + j + k) and let LEMMA 7.4.4 H is a subring of Q. If x E H then x* E H and N(x)~is a positive integer for every nonzero x in H. We leave the proof of Lemma 7.4.4 to the reader. It should offer no difficulties. ~ In some ways H might appear to be a rather contrived ring. Why use the quaternions '? Why not merely consider the more natural ring Q0 = {m0 + m1i + m2 j + m3k I m0 , m1, m2 , m3 are integers}? The answer is that Q0 is not large enough, whereas H is, for the key lemma which follows to hold in it. But we want this next lemma to be true in the ring at our disposal for it allows us to characterize its left-ideals. This, perhaps, indicates why we (or rather Hurwitz) chose to work in H rather than in Q0 • LEMMA 7.4.5 (LEFT-DIVISION ALGORITHM) Let a and b be in H with b ¥ 0. Then there exist two elements c and d in H such that a = cb + d and N(d) < N(b). 374 Selected Topics Ch. 7 Proof. Before proving the lemma, let's see what it tells us. If we look back in the section in Chapter 3 which deals with Euclidean rings, we can see that Lemma 7.4.5 assures us that except for its lack of commutativity H has all the properties of a Euclidean ring. The fact that elements in H may fail to commute will not bother us. True, we must be a little careful not to jump to erroneous conclusions; for instance a = eb + d but we have no right to assume that a is also equal to be + d, for b and e might not commute. But this will not influence any argument that we shall use. In order to prove the lemma we first do so for a very special case, namely, that one in which a is an arbitrary element of H but b is a positive integer n. Suppose that a = t0 ' + t1 i + t2 j + t3k where t0 , t1 , t2 , t3 are integers and that b = n where n is a positive integer. Let e = x0 ' + x1i + x2 j + x3k where x0 , x1 , x2 , x3 are integers yet to be determined. We want to choose them in such a manner as to force N(a - en) < N(n) = n2 • But ( ( 1 + i + j + k) . . k) a - en = t0 2 + t1 z + t2 ) + t3 ( 1 + i + j + k) . . k - nx0 2 - nx1z - nx2J - nx3 = -!(to - nx0 ) + -!(t0 + 2t1 - n(t0 + 2x1) )i + -!(t0 + 2t1 - n(t0 + 2x2 )) j + -!(t0 + 2t3 - n(t0 + 2x3 ) )k. If we could choose the integers x0 , xv x2 , x3 in such a way as to make Ito - nx0 1 ~ -!n, Ito + 2t1 - n(t0 + 2x1)1 ~ n, Ito + 2t2 - n(t0 + 2x2 )1 ~ n and Ito + 2t3 - n(t0 + 2x3 ) I ~ n then we would have (t0 - nx0 ) 2 (t0 + 2t1 - n(t0 + 2x1 )) 2 N(a - en) = + + · · · 4 4 which is the desired result. But now we claim this can always be done: 1. There is an integer x0 such that t0 = x0n + r where -tn ~ r ~ -!n; for this x0 , Ito - x0nl = lrl ~ -!n. 2. There is an integer k such that t0 + 2t1 = kn + r and 0 ~ r ~ n. If k - t0 is even, put 2x1 = k - t0 ; then t0 + 2t1 = (2x1 + t0 )n + r and Ito + 2t1 - (2x1 + t0 )nl = r < n. If, on the other hand, k - t0 is odd, put 2x1 = k - t0 + 1; tnus t0 + 2t1 = (2x1 + t0 - 1 )n + r = (2x1 + t0 )n + r - n, whence lt0 + 2t1 - (2x1 + t0 )nl = lr - nl ~ n since 0 ~ r < n. Therefore we can find an integer x1 satisfying Ito + 2t1 - (2x1 + t0 )nl ~ n. 3. As in part 2, we can find integers x 2 and x3 which satisfy Ito + 2t2 - (2x2 + t0 )nl ~ n and Ito + 2t3 - (2x3 + t0 )nl ~ n, respectively. Sec. 7.4 Integral Quaternions and the Four-Square Theorem In the special case in which a is an arbitrary element of H and b is a positive integer we have now shown the lemma to be true. We go to the general case wherein a and b are arbitrary elements of H and b =I= 0. By Lemma 7.4.4, n = bb* is a positive integer; thus there exists acE H such that ab* = en + d1 where N(d1 ) < N(n). Thus N(ab* - en) < N(n); but n = bb* whence we get N(ab* - ebb*) < N(n), and so N((a - cb)b*) < N(n) = N(bb*). By Lemma 7.4.2 this reduces to N(a - cb)N(b*) < N(b)N(b*); since N(b*) > 0 we get N(a - cb) < N(b). Putting d = a - cb we have a = cb + d where N(d) < N(b). This completely proves the lemma. As in the commutative case we are able to deduce from Lemma 7 .4.5 LEMMA 7.4.6 Let L be a left-ideal of H. Then there exists an element u E L such that every element in L is a left-multiple of u .; in other words, there exists u E L such that every x E L is of the form x = ru where r E H. Proof. If L = (0) there is nothing to prove, merely put u = 0. Therefore we may assume that L has nonzero elements. The norms ofthe nonzero elements are positive integers (Lemma 7.4.4) whence there is an element u =1= 0 in L whose norm is minimal over the nonzero elements of L. If x e L, by Lemma 7.4.5, x = cu + d where N(d) < N(u). However d is in L because both x and u, and so cu, are in L which is a left-ideal. Thus N (d) = 0 and so d = 0. From this x = cu is a consequence. Before we can prove the four-square theorem, which is the goal of this section, we need one more lemma, namely LEMMA 7.4.7 If a e H then a- 1 e H if and only if N(a) = 1. Proof. If both a and a- 1 are in H, then by Lemma 7.4.4 both N(a) and N(a- 1 ) are positive integers. However, aa- 1 = I, hence, by Lemma 7.4.2, N(a)N(a- 1 ) = N(aa- 1),= N(l) = 1. This forces N(a) = 1. On the other hand, if a E Hand N(a) = I, then aa* = N(a) = I and so a- 1 = a*. But, by Lemma 7.4.4, since a e H we have that a* E H, and so a- 1 = a* is also in H. We now have determined enough of the structure of H to use it effectively to study properties of the integers. We prove the famous classical theorem of Lagrange, THEOREM 7 .4.1 Every positive integer can be expressed as the sum of squares of four integers. Proof. Given a positive integer n we claim in the theorem that n = x0 2 + x1 2 + x2 2 + x3 2 for four integers x0 , x1 , x2 , x3 • Since every integer factors into a product of prime numbers, if every prime number were 375 376 Selected Topics Ch. 7 realizable as a sum of four squares, in view of Lagrange's identity (Lemma 7.4.3) every integer would be expressible as a sum of four squares. We have reduced the problem to consider only prime numbers n. Certainly the prime number 2 can be written as 12 + 12 + 0 2 + 0 2 as a sum of four squares. Thus, without loss of generality, we may assume that n is an odd prime number. As is customary we denote it by p. Consider the quaternions WP over ]p, the integers mod p; WP = {oc0 + oc1i + oc2 j + oc3k I oc0 , oc1 , oc2 , oc3 E ]p}· WP is a finite ring; moreover, since p =I 2 it is not commutative for ij = --ji =I ji. Thus, by Wedder- burn's theorem it cannot be a division ring, hence by Problem 1 at the end of Section 3.5, it must have a left-ideal which is neither (0) nor wp: But then the two-sided ideal V in H defined by V = {x0 ' + x1 i + x2 j + x3k I p divides all of x0 , x1 , x2 , x3 } cannot be a maximal left-ideal of H, since HJ V is isomorphic to WP. (Prove!) (If V were a maximal left-ideal in H, HJ V, and so WP, would have no left-ideals other than (0) and HJV). Thus there is a left-ideal L of H satisfying: L =I H, L =1 V, and L ~ V. By Lemma 7.4.6, there is an element u E L such that every element in L is a left-multiple of u. Since p E V, p E L, whence p = cu for some c E H. Since u ¢: V, c cannot have an inverse in H, otherwise u = c- 1p would be in V. Thus N(c) > 1 by Lemma 7.4.7. Since L =I H, u cannot have an inverse in H, whence N(u) > 1. Since p = cu, p 2 = N(p) = N(cu) = N(c)N(u). But N(c) and N(u) are integers, since both c and u are in H, both are larger than 1 and both divide p2 • The only way this is possible is that N(c) = N(u) = p. Since u E H, u = m0 ' + m1i + m2J + m3k where m0 , mv m2 , m3 are in- tegers; thus 2u = 2m0 ' + 2m1i + 2m2 j + 2m3k = (m0 + m0i +m0 j + m0 k) + 2m1i + 2m2 j + 2m3k = m0 + (2m 1 + m0 )i + (2m2 + m0 ) j + (2m3 + m0 )k. Therefore N(2u) = m0 2 + (2m1 + m0 ) 2 +(2m 2 + m0 ) 2 + (2m3 + m0 ) 2 . But N(2u) = N(2)N(u) = 4p since N(2) = 4 and N(u) = p. We have shown that 4p = m0 2 + (2m1 + m0 ) 2 + (2m2 + m0 ) 2 + (2m3 + m0 ) 2 • We are almost done. To finish the proof we introduce an old trick of Euler's: If 2a = x0 2 + x1 2 + x2 2 + x3 2 where a, x0 , x1 , x2 and x3 are integers, then a =Yo 2 + y 1 2 + y 2 2 + y 3 2 for some integers y 0 ,y 1,y2 ,y3 • To see this note that, since 2a is even, the x's are all even, all Odd or two are even and two are odd. At any rate in all three cases we can renumber the x's and pair them in such a way that Xo + xt Yo =-- 2-, Xo- xt Yt =-- 2-, x2 + x3 Y2 = -- 2-, and Sec. 7.4 Integral Ouaternions and the Four-Square Theorem 377 are all integers. But Yo 2 + Y1 2 + Y2 2 + Y3 2 = !(2a) =a. Since 4p is a sum of four squares, by the remark just made 2p also is; since 2p is a sum of four squares, p also must be such a sum. Thus p = a0 2 + a1 2 + a2 2 + a3 2 for some integers a0 , a1 , a2 , a3 and Lagrange's theorem is established. This theorem itself is the starting point of a large research area in number theory, the so-called Waring problem. This asks if every integer can be written as a sum of a fixed number of kth powers. For instance it can be shown that every integer is a sum of nine cubes, nineteen fourth powers, etc. The Waring problem was shown to have an affirmative answer, in this century, by the great mathematician Hilbert. Problems 1. Prove Lemma 7.4.4. 2. Find all the elements a in Q0 such that a- 1 is also in Q0 • 3. Prove that there are exactly 24 elements a in H such that a- 1 is also in H. Determine all of them. 4. Give an example of an a a~d b, b =I= 0, in Q0 such that it is impossible to find c and d in Q0 satisfying a = cb + d where N(d) < N(b). 5. Prove that if a E H then there exist integers IX, fJ such that a2 + eta + fJ = 0. 6. Prove that there is a positive integer which cannot be written as the sum of three squares. *7. Exhibit an infinite number ofpositive integers which cannot be written as the sum of three squares. Supplementary Reading For a deeper di~cussion of finite fields: ALBERT, A. A., Fundamental Concepts of Higher Algebra. Chi.::ago: University of Chicago Press, 1956. 378 Selected Topics Ch. 7 For many proofs of the four-square theorem and a discussion of the Waring problem: HARDY, G. H., and WRIGHT, E. M., An Introduction to the Theory of· Numbers, 4th ed. New York: Oxford University Press, 1960. For another proof of the Wedderburn theorem: ARTIN, E., \"Uber einen Satz von Herrn J. H. M. Wedderburn,\" Abhandlungen, Hamburg Mathematisches Seminar, Vol. 5 (1928), pages 245-50. Index .ABEL, 237, 252, 256 Abelian group, 28 structure of finite, 109, 204 structure of finitely generated, 203 Adjoint(s), 318, 321 Hermitian, 318, 319, 322, 336, 339, 340 quaternions, 372 Adjunction of element to a field, 210 ALBERT, 356, 377 Algebra, 262 algebraic division, 368 of all n x n matrices over F, 278, 279 I Boolean, 9, 130 fundamental theorem of, 337 linear, 260 of linear transformations, 261 Algebraic of degree n, 212, 213 Algebraic division algebra, 368 Algebraic element, 209, 210 Algebraic extension, 213 Algebraic integer, 215 Algebraic number(s), 214-216 Algorithm division, 155 Euclidean, 18 left-division, 373 ALPERTN, 119 Alternating group, 80, 256 Angle, trisecting, 230 Annihilator of a subspace, 188 ARTIN, 237, 259, 378 Associates, 146, 162 Associative law(s), 14, 23, 27, 28, 36 Associative ring, 121 Automorphism(s) of a cyclic group, 66, 67 of the field, 237 fixed field of a group of, 238 group of inner, 68 group of outer, 70 inner, 68 of K relative to F, 239 Axiom of choice, 138, 367 Basis(es), 177, 180 dual, 187 orthonormal, 196, 338 Bessel inequality, 200 Binary relation, 11 BIRKHOFF, 25 BIRKHOFF, G. D., 362 Boolean algebra, 9, 130 Boolean ring, 130 Brauer-Cartan-Hua theorem, 368 BURNSIDE, 119 379 1:10 Index Cancellation laws, 34 Canonical form(s), 285 Jordan, 299, 301, 302 rational, 305, 306, 308 Cardan's formulas, 251 Cartesian product, 5, 6 CAUCHY, 61, 86, 87 Cauchy's theorem, 61, 87 CAYLEY, 71 Cayley-Hamilton theorem, 263, 309, 334, 335 Cayley's theorem, 71, 262 Center of a group, 4 7, 68 Centralizer, 47 Characteristic of integral domain, 129, 232, 235, 357 Characteristic polynomial, 309, 332 Characteristic root(s), 270, 286-289 multiplicity of, 303 Characteristic subgroup, 70 Characteristic vector, 271 Characteristic zero, 129, 232, 235 Choice, axiom of, 138, 367 Class(es) congruence, 22, 353, 354 conjugate, 83, 89, 361 equivalence, 7 similarity, 285 Class equation, 85, 361 Closure under operation, 27 Coefficients, 153 Cofactor, 334 Column of a matrix, 277 Combination, linear, 177 Commutative group, 28 Commutative law, 23 Commutative ring(s), 121 polynomial rings over, 161 Commutator, 252 Commutator subgroup(s), 65, 70, 117, 252, 253 Companion matrix, 307 Complement, 5 Complement, orthogonal, 195 Complex vector space, 191 Composition of mappings, 13 Congruence class, 22, 353, 354 Congruence modulo a subgroup, 39 Congruence modulo n, 22 Congruent, 352 Conjugacy, 83 Conjugate, 83 Conjugate class(es), 83, 89, 361 Conjugate elements, 83 Conjugate subgroups, 99 Constructible, 228, 230 Constructible number, 228 Construction, invariant, 187, 188 Construction with straightedge and compass, 228 Content of polynomial, 159, 163 Correspondence, one-to-one, 15 Coset double, 49, 97, 98 left, 47 right, 40 Cramer's rule, 331 Criterion Eisenstein, 160, 240, 249 Euler, 360 Cube, duplicating of, 231 Cycle decomposition, 78 Cyclic group, 30, 38, 49 generator of, 48 Cyclic module, 202 Cyclic subgroup, 38 Cyclic subspace, 296, 306 Cyclotomic polynomial, 250, 362 De Morgan rules, 8 Decomposable set of linear transforma- tions, 291 Decomposition, cycle, 78 Definite, positive, 345 Degree n algebraic of, 212, 213 alternating group of, 80, 256 of an extension, 208 general polynomial of, 251 ofpolynomial, 154, 162 symmetric group of, 28, 75, 241, 253-257, 284 DER WAERDEN, vAN, 259 Derivative, 158, 232, 233 Desargues' theorem, 361 Determinant, 322 of linear transformation, 329 of matrix, 324 of system of linear equations, 330 Diagonal matrix, 282, 305 Diagonal subset, 6 Diagonalizable, 305 DICKSON, 356 Difference module, 202 Difference set, 5 Dihedral group, 54, 81 Dimension, 181 DIOPHANTOS, 356 Direct product of groups, 103 external, I 04, 105 internal, I 06 Direct sum external, I 7 5 internal, 174, 175 of modules, 202 Disjoint sets, 4 mutually, 5 Distributive law(s), 23, 121 Divisibility, 144, 145 Division algebra, algebraic, 368 Division algorithm for polynomials, 155 Division ring, 126 finite, 360 Divisor(s), 18 elementary, 308, 309, 310 greatest common, 18, 145 Domain integral, 126 unique factorization, 163 Dot product, 192 Double coset, 49, 97, 98 Dual basis, 187 Dual, second, 188 Dual space, 184, 187 Duplicating the cube, 231 Eigenvalue, 270 Eisenstein criterion, 160, 240, 249 Element(s) algebraic, 209, 210 conjugate, 83 identity, 27, 28 order of, 43 order of (in a module), 206 period of, 43 prime, 146, 163 separable, 236 Elementary divisors of a linear trans- formation, 308, 309, 310 , Elementary symmetric functions, 242, 243 Empty set, 2 Equality of mappings, 13 of sets, 2 Equation(s) class, 85, 361 linear homogeneous, 189, 190 rank of system of linear, 190 secular, 332 Equivalence class, 7 Equivalence relation, 6 Euclidean algorithm, 18 Euclidean rings, 143, 371 EuLER,43, 356,376 Euler criterion, 360 Index 381 Euler phi-function, 43, 71, 227, 250 Even permutation, 78, 79 Extension algebraic, 213 degree of, 208 field, 207 finite, 208-212 normal, 244-248 separable, 236, 237 · simple, 235, 236 External direct product, 104, 105 External direct sum, I 7 5 FERMAT, 44, 144, 149, 152, 356, 366, 371 Fermat theorem, 44, 152, 366 Fermat theorem, little, 44, 366 Field(s), 126, 127, 207 adjunction of element to, 210 automorphism of, 237 extension, 207 finite, 122, 356 perfect, 236 of quotients, 140 of rational functions, 162, 241 of rational functions inn-variables, 241 splitting, 222-227, 245 of symmetric rational functions, 241 Finite abelian group(s), 109 fundamental theorem of, 109, 204 invariants of, Ill Finite characteristic, 129 Finite dimensional, 178 Finite extension, 208-212 Finite field, 122, 356 Finite group, 28 Finitely generated abelian group, 202 Finitely generated modules, 202 fundamental theorem on, 203 Fixed field of group of automorphisms, 238 382 Index Form(s) canonical, 285 Jordan canonical, 299, 301, 302 rational canonical, 305-308 real quadratic, 350 triangular, 285 Four-square theorem, 371 FROBENIUS, 356, 368, 369 Frobenius theorem, 369 Functional, linear, 187, 200 Functions elementary symmetric, 242, 243 rational, 162, 241 symmetric rational, 241 Fundamental theorem of algebra, 337 of finite abelian groups, 109, 204 of finitely generated modules, 203 of Galois theory, 247 GALOIS, 50, 207 Galois group, 237 Galois theory, 237-259 fundamental theorem of, 24 7 Gauss' lemma, 160, 163, 164 Gaussian integers, 149 GELFOND, 216 General polynomial of degree n, 251 Generator of cyclic group, 48 Gram-Schmidt orthogonalization pro- cess, 196 Greatest common divisor, 18, 145 Group(s), 28 abelian, 28, 109, 203, 204 alternating, 80, 256 automorphism(s) of, 66, 67 of automorphisms, fixed field of, 238 of automorphisms of K over F, 239 center of, 47, 68 commutative, 28 cyclic, 30, 38, 49 dihedral, 54, 81 direct product of, 103 factor, 52 finite, 28 Galois, 237 generator of cyclic, 48 homomorphism(s) of, 54 of inner automorphisms, 68 isomorphic, 58 isomorphism(s) of, 58 nilpotent, 117 order of, 28 of outer automorphisms, 70 permutation, 75 quaternion units, 81 quotient, 52 simple, 60 solvable, 116, 252 symmetric, 28, 75, 241, 253-257, 284 HALL, 119 HALMOS, 206, 354 HAMILTON, 124, 334, 356 HARDY, 378 HERMITE, 216, 218 Hermitian adjoint, 318, 319, 322, 336, 339, 340 Hermitian linear transformation, 336, 341 Hermitian matrix, 319, 322, 336 Hexagon, regular, 232 Higher commutator subgroups, 252, 253 HILBERT, 216, 377 Hom (U, V), 173 Homogeneous equations, linear, 189, 190 Homomorphism(s), 54, 131 of groups, 54 kernel of, 56, 131 of modules, 205 of rings, 131 of vector-spaces, 1 73 HuR~Tz, 216, 356, 373 (i, j) entry, 277 Ideal(s), 133, 134, 137 left, 136 maximal, 138 prime, 167 principal, 144 radical of, 167 right, 136 Idempotent, 268 Identity(ies) Lagrange's, 373 Newton's, 249 Identity element, 27, 28 Identity mapping, 11 Image, 11 inverse, 12, 58 of set, 12 Independence, linear, 177 Index of H in G, 41 of nil potence, 268, 294 Index set, 5 Inequality Bessel, 200 Schwartz, 194 triangle, 199 Inertia, Sylvester's law of, 352 Infinite set, 17 Inner automorphism(s), 68 group of, 68 Inner product, 193 Inner product spaces, 191, 337 Integer(s), 18 algebraic, 215 Gaussian, 149 partition of, 88 relatively prime, 19 Integers modulo n, 22, 23 Integral domain, 126 characteristic of, 129, 232, 235, 237 Integral quaternions, 371 Internal direct product, 106 Internal direct sum, 174, 175 Intersection of sets, 3, 4 Invariant construction (or proof), 187, 188 Invariant subspace, 285, 290 Invariants of finite abelian group, Ill of nilpotent linear transformation, 296 Inverse element, 28 Inverse image, 12, 58 Inverse of mapping, 15 Invertible linear transformation, 264 Irreducible elements, 163 Irreducible module, 206 Irreducible polynomial, 156 Irreducible set of linear transformations, 291 Isomorphic groups, 58 Isomorphic rings, 133 Isomorphic vector spaces, 173 Isomorphism of groups, 58 of modules, 205 of rings, 133 of vector spaces, 173 jACOBSON, 355, 367 Jacobson's lemr:1a, 316, 320 Jacobson's theorem, 367 Jordan block, 30 I Index 383 Jordan canonical form, 299, 301, 302 KAPLANSKY, 259 Kernel of homomorphism, 56, 131 LAGRANGE, 40, 356, 371 Lagrange's identity, 373 Lagrange's theorem, 40, 375 Law(s) associative, 14, 23, 27, 28, 36 cancellation, 34 commutative, 23 distributive, 23, 121 of inertia, Sylvester's, 352 Sylvester's, 352 Least common multiple, 23, 149 Left coset, 4 7 Left-division algorithm, 37.3 Left ideal, 136 Left-invertible, 264 Lemma Gauss', 160, 163, 164 Jacobson's, 316, 320 Schur's, 206 Length, 192, 193 LINDEMANN, 216 Linear algebra, 260 Linear combination, 177 Linear equations determinant of system of, 330 rank of system of, 190 Linear functional, 187, 200 Linear homogeneous equations, 189, 190 Linear independence, 1 77 Linear SRan, 177 Linear transformation(s), 26 algebra of, 261 decomposable set of, 291 determinant of, 329 elementary divisors of, 308, 309, 310 Hermitian, 336 invariants of nilpotent, 296 invertible, 264 irreducible set of, 291 matrix of, 274 nilpotent, 268, 292, 294 nonnegative, 345 normal, 342 positive, 345 positive definite, 345 ;84 Index Linear transformation(s) (continued) range of, 266 rank of, 266 regular, 264 ring of, 261 singular, 264 trace of, 314 Linearly dependent vectors, 177 LIOUVILLE, 216 Little Fermat theorem, 44, 366 McCoY, 169 McKAY, 87, 119 MACLANE, 25 Mapping(s), 10 composition of, 13 equality of, 13 identity, 11 inverse of, 15 one-to-one, 12 onto, 12 product of, 13 restriction of, 17 set of all one-to-one 15 Matrix(ces), 273 ' column of, 277 companion, 307 determinant of, 324 diagonal, 282, 305 Hermitian, 319, 322, 336 of a linear transformation, 274 orthogonal, 346 permutation, 284 real symmetric, 34 7 row of, 277 scalar, 279 skew-symmetric, 317 theory of, 260, 273 trace of, 313 transpose of, 316 triangular, 284, 286 unit, 279 Maximum ideal, 138 Minimal polynomial, 211, 264 Module(s), 201 cyclic, 202 difference, 202 direct sum of, 202 finitely generated, 202 fundamental theorem on finitely gen- erated, 203 homomorphism(s) of, 205 irreducible, 206 isomorphism of, 205 order of element in, 206 quotient, 202 rank of, 203 unital, 201 Modulus, 22 Monic polynomial, 160 Morgan rules, De, 8 MoTZKIN, 144, 169 Multiple, least common 23 149 Multiple root, 233 ' ' Multiplicative system, 142 Multiplicity of a characteristic root, 303 of a root, 220 Mutually disjoint, 5 n x n matrix(ces) over F, 278 algebra of all, 278, 279 n-variables field of rational functions, 241 polynomials in, 162 ring of polynomials in, 162 Newton's identities, 249 Nilpotence, index of, 268, 294 Nilpotent group, 117 Nilpotent linear transformation 268 ' ' 292,294 invariants of, 296 NIVEN, 216, 259 Non-abelian, 28 Nonassociative ring, 121 Nonnegative linear transformation 345 Nontrivial subgroups, 38 ' Norm, 193 Norm of quaternion, 372 Normal extension(s), 244-248 Normal linear transformation, 342 Normal subgroup(s), 49 Normalizer, 47, 84, 99, 361 nth root of unity, primitive, 249 Null set, 2 Number(s) algebraic, 214-216 constructible, 228-230 prime, 19 transcendental, 214 Odd permutation, 78, 79 One-to-one correspondence, 15 One-to-one mapping(s), 12 set of all, 15 Onto mappings, 12 Operation, closure under, 27 Order of an element, 43 of an element in a module, 206 of a group, 28 Orthogonal complement, 195 Orthogonal matrices, 346 Orthogonalization process, Gram- Schmidt, 196 Orthonormal basis, 196, 338 Orthonormal set, 196 Outer automorphism, 70 group of, 70 p-Sylow subgroup, 93 Pappus' theorem, 361 Partitions of an integer, 88 Pentagon, regular, 232 Perfect field, 236 Period of an element, 43 Permutation even, 78, 79 groups, 75 matrices, 284 odd, 78, 79 representation, 81 representation, second, 81 Perpendicularity, 191, 195 phi-function, Euler, 43, 71, 227, 250 Pigeonhole principle, 127 POLLARD, 259 Polynomial ( s) characteristic, 308, 332 content of, 159, 163 cyclotomic, 250, 362 degree of, 152, 162 division algorithm for, 155 irreducible, 156 minimal, 211, 264 monic, 160 in n-variables, 162 over ring, 161 over rational field, 159 primitive, 159, 163 ring of, 161 roots of, 219 symmetric, 243, 244 value of, 209 Positive definite, 345 linear transformation, 345 Prime primitive root of, 360 relatively, 19, 14 7 Prime element, 146, 163 Prime ideal, 167 Prime number, 19 Primitive nth root of unity, 249 Primitive polynomial, 159, 163 Primitive root of a prime, 360 Product Cartesian, 5, 6 direct, 103 dot, 192 inner, 193 of mappings, 13 Projection, 11 Proper subset, 2 Quadratic forms, real, 350 Quadratic residue, 116, 360 Quaternions, 81, 124, 371 adjoint of, 372 group of quaternion units, 81 integral, 371 norm of, 372 Quotient group, 52 Quotient module, 202 Quotient ring, 133 Quotient space, 174 Quotient structure, 51 Quotients, field of, 140 R-m.£?dule, 201 unital, 201 Radical of an ideal, 167 Radicals, solvable by, 250-256 Range of linear transformation, 266 Rank of linear transformation, 266 of module, 203 Index 385 of system of linear equations, 190 Rational canonical form, 305, 306, 308 Rational functions, 162, 2 41 field of, 162, 241 symmetric, 241 Real quadratic forms, 350 Real quaternions, 81 386 Index Real symmetric matrix, 34 7 Real vector space, 191 Reflexivity of relations, 6 Regular hexagon, 232 Regular linear transformation, 264 Regular pentagon, 232 Regular septagon, 232 Regular 15-gon, 232 Regular 9-gon, 232 Regular 17-gon, 232 Relation(s) binary, 11 equivalence, 6 reflexivity of, 6 symmetry of, 6 transitivity of, 6 Relatively prime, 19, 147 Relatively prime integers, 19 Remainder theorem, 219 Representation, permutation, 81 second, 81 Residue, quadratic, 116, 360 Resolution, spectral, 350 Restriction of mapping, 17 Right coset, 40 Right ideal, 136 Right invertible, 264 Ring(s), 120 associative, 121 Boolean, 9, 130 commutative, 121 division, 126, 360 Euclidean, 143, 371 homomorphisms of, 131 isomorphisms of, 133 of linear transformations, 261 nonassociative, 121 polynomial, 161 of polynomials, 161 of polynomials inn-variables, 162 quotient, 133 of 2 x 2 rational matrices, 123 unit in, 145 with unit element, 121 Root(s), 219, 232 characteristic, 270, 286-289 multiple, 233 multiplicity of, 220, 303 of polynomial, 219 Row ofmatrix, 277 Rule, Cramer's, 331 Rule, De Morgan's, 8 SAMUEL, 169 Scalar(s), 171 Scalar matrices, 279 Scalar product, 192 ScHNEIDER, 216 Schur's lemma, 206 Schwarz' inequality, 194 Second dual, 188 Second permutation representation, 81 Secular equation, 332 SEGAL, 119 Self-adjoint, 341 Separable element, 236 Separable extension, 236 Septagon, regular, 232 Set(s), 2 of all one-to-one mappings, 15 of all subsets, 12 difference, 5 disjoint, 4 empty, 2 image under mapping, 12 index, 5 infinite, 1 7 of integers modulo n, 22, 23 intersection of, 3, 4 null,2 orthonormal, 2 theory of, 2 union of, 3 SIEGEL, 216, 259 Signature of a real quadratic form, 352 Similar, 285 Similarity class, 285 Simple extension, 235, 236 Simple group, 60 Singular, 264 Singular linear transformation, 264 Skew-field, 125 Skew-Hermitian, 341 Skew-symmetric matrix, 317 Solvable group, 116, 252 Solvable by radicals, 250-256 Space(s) complex vector, 191 dual, 184, 187 inner product, 191, 33 7 quotient, 174 real vector, 191 vector, 170 I , Span, linear, 177 Spectral resolution, 350 Splitting field, 222-227, 245 Straightedge and compass, construction with, 228 Subgroup(s), 37 commutator, 65, 70, 117, 252, 253 conjugate, 99 cyclic, 38 generated by a set, 64 higher commutator, 253 left coset of, 4 7 nontrivial, 38 normal, 49 p-Sylow, 93 right coset of, 40 trivial, 38 Subgroup of G characteristic, 70 commutator, 65, 70, 117, 252, 253 generated by a set, 64 Submodule, 202 Subset(s), 2 diagonal, 6 proper, 2 restriction of mapping to, 17 set of all, 12 Subspace, 172 annihilator of, 188 cyclic, 296, 306 invariant, 285, 290 Sum direct, 202 external direct, 1 7 5 internal direct, 174, 175 SYLOW, 62, 87, 91 Sylow's theorem, 62, 91-101 Sylvester's law of inertia, 352 Symmetric difference, 9 Symmetric functions, elementary, 242, 243 Symmetric group(s), 28, 75, 241, 253- 257, 284 Symmetric matrix, 317 Symmetric polynomial, 243, 244 Symmetric rational functions, 241 field of, 241 Symmetry of relations, 6 System, multiplicative, 142 System of linear equations, 189, 190 determinant of, 330 rank of, 190 Theorem of algebra, fundamental, 337 Brauer-Cartan-Hua, 368 Cauchy's, 61, 87 Index 387 Cayley-Hamilton, 263, 309, 334, 335 Cayley's, 71, 262 Desargues', 361 Fermat, 44, 152, 366 four-square, 371 Frobenius', 356, 359 Jacobson's, 367 Lagrange's, 40, 356, 375 little Fermat, 44, 366 Pappus', 361 remainder, 219 Sylow's, 62, 91-101 on symmetric polynomials, 244 unique factorization, 20, 148 Wedderburn's, 355, 360, 376 Wilson's, 116, 152 Theory Galois, 237-259 matrix, 260, 273 set, 2 THOMPSON, 60 Trace, 313 of a linear transformation, 314 of a matrix, 313 Transcendence of e, 216 ofn, 216 Transcendental number(s), 214 Transformation(s) algebra of linear, 261 Hermitian linear, 336, 341 invariants of nilpotent linear, 296 invertible linear, 264 linear, 261 nilpotent linear, 268, 292, 294 nonnegative linear, 345 normal linear, 336, 342 range of linear, 266 rank of linear, 266 regular linear, 261 singular linear, 264 unitary, 336, 338 Transitivity of relations, 6 Transpose, 313, 316 of a matrix, 316 Transpositions, 78 Triangle inequality, 199 388 Index Triangular form, 285 Triangular matrix, 284, 286 Trisecting an angle, 230 Trivial subgroups, 38 Union of sets, 3 Unique factorization domain, 163 Unique factorization theorem, 20, 148 Unit in matrix algebra, 279 Unit in ring, 145 Unital R-module, 201 Unitary transformation, 336, 338 Unity, primitive nth root of, 249 Value of polynomial, 209 vAN DER WAERDEN, 259 VANDIVER, 362 Vector(s), 171 characteristic, 271 linearly dependent, 177 Vector space(s), 170 complex, 191 homomorphism of, 173 isomorphism of, 173 real, 191 WAERDEN, VANDER, 259 Waring problem, 377 WEDDERBURN, 355, 356, 360 Wedderburn's theorem, 355, 360, 376 WEISNER, 259 WIELANDT, 92 Wilson's theorem, 116, 152 WRIGHT, 178 ZARISKI, 169 Zero-divisor, 125 Zero-matrix, 279 15-gon, regular, 232 9-gon, regular, 232 17-gon, regular, 232 2 x 2 rational matrices, ring of, 123","libVersion":"0.3.1","langs":""}