{"path":"iCloudDrive/bks/Combinatorics/Combinatorics/Probability - Daniel Spivak - Canada 2015.pdf","text":"Probability By Daniel Spivak, with some non-plagiarized content Probabilistic methods may be used to solve some combinatorics problems; while they are not universally applicable, when they do work they are very powerful. The type of problems that these apply to are generally those where you have to show that something exists in a problem where the current year is involved and the setup is such that you can’t easily apply methods such as induction. 1. What are we trying to do The idea of the method is to ﬁnd the expected value of some random variable; if its expected value is a, then either the random variable is uniformly equal to a, or it is sometimes bigger and other times smaller. Example 1.1. Show that a square’s sides and diagonals can be coloured in two colours so that there is no monochromatic triangle consisting of the square’s sides and diagonals. Colour each side with in one colour with probability 1/2 and with the other with prob- abilty 1/2. Then for every triangle, there is a 1/4 chance that its sides are of the same colour. Since there are 4 triangles, the expected number of triangles with sides of the same colour is 1. If we colour the square uniformly of one colour, the number of such triangles is 4, so we have some colouring of the square where this number is less than 1 and hence 0. Of course, this could’ve easily been done without the probabilistic method, and in fact it could be done for a pentagon, for which the probabilistic method yields nothing. However, if isntead of a square and triangles we have less trivial shapes, it very quickly becomes impossible to use traditional combinatorial arguments to solve the problem, whereas the probabilistic method gives a decent result. Note that you don’t have to assign the two colours equal probability. If the problem statement is biased in favour of one colour or the other, you can calculate a favourable probability to set for each colour (see problems). A quick check can verify that the best bound in this case is given by setting equal probabilities for the colours. 2. How do we do it The expectation of a random variable can be thought of as its weighted average. In combinatorics, when things are hard to calculate, we can instead calculate easier things and then use linearity of expectation. In the example, it would be appreciably harder to ﬁgure out how many triangles in a square are monochromatic, so we instead calculated the probability of a single triangle being monochromatic and multiplied by 4. In general, expectation is linear, meaning that: (ii) If P (X = a) = 1, then E[X] = a. 1 (ii) E[X + Y ] = E[X] + E[Y ]; (iii) E[kX] = kE[X]; (iv) if P (Z = X) = p and P (Z = Y ) = 1 − p, then E[Z] = pE[X] + (1 − p)E[Y ]. (v) If X and Y are independent, then E[XY ] = E[X]E[Y ] (note that a random variable will almost never be independent of itself, so usually E[X 2] ̸= E[X]2). Because of the last point, it often helps to do things independently (e.g. I picked the colours in the example independently from each other so that I could immediately tell that the probability of a triangle being monochromatic is 1/8 + 1/8 = 1/4). There are also some interesting facts about E[X 2] which can, from time to time, show up in probability or inequalities: (i) V ar[X] = E[X 2] − E[X]2 = E[(X − E[X])2] ≥ 0; (ii) E[(X − c)2] ≥ V ar[X] for all real constants c; (iii) If X and Y are independent, then so are X and Y 2, as well as X 2 and Y 2 (or in general, X and f (Y ) for any reasonable function f ), so V ar[XY ] = V ar[X]V ar[Y ]; (iv) P ((X − E[X])2 ≥ kV ar[X]) ≤ 1/k. These points were listed in reverse order of usefulness. 2","libVersion":"0.3.1","langs":""}