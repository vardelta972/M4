{"path":"iCloudDrive/bks/Calculus/Handouts/Feynman technique.pdf","text":"DIFFERENTIATION UNDER THE INTEGRAL SIGN LEO GOLDMAKHER In his autobiography Surely you’re joking, Mr. Feynman, Richard Feynman discusses his “different box of tools”. Among other things he mentions a tool he picked up from the text Advanced Calculus by Woods, of differentiating under the integral sign: “it’s a certain operation... that’s not taught very much in the universities”. It seems some things haven’t changed in the way calculus is taught, since I was never exposed to this trick in my calculus courses either! However, I’ve accidentally stumbled upon several elegant applications of the maneuver, and thought I’d write a few of them down. 1. DERIVATION OF THE GAMMA FUNCTION An old problem is to extend the factorial function to non-integer arguments. This was resolved by Euler, who discovered two formulas for n! (one an integral, the other an inﬁnite product) which make sense even when n is not an integer. We derive one of Euler’s formulas by employing the trick of differentiating under the integral sign. I learned about this method from the website of Noam Elkies, who reports that it was employed by Inna Zakharevich on a Math 55a problem set. Let F (t) = ∫ ∞ 0 e−tx dx. The integral is easily evaluated: F (t) = 1 t for all t > 0. Differentiating F with respect to t leads to the identity F ′(t) = − ∫ ∞ 0 x e −tx dx = − 1 t2 . Taking further derivatives yields ∫ ∞ 0 xne−tx dx = n! tn+1 which immediately implies the formula n! = ∫ ∞ 0 xne−x dx. The right hand side is the famous Gamma function, and does not depend on n being an integer. This example captures the spirit of Feynman’s trick: when integrating a function with respect to one variable, one can differentiate with respect to a different variable and sometimes glean new information. Below we’ll explore several more examples of this. 2. WARM-UP EXAMPLE In practice, Feynman’s trick is often applied to integrals with a single variable; the idea is to artiﬁcially introduce a second variable and then build a differential equation in terms of derivatives with respect to 1 the new variable. Solving this differential equation often allows us to compute the original integral. We warm up with a relatively simple example. Consider ∫ 1 0 x2 − 1 log x dx. I defy the reader to evaluate this integral by the usual methods! Instead, we introduce a new variable, t, by deﬁning G(t) := ∫ 1 0 xt − 1 log x dx; our goal is thus to determine G(2). We have G ′(t) = ∫ 1 0 xt dx = 1 t + 1 , whence G(2) = ∫ 2 0 G ′(t) dt = ∫ 2 0 dt t + 1 = log 3. 3. A SUBSTITUTE FOR CONTOUR INTEGRATION Feynman’s trick can sometimes be used in situations when one would typically use contour integration. For example, a standard integral that arises in a course on complex analysis is ∫ ∞ −∞ sin x x dx. This integral is difﬁcult to handle by standard methods, because the antiderivative of sin x x cannot be expressed as a ﬁnite composition of elementary functions. Rather than using complex analysis, we follow the idea of the previous example: we introduce an auxiliary variable and then differentiate. First, observe that ∫ ∞ −∞ sin x x dx = 2 ∫ ∞ 0 sin x x dx so that it sufﬁces to evaluate the integral on the right hand side. Set H(t) = ∫ ∞ 0 sin x x e−tx dx. This clearly converges for all t ≥ 0, and our aim is to evaluate H(0). We have H ′(t) = − ∫ ∞ 0 e −tx sin x dx. This type of integral is familiar from intro calculus courses: the trick is to integrate by parts twice. I prefer a different approach which is more broadly applicable: we rewrite the integrand as a linear combination of exponentials (which are easy to integrate) by recognizing that sin x = 1 2i(e ix − e −ix). However one evaluates the integral, we ﬁnd that for every t > 0, H ′(t) = − 1 1 + t2 . It follows that H(t) = C − arctan t for some constant C. Since H(t) tends to 0 as t → ∞ while arctan t → π 2 , we see that C = π 2 ; in other words, H(t) = π 2 − arctan t for all t > 0. Letting t tend to 0 from the right, we conclude that H(0) = π 2 . This implies that ∫ ∞ −∞ sin x x dx = π. 2 3.1. Down the rabbit hole. Before moving on to a new application of Feynman’s trick, we consider this example a bit further. Above we discovered that H ′(t) = − 1 1+t2 . In other words, we have the identity ∫ ∞ 0 e −tx sin x dx = 1 1 + t2 . Inspired by our computation of the Gamma function, we differentiate both sides with respect to t repeat- edly to ﬁnd ∫ ∞ 0 xne −tx sin x dx x = (n − 1)! (1 + t2) n hn(t) (1) where hn(t) = ∑ k≥0(−1)k( n 2k + 1 )t n−(2k+1). Observe ∗ that hn(t) = i 2((t − i) n − (t + i)n). Substituting this into (1) and tidying up a bit leads to the following curious result: Theorem 1. For any real number t ≥ 0 and any integer n ≥ 1 we have ∫ ∞ 0 xne−tx sin xdx x = sin nθ (1 + t2)n/2 (n − 1)! where θ := arcsin 1√1+t2 . This is the Mellin transform of the function e−tx sin x, evaluated at any positive integer n. In fact, we can compute it for real numbers n in terms of the Gamma function, an avenue I leave to the reader. 4. THE GAUSSIAN INTEGRAL One of the most notorious deﬁnite integrals is ∫ ∞ −∞ e−x2 dx = √ π. Because of its connection to the normal distribution, this integral plays a fundamental role in many areas, and is surprisingly difﬁcult to compute. The most familiar trick for evaluating this integral is to square it and then reinterpret the resulting double integral in polar coordinates. Here we present a different approach, using Feynman’s trick. † As before, we simplify matters by computing the right half of the integral, ∫ ∞ 0 e −x2 dx. Set I(t) := ∫ ∞ 0 e −x2 1 + (x/t)2 dx. This is deﬁned for all t > 0, and we’re hoping to determine I(∞). We simplify this integral by making the natural substitution: I(t) = t ∫ ∞ 0 e−t2x2 1 + x2 dx. ∗I’m grateful to B. Sury for pointing this out to me. †I learned this approach thanks to a post by Carl Turner (aka not all wrong) on Math StackExchange. 3 Right away this tells us new information about I(t), for example, that lim t→0 I(t) t = π 2 . (2) (In fact, the left hand side of this is I ′(0), but we won’t make use of this below.) To apply Feynman’s trick we want to ﬁnd I ′(t), but if we do this naively the outcome will be ugly. To simplify matters, we recall our warm-up example G(t), in which case differentiating simpliﬁed the integral because the denominator cancelled! Inspired by this, we instead work with the function e−t2I(t) = t ∫ ∞ 0 e −t2(1+x2) 1 + x2 dx. Finally, to avoid the product rule we divide both sides by t. Having thus renormalized I, we differentiate: d dt (t −1e −t2I(t)) = ∫ ∞ 0 −2te −t2(1+x2) dx = −2e−t2 ∫ ∞ 0 e −u2 du = −2e −t2I(∞) Now, somewhat ridiculously, we undo what we just did by integrating both sides: ∫ ∞ 0 d dt ( t−1e−t2I(t) ) dt | {z } =− lim t→0 I(t) t = ∫ ∞ 0 −2e −t2I(∞) dt | {z } =−2I(∞)2 Since we computed the left hand side in (2), we deduce I(∞)2 = π 4 . It follows that I(∞) = √π 2 , whence ∫ ∞ −∞ e−x2 dx = √ π. DEPARTMENT OF MATHEMATICS AND STATISTICS, WILLIAMS COLLEGE, WILLIAMSTOWN, MA 01267 Email address: Leo.Goldmakher@williams.edu 4","libVersion":"0.3.1","langs":""}