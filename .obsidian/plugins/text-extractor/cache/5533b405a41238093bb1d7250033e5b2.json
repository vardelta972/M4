{"path":"iCloudDrive/bks/Algebra/Algebra/putnam_linear_algebra.pdf","text":"UMA Putnam Talk LINEAR ALGEBRA TRICKS FOR THE PUTNAM YUFEI ZHAO In this talk, I want give some examples to show you some linear algebra tricks for the Putnam. Many of you probably did math contests in high school, but you might not have had much experience with linear algebra problems since they do not appear on high school contests. My goal here today is not to teach linear algebra. Instead, I want to show you how to use linear algebra. I will remind you some linear algebraic facts along the way as we use them. If there’s anything that I quote that you have not seen before, then it’s a probably a good idea for you to look them up when you go home. 1. Determinants Let us start with the classic example of the Vandermonde determinant. Problem 1 (Vandermonde determinant). Let V =     1 x1 x2 1 · · · xn−1 1 1 x2 x2 2 · · · xn−1 2 ... ... ... . . . ... 1 xn x2 n · · · xn−1 n  | |  . Show that det V = ∏ 1≤i<j≤n (xj − xi). The most standard way of evaluating the Vandermonde determinant comes from noting that the determinant vanishes when two of the variables are set be equal, thereby allowing us to extract a linear factor. Solution. Let p(x1, x2, . . . , xn) = det V, viewed as a polynomial in n variables. Now, suppose we view p as a single-variable polynomial in x1 with coeﬃcients in Q(x2, . . . , xn). If we set x1 to xi, for any i ̸= 1, then two rows of the matrix are equal and hence the determinant vanishes, and therefore (x1 − xi) must be a factor of p. Similarly, every (xi − xj) for i ̸= j is a factor of p(x1, . . . , xn). But the degree of p is 1 2n(n − 1) (from looking at the matrix), and we just showed that ∏ 1≤i<j≤n(xj − xi) (which has degree 1 2n(n − 1)) divides p. Therefore, p(x1, x2, . . . , xn) = c ∏ 1≤i<j≤n (xj − xi), for some constant c. Comparing the coeﬃcient of the term x2x2 3x3 4 · · · xn−1 n shows that k = 1. □ Date: November 22, 2011. 1 2 YUFEI ZHAO The above approach for evaluating the Vandermonde determinant is very slick. However, for other determinant evaluation problems such tricks might not be available. A more versa- tile method for evaluating determinants is by elementary row/column operations. Sometimes you’ll have to get your hands dirty and play with the algebra. Let me illustrate this concretely with the 4 × 4 case of the Vandermonde determinant. We know that subtracting a multiple of one column from another column does not change the determinant of a matrix. So let us do this successively with the goal of getting some zeros entries in the matrix.     1 a a2 a3 1 b b 2 b3 1 c c 2 c3 1 d d 2 d3  | |  ⇝     1 a a2 0 1 b b2 b2(b − a) 1 c c 2 c2(c − a) 1 d d2 d2(d − a)  | |  ⇝     1 a 0 0 1 b b(b − a) b2(b − a) 1 c c(c − a) c2(c − a) 1 d d(d − a) d2(d − a)  | |  ⇝     1 0 0 0 1 b − a b(b − a) b2(b − a) 1 c − a c(c − a) c2(c − a) 1 d − a d(d − a) d2(d − a)  | |  ⇝     1 0 0 0 0 b − a b(b − a) b2(b − a) 0 c − a c(c − a) c2(c − a) 0 d − a d(d − a) d2(d − a)  | |  Here we ﬁrst subtracted a times the 3rd column from the 4th column, thereby eliminating the (4, 1) entry. Next we subtracted a times the 2nd column from the 3rd column. Next we subtracted a times the 1st column from the 2nd column. Finally we subtracted the ﬁrst row from each of the other rows. When evaluating the determinant, note that we can take out the common factors from each row. Thus \f \f \f \f \f \f \f \f 1 a a2 a3 1 b b 2 b3 1 c c 2 c3 1 d d 2 d3 \f \f \f \f \f \f \f \f = \f \f \f \f \f \f b − a b(b − a) b2(b − a) c − a c(c − a) c2(c − a) d − a d(d − a) d 2(d − a) \f \f \f \f \f \f = (b − a)(c − a)(d − a) \f \f \f \f \f \f 1 b b 2 1 c c 2 1 d d2 \f \f \f \f \f \f . Now we get a smaller Vandermonde determinant. We can then repeat the process (or use induction) to ﬁnish the computation. This method of row elimination comes very handy when we need to evaluate the deter- minant of other large structured matrices. In particular, this is how I solved the following problem on the 2008 Putnam. Problem 2 (Putnam 2008/B6). Let n and k be positive integers. Say that a permutation σ of {1, 2, . . . , n} is k-limited if |σ(i) − i| ≤ k for all i. Prove that the number of k-limited permutations of {1, 2, . . . , n} is odd if and only if n ≡ 0 or 1 (mod 2k + 1). At ﬁrst the problem might not seem related to determinants. However, recall that deter- minants in some sense encode permutations, especially if we work mod 2 so that we do not distinguish between plus and minus signs. Indeed, if the matrix is A = (aij)n i,j=1, then det A = ∑ σ sgn(σ)a1σ(1)a2σ(2) · · · anσ(n), where the sum is taken over all permutations σ of {1, 2, . . . , n}. To count (mod 2) k-limited permutations, consider the matrix A where we set aij = 1 if |i − j| ≤ k, and aij = 0 otherwise. Then the number of k-limited permutations has the same parity as det A. For LINEAR ALGEBRA TRICKS FOR THE PUTNAM 3 instance, when n = 6, k = 2, the matrix is        1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1  | | | | |  It remains to compute the determinant of this matrix mod 2. This can be done by successive row/column manipulation. It gets a little messy, but do-able. I refer to Kiran Kedlaya’s write-up 1 for details. 2. Inverses commute Now let us look at some matrix algebra problems. Problem 3. Let A and B be n × n matrices satisfying A + B = AB. Show that AB = BA. Solution. The given equation can be rewritten as I = AB − A − B + I. We can factor the right-hand side as (A − I)(B − I) = I. So A − I and B − I are inverses. Hence (B − I)(A − I) = I. Expanding, we get A + B = BA. Hence AB = BA. □ The key fact used in the solution is: for square matrices X and Y , if XY = I, then Y X = I. In other words, left inverse and right inverse are the same for matrices. This is a very useful fact that allows us to get additional relations from matrices “for free.” It is actually somewhat nontrivial (as in, if you try to prove it by writing down the equations relating the matrix entries, you might quickly get stuck). Furthermore, the analogous statement is false for inﬁnite dimensional matrices 2. Problem 4 (Putnam 1986/B6). Suppose A, B, C, D are n × n matrices, satisfying the con- ditions that ABt and CDt are symmetric and ADt − BC t = I. Prove that A tD − C tB = I. (M t is the transpose of M .) Solution. The information given imply that (A B C D ) ( Dt −Bt −C t A t ) = (ADt − BC t −ABt + BAt CDt − DC t −CBt + DAt ) = ( I 0 0 I ) . Here the matrices are (2n) × (2n) written in block form. It follows that ( Dt −Bt −C t A t ) (A B C D ) = ( I 0 0 I ) . Equating the lower-right block gives us −C tB + A tD = I, as desired. □ Here’s a cute problem just for fun. Problem 5. Do there exist square matrices A and B such that AB − BA = I? 1http://amc.maa.org/a-activities/a7-problems/putnam/-pdf/2008s.pdf 2Take X to be the inﬁnite matrix of with only 1’s in the positions immediate above the main diagonal, and Y to be the transpose of X. 4 YUFEI ZHAO Solution. No. The key fact here is that for any matrices A and B, tr(AB) = tr(BA) (important: it is NOT true that tr(AB) = tr A tr B in general). So we have tr(AB −BA) = 0 while tr(I) ̸= 0. □ 3. Linear independence So far we’ve been just dealing with matrices. Linear algebra also comes with even when we are not working with matrices. A very useful concept is that of linear independence. Recall that vectors v1, . . . , vk ∈ V said to be linearly independent if c1v1 + · · · + ckvk = 0 implies that c1 = c2 = · · · = 0. Problem 6 (Putnam 2003/B1). Do there exist polynomials a(x), b(x), c(y), d(y) such that 1 + xy + x2y2 = a(x)c(y) + b(x)d(y) holds identically? It is possible to do this problem by writing down some polynomial coeﬃcients and playing with them. Though it might be get somewhat messy. A better way to view R[x], the set of polynomials in x, as a vector space, with basis {1, x, x2, . . .}. Solution. No they do not exist. If such polynomials existed, then setting y = −1, 0, 1 shows that 1 − x + x2, 1, 1 + x + x2 are in the linear span of a(x) and b(x). However, {1 − x + x2, 1, 1 + x + x2} is a linearly independent set, so it cannot be contained in a two- dimensional span, thus a contradiction. □ Remark. We only needed a(x) and b(x) to be polynomials, while c(y) and d(y) could be ar- bitrary functions. Similarly, if c(y) and d(y) are polynomials and a(x) and b(x) are arbitrary functions, then the same conclusion holds, by switching the roles of x and y in the proof. Problem 7. Let A = (aij) be a real n × n matrix satisfying |aii| > ∑ j̸=i |aij| for all 1 ≤ i ≤ n. Prove that A is invertible. Remark. Such matrices are called (strictly) diagonally dominant. Solution. We know that A is invertible if and only if its columns are linearly independent. Suppose not, there would exist real numbers c1, . . . , cn, not all zero, such that n∑ j=1 cjaij = 0 for each i = 1, . . . , n. In particular, let us pick i so that ci has the largest absolute value among c1, . . . , cn. Then aii = − ∑ j̸=i cj ci aij. Since |ci| ≥ |cj| for each j, we have |aii| = \f \f \f \f \f ∑ j̸=i cj ci aij \f \f \f \f \f ≤ ∑ j̸=i |cj| |ci| |aij| ≤ ∑ j̸=i |aij| < |aii| , LINEAR ALGEBRA TRICKS FOR THE PUTNAM 5 a contradiction. Therefore the columns of A are linearly independent, and hence A is invert- ible. □ Problem 8 (Oddtown). In a town with n people, m clubs have been formed. Every club have an odd number of members, and every two clubs have an even number of members in common. Prove that m ≤ n. Previously we worked with linear independence over the real numbers. It turns out that we can actually do linear algebra over any ﬁeld. In particular, in this problem, we want to work in mod 2, so it is natural to use the ﬁeld F2 with two elements. We can talk about vectors and linear independence over F2 just as in the real case. Solution. For each i = 1, 2, . . . , m, let vi ∈ Fn 2 be the vector encoding the membership data for the i-th club, so that the j-th component of vi is 1 if person j is in club i, and 0 otherwise. Then vi · vi is the number of members in the i-th club, which we know is odd, so vi · vi = 1 in F2. Similarly, vi · vj = 0 whenever i ̸= j since this count the number of people who are in both club i and club j. We claim that the vectors v1, . . . , vm are linearly independent. Indeed, if c1, . . . , cm ∈ F2 satisfy c1v1 + · · · + cmvm = 0. For each i = 1, . . . , m, then the dot product with vi shows that ci = 0 for each i. So v1, . . . , vm are m linearly independent vectors in a n-dimensional vector space Fn 2 , we have m ≤ n. □ Problem 9 (Fisher’s inequality). Let k be a positive integer. In a town with n people, m clubs have been formed. Every two clubs share exactly k members. Prove that m ≤ n. The idea is similar to the previous problem. We use incidence vectors of the clubs, and note that dot products correspond to intersections. However, now we work over the reals. Solution. Let v1, . . . , vm ∈ R n, where the vi is a {0, 1}-vector encoding the i-th club: the j-th component of vi is 1 if person j is in club i, and 0 otherwise. Then vi · vj = k whenever i ̸= j. If v1, . . . , vm are linearly independent, then m ≤ n. Otherwise, then there exists c1, . . . , cm ∈ R, not all zero, such that c1v1 + · · · + cmvm = 0. 6 YUFEI ZHAO Taking the dot product of the left-hand side sum with itself, and then expanding, we ﬁnd that 0 = (c1v1 + · · · + cmvm) · (c1v1 + · · · + cmvm) = m∑ i=1 c2 i vi · vi + 2 ∑ i<j cicjvi · vj = m∑ i=1 c2 i |vi| 2 + 2k ∑ i<j cicj = m∑ i=1 c2 i (|vi| 2 − k) + n∑ i=1 n∑ j=1 cicj = m∑ i=1 c2 i (|vi| 2 − k) + ( n∑ i=1 ci )2 . Since every two clubs share exactly k members, every club has at least k members. So |vi| 2 ≥ k for each i. So all the terms in the ﬁnal sum are nonnegative, and hence must be zero. Since some ci is nonzero, it follows that |vi| 2 − k = 0 for some i, so that some club C has exactly i members. It follows that all other clubs must all contain members of C but are otherwise disjoint. It follows immediately that there can be at most n clubs. □ 4. Some general advice Putnam is right around the corner. Here’s some general advice that I have for preparing for the competition: • Work through some past papers3. Right now the most useful thing to do for prepara- tion is to think about time management. Three hours can ﬂy by really quickly when you’re writing the Putnam. It’s a good idea to do some practice tests in a timed setting, so that you have some sense of what you can accomplish in three hours. • Every year, you’ll hear some people complain that they did not receive points for problems they thought they solved. Often this is due to poor write-up. Since you have limited time during the exam, you might end up rushing your write-ups. Make sure to always check your write-ups to make sure that they’re understandable, and that no important points were left out. • Get some good rest before the exam. Don’t cram the night before the exam, instead, go to bed early. The Putnam, being six-hours long, can drain a lot of energy. Also, set your alarm clock; you don’t want to be the guy who shows up after the exam has started. • Most importantly, have fun and enjoy the Putnam! 3A good source is Kiran Kedlaya’s Putnam directory http://amc.maa.org/a-activities/a7-problems/ putnamindex.shtml. There are also some books archiving with older exams and solutions","libVersion":"0.3.1","langs":""}