{"path":"iCloudDrive/bks/Inequalities/AGhost Notes/kedlaya_inequalities.pdf","text":"A < B (A is less than B) Kiran Kedlaya based on notes for the Math Olympiad Program (MOP) Version 1.0, last revised August 2, 1999 c\rKiran S. Kedlaya. This is an un\fnished manuscript distributed for personal use only. In particular, any publication of all or part of this manuscript without prior consent of the author is strictly prohibited. Please send all comments and corrections to the author at kedlaya@math.mit.edu. Thank you! Introduction These notes constitute a survey of the theory and practice of inequalities. While their intended audience is high-school students, primarily present and aspiring participants of the Math Olympiad Program (MOP), I hope they prove informative to a wider audience. In particular, those who experience inequalities via the Putnam competition, or via problem columns in such journals as Crux Mathematicorum or the American Mathematical Monthly, should \fnd some bene\ft. Having named high-school students as my target audience, I must now turn around and admit that I have not made any e\u000bort to keep calculus out of the exposition, for several reasons. First, in certain places, rewriting to avoid calculus would make the exposition a lot more awkward. Second, the calculus I invoke is for the most part pretty basic, mostly properties of the \frst and second derivative. Finally, it is my experience that many Olympiad participants have studied calculus anyway. In any case, I have clearly \ragged uses of calculus in the text, and I've included a crash course in calculus (Chapter ??) to \fll in the details. By no means is this primer a substitute for an honest treatise on inequalities, such as the magnum opus of Hardy, Littlewood and P\u0013olya [?] or its latter-day sequel [?], nor for a comprehensive catalog of problems in the area, for which we have Stanley Rabinowitz' series [?]. My aim, rather than to provide complete information, is to whet the reader's appetite for this beautiful and boundless subject. Also note that I have given geometric inequalities short shrift, except to the extent that they can be written in an algebraic or trigonometric form. ADD REFERENCE. Thanks to Paul Zeitz for his MOP 1995 notes, upon which these notes are ultimately based. (In particular, they are my source for symmetric sum notation.) Thanks also to the participants of the 1998 and 1999 MOPs for working through preliminary versions of these notes.veat solver! It seems easier to fool oneself by constructing a false proof of an inequality than of any other type of mathematical assertion. All it takes is one reversed inequality to turn an apparently correct proof into a wreck. The adage \\if it seems too good to be true, it probably is\" applies in full force. To impress the gravity of this point upon the reader, we provide a little exercise in 1 mathematical proofreading. Of the following X proofs, only Y are correct. Can you spot the fakes? PUT IN THE EXAMPLES. 2 Chapter 1 Separable inequalities This chapter covers what I call \\separable\" inequalities, those which can be put in the form f (x1) + \u0001 \u0001 \u0001 + f (xn) \u0015 c for suitably constrained x1; : : : ; xn. For example, if one \fxes the product, or the sum, of the variables, the AM-GM inequality takes this form, and in fact this will be our \frst example. 1.1 Smoothing, convexity and Jensen's inequality The \\smoothing principle\" states that if you have a quantity of the form f (x1) + \u0001 \u0001 \u0001 + f (xn) which becomes smaller as you move two of the variables closer together (while preserving some constraint, e.g. the sum of the variables), then the quantity is minimized by making the variables all equal. This remark is best illustrated with an example: the famous arithmetic mean and geometric mean (AM-GM) inequality. Theorem 1 (AM-GM). Let x1; : : : ; xn be positive real numbers. Then x1 + \u0001 \u0001 \u0001 + xn n \u0015 npx1 \u0001 \u0001 \u0001 xn; with equality if and only if x1 = \u0001 \u0001 \u0001 = xn. Proof. We will make a series of substitutions that preserve the left-hand side while strictly increasing the right-hand side. At the end, the xi will all be equal and the left-hand side will equal the right-hand side; the desired inequality will follow at once. (Make sure that you understand this reasoning before proceeding!) If the xi are not already all equal to their arithmetic mean, which we call a for conve- nience, then there must exist two indices, say i and j, such that xi < a < xj. (If the xi were all bigger than a, then so would be their arithmetic mean, which is impossible; similarly if they were all smaller than a.) We will replace the pair xi and xj by x0 = a; x0 = xi + xj \u0000 a; 3 by design, x0 and x0 have the same sum as xi and xj, but since they are closer together, their product is larger. To be precise, a(xi + xj \u0000 a) = xixj + (xj \u0000 a)(a \u0000 xi) > xixj because xj \u0000 a and a \u0000 xi are positive numbers. By this replacement, we increase the number of the xi which are equal to a, preserving the left-hand side of the desired inequality by increasing the right-hand side. As noted initially, eventually this process ends when all of the xi are equal to a, and the inequality becomes equality in that case. It follows that in all other cases, the inequality holds strictly. Note that we made sure that the replacement procedure terminates in a \fnite number of steps. If we had proceeded more naively, replacing a pair of xi by their arithmetic meaon, we would get an in\fnite procedure, and then would have to show that the xi were \\converging\" in a suitable sense. (They do converge, but making this precise would require some additional e\u000bort which our alternate procedure avoids.) A strong generalization of this smoothing can be formulated for an arbitrary convex function. Recall that a set of points in the plane is said to be convex if the line segment joining any two points in the set lies entirely within the set. A function f de\fned on an interval (which may be open, closed or in\fnite on either end) is said to be convex if the set f(x; y) 2 R2 : y \u0015 f (x)g is convex. We say f is concave if \u0000f is convex. (This terminology was standard at one time, but today most calculus textbooks use \\concave up\" and \\concave down\" for our \\convex\" and \\concave\". Others use the evocative sobriquets \\holds water\" and \\spills water\".) A more enlightening way to state the de\fnition might be that f is convex if for any t 2 [0; 1] and any x; y in the domain of f , tf (x) + (1 \u0000 t)f (y) \u0015 f (tx + (1 \u0000 t)y): If f is continuous, it su\u000eces to check this for t = 1=2. Conversely, a convex function is automatically continuous except possibly at the endpoints of the interval on which it is de\fned. DIAGRAM. Theorem 2. If f is a convex function, then the following statements hold: 1. If a \u0014 b < c \u0014 d, then f (c)\u0000f (a) c\u0000a \u0014 f (d)\u0000f (b) d\u0000b . (The slopes of secant lines through the graph of f increase with either endpoint.) 2. If f is di\u000berentiable everywhere, then its derivative (that is, the slope of the tangent line to the graph of f is an increasing function.) The utility of convexity for proving inequalities comes from two factors. The \frst factor is Jensen's inequality, which one may regard as a formal statement of the \\smoothing principle\" for convex functions. 4 Theorem 3 (Jensen). Let f be a convex function on an interval I and let w1; : : : ; wn be nonnegative real numbers whose sum is 1. Then for all x1; : : : ; xn 2 I, w1f (x1) + \u0001 \u0001 \u0001 + wnf (xn) \u0015 f (w1x1 + \u0001 \u0001 \u0001 + wnxn): Proof. An easy induction on n, the case n = 2 being the second de\fnition above. The second factor is the ease with which convexity can be checked using calculus, namely via the second derivative test. Theorem 4. Let f be a twice-di\u000berentiable function on an open interval I. Then f is convex on I if and only if f 00(x) \u0015 0 for all x 2 I. For example, the AM-GM inequality can be proved by noting that f (x) = log x is concave; its \frst derivative is 1=x and its second \u00001=x2. In fact, one immediately deduces a weighted AM-GM inequality; as we will generalize AM-GM again later, we will not belabor this point. We close this section by pointing out that separable inequalities sometimes concern functions which are not necessarily convex. Nonetheless one can prove something! Example 5 (USA, 1998). Let a0; : : : ; an be numbers in the interval (0; ˇ=2) such that tan(a0 \u0000 ˇ=4) + tan(a1 \u0000 ˇ=4) + \u0001 \u0001 \u0001 + tan(an \u0000 ˇ=4) \u0015 n \u0000 1: Prove that tan a0 tan a1 \u0001 \u0001 \u0001 tan an \u0015 nn+1. Solution. Let xi = tan(ai \u0000 ˇ=4) and yi = tan ai = (1 + xi)=(1 \u0000 xi), so that xi 2 (\u00001; 1). The claim would follow immediately from Jensen's inequality if only the function f (x) = log(1 + x)=(1 \u0000 x) were convex on the interval (\u00001; 1), but alas, it isn't. It's concave on (\u00001; 0] and convex on [0; 1). So we have to fall back on the smoothing principle. What happens if we try to replace xi and xj by two numbers that have the same sum but are closer together? The contribution of xi and xj to the left side of the desired inequality is 1 + xi 1 \u0000 xi \u0001 1 + xj 1 \u0000 xj = 1 + 2 xixj +1 xi+xj \u0000 1 : The replacement in question will increase xixj, and so will decrease the above quantity provided that xi + xj > 0. So all we need to show is that we can carry out the smoothing process so that every pair we smooth satis\fes this restriction. Obviously there is no problem if all of the xi are positive, so we concentrate on the possibility of having xi < 0. Fortunately, we can't have more than one negative xi, since x0 + \u0001 \u0001 \u0001 + xn \u0015 n \u0000 1 and each xi is less than 1. (So if two were negative, the sum would be at most the sum of the remaining n \u0000 1 terms, which is less than n \u0000 1.) Moreover, if x0 < 0, we could not have x0 + xj < 0 for j = 1; : : : ; n, else we would have the contradiction x0 + \u0001 \u0001 \u0001 + xn \u0014 (1 \u0000 n)x0 < n \u0000 1: 5 Thus x0 + xj > 0 for some j, and we can replace these two by their arithmetic mean. Now all of the xi are positive and smoothing (or Jensen) may continue without further restrictions, yielding the desired inequality. Problems for Section 1.1 1. Make up a problem by taking a standard property of convex functions, and specializing to a particular function. The less evident it is where your problem came from, the better! 2. Given real numbers x1; : : : ; xn, what is the minimum value of jx \u0000 x1j + \u0001 \u0001 \u0001 + jx \u0000 xnj? 3. (via Titu Andreescu) If f is a convex function and x1; x2; x3 lie in its domain, then f (x1) + f (x2) + f (x3) + f \u0012 x1 + x2 + x3 3 \u0013 \u0015 4 3 \u0014f \u0012x1 + x2 2 \u0013 + f \u0012 x2 + x3 2 \u0013 +f \u0012x3 + x1 2 \u0013\u0015 : 4. (USAMO 1974/1) For a; b; c > 0, prove that aabbcc \u0015 (abc)(a+b+c)=3. 5. (India, 1995) Let x1; : : : ; xn be n positive numbers whose sum is 1. Prove that x1p 1 \u0000 x1 + \u0001 \u0001 \u0001 + xnp1 \u0000 xn \u0015 r n n \u0000 1 : 6. Let A; B; C be the angles of a triangle. Prove that 1. sin A + sin B + sin C \u0014 3p3=2; 2. cos A + cos B + cos C \u0014 3=2; 3. sin A=2 sin B=2 sin C=2 \u0014 1=8; 4. cot A + cot B + cot C \u0015 p 3 (i.e. the Brocard angle is at most ˇ=6). (Beware: not all of the requisite functions are convex everywhere!) 7. (Vietnam, 1998) Let x1; : : : ; xn (n \u0015 2) be positive numbers satisfying 1 x1 + 1998 + 1 x2 + 1998 + \u0001 \u0001 \u0001 + 1 xn + 1998 = 1 1998 : 6 Prove that np x1x2 \u0001 \u0001 \u0001 xn n \u0000 1 \u0015 1998: (Again, beware of nonconvexity.) 8. Let x1; x2; : : : be a sequence of positive real numbers. If ak and gk are the arithmetic and geometric means, respectively, of x1; : : : ; xk, prove that an gn k \u0015 an\u00001 n\u00001 gn\u00001 k (1.1) n(an \u0000 gn) \u0015 (n \u0000 1)(an\u00001 \u0000 gn\u00001): (1.2) These strong versions of the AM-GM inequality are due to Rado [?, Theorem 60] and Popoviciu [?], respectively. (These are just a sample of the many ways the AM-GM inequality can be sharpened, as evidenced by [?].) 1.2 Unsmoothing and noninterior extrema A convex function has no interior local maximum. (If it had an interior local maximum at x, then the secant line through (x \u0000 \u000f; f (x \u0000 \u000f)) and (x + \u000f; f (x + \u000f)) would lie under the curve at x, which cannot occur for a convex function.) Even better, a function which is linear in a given variable, as the others are held \fxed, attains no extrema in either direction except at its boundary values. Problems for Section 1.2 1. (IMO 1974/5) Determine all possible values of S = a a + b + d + b a + b + c + c b + c + d + d a + c + d where a; b; c; d are arbitrary positive numbers. 2. (Bulgaria, 1995) Let n \u0015 2 and 0 \u0014 xi \u0014 1 for all i = 1; 2; : : : ; n. Show that (x1 + x2 + \u0001 \u0001 \u0001 + xn) \u0000 (x1x2 + x2x3 + \u0001 \u0001 \u0001 + xnx1) \u0014 j n 2 k ; and determine when there is equality. 1.3 Discrete smoothing The notions of smoothing and convexity can also be applied to functions only de\fned on integers. 7 Example 6. How should n balls be put into k boxes to minimize the number of pairs of balls which lie in the same box? Solution. In other words, we want to minimize Pk=1 \u0000ni 2 \u0001 over sequences n1; : : : ; nk of non- negative integers adding up to n. If ni \u0000 nj \u0015 2 for some i; j, then moving one ball from i to j decreases the number of pairs in the same box by \u0012ni 2 \u0013 \u0000 \u0012ni \u0000 1 2 \u0013 + \u0012nj 2 \u0013 \u0000 \u0012nj + 1 2 \u0013 = ni \u0000 nj \u0000 1 > 0: Thus we minimize the number of pairs by making sure no two boxes di\u000ber by more than one ball; one can easily check that the boxes must each contain bn=kc or dn=ke balls. Problems for Section 1.3 1. (Germany, 1995) Prove that for all integers k and n with 1 \u0014 k \u0014 2n, \u00122n + 1 k \u0000 1 \u0013 + \u00122n + 1 k + 1 \u0013 \u0015 2 \u0001 n + 1 n + 2 \u0001 \u00122n + 1 k \u0013: 2. (Arbelos) Let a1; a2; : : : be a convex sequence of real numbers, which is to say ak\u00001 + ak+1 \u0015 2ak for all k \u0015 2. Prove that for all n \u0015 1, a1 + a3 + \u0001 \u0001 \u0001 + a2n+1 n + 1 \u0015 a2 + a4 + \u0001 \u0001 \u0001 + a2n n : 3. (USAMO 1993/5) Let a0; a1; a2; : : : be a sequence of positive real numbers satisfying ai\u00001ai+1 \u0014 a2 for i = 1; 2; 3; : : : . (Such a sequence is said to be log concave.) Show that for each n \u0015 1, a0 + \u0001 \u0001 \u0001 + an n + 1 \u0001 a1 + \u0001 \u0001 \u0001 + an\u00001 n \u0000 1 \u0015 a0 + \u0001 \u0001 \u0001 + an\u00001 n \u0001 a1 + \u0001 \u0001 \u0001 + an n : 4. (MOP 1997) Given a sequence fxng1=0 with xn > 0 for all n \u0015 0, such that the sequence fanxng1=0 is convex for all a > 0, show that the sequence flog xng1=0 is also convex. 5. How should the numbers 1; : : : ; n be arranged in a circle to make the sum of the products of pairs of adjacent numbers as large as possible? As small as possible? 8 Chapter 2 Symmetric polynomial inequalities This section is a basic guide to polynomial inequalities, which is to say, those inequalities which are in (or can be put into) the form P (x1; : : : ; xn) \u0015 0 with P a symmetric polynomial in the real (or sometimes nonnegative) variables x1; : : : ; xn. 2.1 Introduction to symmetric polynomials Many inequalities express some symmetric relationship between a collection of numbers. For this reason, it seems worthwhile to brush up on some classical facts about symmetric polynomials. For arbitrary x1; : : : ; xn, the coe\u000ecients c1; : : : ; cn of the polynomial (t + x1) \u0001 \u0001 \u0001 (t + xn) = tn + c1tn\u00001 + \u0001 \u0001 \u0001 + cn\u00001t + cn are called the elementary symmetric functions of the xi. (That is, ck is the sum of the products of the xi taken k at a time.) Sometimes it proves more convenient to work with the symmetric averages di = ci\u0000n i\u0001 : For notational convenience, we put c0 = d0 = 1 and ck = dk = 0 for k > n. Two basic inequalities regarding symmetric functions are the following. (Note that the second follows from the \frst.) Theorem 7 (Newton). If x1; : : : ; xn are nonnegative, then d2 \u0015 di\u00001di+1 i = 1; : : : ; n \u0000 1: Theorem 8 (Maclaurin). If x1; : : : ; xn are positive, then d1 \u0015 d1=2 2 \u0015 \u0001 \u0001 \u0001 \u0015 d1=n n ; with equality if and only if x1 = \u0001 \u0001 \u0001 = xn. 9 These inequalities and many others can be proved using the following trick. Theorem 9. Suppose the inequality f (d1; : : : ; dk) \u0015 0 holds for all real (resp. positive) x1; : : : ; xn for some n \u0015 k. Then it also holds for all real (resp. positive) x1; : : : ; xn+1. Proof. Let P (t) = (t + x1) \u0001 \u0001 \u0001 (t + xn+1) = n+1X i=0 \u0012n + 1 i \u0013ditn+1\u0000i be the monic polynomial with roots \u0000x1; : : : ; \u0000xn. Recall that between any two zeros of a di\u000berentiable function, there lies a zero of its derivative (Rolle's theorem). Thus the roots of P 0(t) are all real if the xi are real, and all negative if the xi are positive. Since P 0(t) = n+1X i=0 (n + 1 \u0000 i)\u0012n + 1 i \u0013ditn\u0000i = (n + 1) nX i=0 \u0012n i \u0013ditn\u0000i; we have by assumption f (d1; : : : ; dk) \u0015 0. Incidentally, the same trick can also be used to prove certain polynomial identities. Problems for Section 2.1 1. Prove that every symmetric polynomial in x1; : : : ; xn can be (uniquely) expressed as a polynomial in the elementary symmetric polynomials. 2. Prove Newton's and Maclaurin's inequalities. 3. Prove Newton's identities: if si = xi + \u0001 \u0001 \u0001 + xi , then c0sk + c1sk\u00001 + \u0001 \u0001 \u0001 + ck\u00001s1 + kck = 0: (Hint: \frst consider the case n = k.) 4. (Hungary) Let f (x) = xn + an\u00001xn\u00001 + \u0001 \u0001 \u0001 + a1x + 1 be a polynomial with non-negative real coe\u000ecients and n real roots. Prove that f (x) \u0015 (1 + x)n for all x \u0015 0. 5. (Gauss-??) Let P (z) be a polynomial with complex coe\u000ecients. Prove that the (com- plex) zeroes of P 0(z) all lie in the convex hull of the zeroes of P (z). Deduce that if S is a convex subset of the complex plane (e.g., the unit disk), and if <f (d1; : : : ; dk) \u0015 0 holds for all x1; : : : ; xn 2 S for some n \u0015 k, then the same holds for x1; : : : ; xn+1 2 S. 6. Prove Descartes' Rule of Signs: let P (x) be a polynomial with real coe\u000ecients written as P (x) = P akixki, where all of the aki are nonzero. Prove that the number of positive roots of P (x), counting multiplicities, is equal to the number of sign changes (the number of i such that aki\u00001aki < 0) minus a nonnegative even integer. (For negative roots, apply the same criterion to P (\u0000x).) 10 2.2 The idiot's guide to homogeneous inequalities Suppose one is given a homogeneous symmetric polynomial P and asked to prove that P (x1; : : : ; xn) \u0015 0. How should one proceed? Our \frst step is purely formal, but may be psychologically helpful. We introduce the following notation: X sym Q(x1; : : : ; xn) = X ˙ Q(x˙(1); : : : ; x˙(n)) where ˙ runs over all permutations of 1; : : : ; n (for a total of n! terms). For example, if n = 3, and we write x; y; z for x1; x2; x3, X sym x3 = 2x3 + 2y3 + 2z3 X sym x2y = x2y + y2z + z2x + x2z + y2x + z2y X sym xyz = 6xyz: Using symmetric sum notation can help prevent errors, particularly when one begins with rational functions whose denominators must \frst be cleared. Of course, it is always a good algebra check to make sure that equality continues to hold when it's supposed to. In passing, we note that other types of symmetric sums can be useful when the inequal- ities in question do not have complete symmetry, most notably cyclic summation X cyclic x2y = x2y + y2z + z2x: However, it is probably a bad idea to mix, say, cyclic and symmetric sums in the same calculation! Next, we attempt to \\bunch\" the terms of P into expressions which are nonnegative for a simple reason. For example, X sym (x3 \u0000 xyz) \u0015 0 by the AM-GM inequality, while X sym (x2y2 \u0000 x2yz) \u0015 0 by a slightly jazzed-up but no more sophisticated argument: we have x2y2 + x2z2 \u0015 x2yz again by AM-GM. We can formalize what we are doing using the notion of majorization. If s = (s1; : : : ; sn) and t = (t1; : : : ; tn) are two nonincreasing sequences, we say that s majorizes t if s1+\u0001 \u0001 \u0001+sn = t1 + \u0001 \u0001 \u0001 + tn and s1 + \u0001 \u0001 \u0001 + si \u0015 t1 + \u0001 \u0001 \u0001 + ti for i = 1; : : : ; n. 11 Theorem 10 (\\Bunching\"). If s and t are sequences of nonnegative reals such that s majorizes t, then X sym xs1 1 \u0001 \u0001 \u0001 xsn n \u0015 X sym xt1 1 \u0001 \u0001 \u0001 xtn n : Proof. One \frst shows that if s majorizes t, then there exist nonnegative constants k˙, as ˙ ranges over the permutations of f1; : : : ; ng, whose sum is 1 and such that X ˙ k˙(s˙1; : : : ; s˙n) = (t1; : : : ; tn) (and conversely). Then apply weighted AM-GM as follows: X ˙ xs˙(n) 1 \u0001 \u0001 \u0001 xs˙(n) n = X ˙;˝ k˝ xs˙(˝ (1)) 1 \u0001 \u0001 \u0001 xs˙(˝ (n)) n \u0015 X ˙ x P˝ k˝ s˙(˝ (1)) 1 \u0001 \u0001 \u0001 x P˝ k˝ s˙(˝ (n)) n = X ˙ xt˙(1) 1 \u0001 \u0001 \u0001 xt˙(n) n : If the indices in the above proof are too bewildering, here's an example to illustrate what's going on: for s = (5; 2; 1) and t = (3; 3; 2), we have (3; 3; 2) = (5; 2; 1)+ and so BLAH. Example 11 (USA, 1997). Prove that, for all positive real numbers a; b; c, 1 a3 + b3 + abc + 1 b3 + c3 + abc + 1 c3 + a3 + abc \u0014 1 abc: Solution. Clearing denominators and multiplying by 2, we have X sym (a3 + b3 + abc)(b3 + c3 + abc)abc \u0014 2(a3 + b3 + abc)(b3 + c3 + abc)(c3 + a3 + abc); which simpli\fes to X sym a7bc + 3a4b4c + 4a5b2c2 + a3b3c3 \u0014 X sym a3b3c3 + 2a6b3 + 3a4b4c + 2a5b2c2 + a7bc; and in turn to X sym 2a6b3 \u0000 2a5b2c2 \u0015 0; which holds by bunching. 12 In this case we were fortunate that after slogging through the algebra, the resulting symmetric polynomial inequality was quite straightforward. Alas, there are cases where bunching will not su\u000ece, but for these we have the beautiful inequality of Schur. Note the extra equality condition in Schur's inequality; this is a much stronger result than AM-GM, and so cannot follow from a direct AM-GM argument. In general, one can avoid false leads by remembering that all of the steps in the proof of a given inequality must have equality conditions at least as permissive as those of the desired result! Theorem 12 (Schur). Let x; y; z be nonnegative real numbers. Then for any r > 0, xr(x \u0000 y)(x \u0000 z) + yr(y \u0000 z)(y \u0000 x) + zr(z \u0000 x)(z \u0000 y) \u0015 0: Equality holds if and only if x = y = z, or if two of x; y; z are equal and the third is zero. Proof. Since the inequality is symmetric in the three variables, we may assume without loss of generality that x \u0015 y \u0015 z. Then the given inequality may be rewritten as (x \u0000 y)[xr(x \u0000 z) \u0000 yr(y \u0000 z)] + zr(x \u0000 z)(y \u0000 z) \u0015 0; and every term on the left-hand side is clearly nonnegative. Keep an eye out for the trick we just used: creatively rearranging a polynomial into the sum of products of obviously nonnegative terms. The most commonly used case of Schur's inequality is r = 1, which, depending on your notation, can be written 3d3 + d3 \u0015 4d1d2 or X sym x3 \u0000 2x2y + xyz \u0015 0: If Schur is invoked with no mention r, you should assume r = 1. Example 13. (Japan, 1997) (Japan, 1997) Let a; b; c be positive real numbers, Prove that (b + c \u0000 a)2 (b + c)2 + a2 + (c + a \u0000 b)2 (c + a)2 + b2 + (a + b \u0000 c)2 (a + b)2 + c2 \u0015 3 5 : Solution. We \frst simplify slightly, and introduce symmetric sum notation: X sym 2ab + 2ac a2 + b2 + c2 + 2bc \u0014 24 5 : Writing s = a2 + b2 + c2, and clearing denominators, this becomes 5s2 X sym ab + 10s X sym a2bc + 20 X sym a3b2c \u0014 6s3 + 6s2 X sym ab + 12s X sym a2bc + 48a2b2c2 13 which simpli\fes a bit to 6s3 + s2 X sym ab + 2s X sym a2bc + 8 X sym a2b2c2 \u0015 10s X sym a2bc + 20 X sym a3b2c: Now we multiply out the powers of s: X sym 3a6 + 2a5b \u0000 2a4b2 + 3a4bc + 2a3b3 \u0000 12a3b2c + 4a2b2c2 \u0015 0: The trouble with proving this by AM-GM alone is the a2b2c2 with a positive coe\u000ecient, since it is the term with the most evenly distributed exponents. We save face using Schur's inequality (multiplied by 4abc:) X sym 4a4bc \u0000 8a3b2c + 4a2b2c2 \u0015 0; which reduces our claim to X sym 3a6 + 2a5b \u0000 2a4b2 \u0000 a4bc + 2a3b3 \u0000 4a3b2c \u0015 0: Fortunately, this is a sum of four expressions which are nonnegative by weighted AM-GM: 0 \u0014 2 X sym (2a6 + b6)=3 \u0000 a4b2 0 \u0014 X sym (4a6 + b6 + c6)=6 \u0000 a4bc 0 \u0014 2 X sym (2a3b3 + c3a3)=3 \u0000 a3b2c 0 \u0014 2 X sym (2a5b + a5c + ab5 + ac5)=6 \u0000 a3b2c: Equality holds in each case of AM-GM, and in Schur, if and only if a = b = c. Problems for Section 2.2 1. Suppose r is an even integer. Show that Schur's inequality still holds when x; y; z are allowed to be arbitrary real numbers (not necessarily positive). 2. (Iran, 1996) Prove the following inequality for positive real numbers x; y; z: (xy + yz + zx) \u0012 1 (x + y)2 + 1 (y + z)2 + 1 (z + x)2 \u0013 \u0015 9 4 : 14 3. The author's solution to the USAMO 1997 problem also uses bunching, but in a subtler way. Can you \fnd it? (Hint: try replacing each summand on the left with one that factors.) 4. (MOP 1998) 5. Prove that for x; y; z > 0, x (x + y)(x + z) + y (y + z)(y + x) + z (z + x)(z + y) \u0014 9 4(x + y + z) : 2.3 Variations: inhomogeneity and constraints One can complicate the picture from the previous section in two ways: one can make the polynomials inhomogeneous, or one can add additional constraints. In many cases, one can reduce to a homogeneous, unconstrained inequality by creative manipulations or substitu- tions; we illustrate this process here. Example 14 (IMO 1995/2). Let a; b; c be positive real numbers such that abc = 1. Prove that 1 a3(b + c) + 1 b3(c + a) + 1 c3(a + b) \u0015 3 2 : Solution. We eliminate both the nonhomogeneity and the constraint by instead proving that 1 a3(b + c) + 1 b3(c + a) + 1 c3(a + b) \u0015 3 2(abc)4=3 : This still doesn't look too appetizing; we'd prefer to have simpler denominators. So we make the additional substitution a = 1=x; b = 1=y; c = 1=z a = x=y, b = y=z, c = z=x, in which case the inequality becomes x2 y + z + y2 z + x + z2 x + y \u0015 3 2 (xyz)1=3: (2.1) Now we may follow the paradigm: multiply out and bunch. We leave the details to the reader. (We will revisit this inequality several times later.) On the other hand, sometimes more complicated maneuvers are required, as in this o\u000bbeat example. Example 15 (Vietnam, 1996). Let a; b; c; d be four nonnegative real numbers satisfying the conditions 2(ab + ac + ad + bc + bd + cd) + abc + abd + acd + bcd = 16: Prove that a + b + c + d \u0015 2 3 (ab + ac + ad + bc + bd + cd) and determine when equality holds. 15 Solution (by Sasha Schwartz). Adopting the notation from the previous section, we want to show that 3d2 + d3 = 4 =) d1 \u0015 d2: Assume on the contrary that we have 3d2 + d3 = 4 but d1 < d2. By Schur's inequality plus Theorem ??, we have 3d3 + d3 \u0015 4d1d2: Substituting d3 = 4 \u0000 3d2 gives 3d3 + 4 \u0015 (4d1 + 3)d2 > 4d2 + 3d1; which when we collect and factor implies (3d1 \u0000 4)(d2 \u0000 1) > 0. However, on one hand 3d1 \u0000 4 < 3d2 \u0000 4 = \u0000d3 < 0; on the other hand, by Maclaurin's inequality d2 \u0015 d2 > d1, so d1 > 1. Thus (3d1 \u0000 4)(d2 \u0000 1) is negative, a contradiction. As for equality, we see it implies (3d1 \u0000 4)(d2 \u0000 1) = 0 as well as equality in Maclaurin and Schur, so d1 = d2 = d3 = 1. Problems for Section 2.3 1. (IMO 1984/1) Prove that 0 \u0014 yz + zx + xy \u0000 2xyz \u0014 7=27, where x; y and z are non-negative real numbers for which x + y + z = 1. 2. (Ireland, 1998) Let a; b; c be positive real numbers such that a + b + c \u0015 abc. Prove that a2 + b2 + c2 \u0015 abc. (In fact, the right hand side can be improved to p 3abc.) 3. (Bulgaria, 1998) Let a; b; c be positive real numbers such that abc = 1. Prove that 1 1 + a + b + 1 1 + b + c + 1 1 + c + a \u0014 1 2 + a + 1 2 + b + 1 2 + c : 2.4 Substitutions, algebraic and trigonometric Sometimes a problem can be simpli\fed by making a suitable substition. In particular, this technique can often be used to simplify unwieldy constraints. One particular substitution occurs frequently in problems of geometric origin. The con- dition that the numbers a; b; c are the sides of a triangle is equivalent to the constraints a + b > c; b + c > a; c + a > b coming from the triangle inequality. If we let x = (b + c \u0000 a)=2; y = (c + a \u0000 b)=2; z = (a + b \u0000 c)=2, then the constraints become x; y; z > 0, and the original variables are also easy to express: a = y + z; b = z + x; c = x + y: A more exotic substitution can be used when the constraint a + b + c = abc is given. Put \u000b = arctan a; \f = arctan b; \r = arctan c; 16 then \u000b + \f + \r is a multiple of ˇ. (If a; b; c are positive, one can also write them as the cotangents of three angles summing to ˇ=2.) Problems for Section 2.4 1. (IMO 1983/6) Let a; b; c be the lengths of the sides of a triangle. Prove that a2b(a \u0000 b) + b2c(b \u0000 c) + c2a(c \u0000 a) \u0015 0; and determine when equality occurs. 2. (Asian Paci\fc, 1996) Let a; b; c be the lengths of the sides of a triangle. Prove that p a + b \u0000 c + pb + c \u0000 a + pc + a \u0000 b \u0014 pa + pb + pc: 3. (MOP, 1999) Let a; b; c be lengths of the the sides of a triangle. Prove that a3 + b3 + c3 \u0000 3abc \u0015 2 maxfja \u0000 bj3; jb \u0000 cj3; jc \u0000 aj3g: 4. (Arbelos) Prove that if a; b; c are the sides of a triangle and 2(ab2 + bc2 + ca2) = a2b + b2c + c2a + 3abc; then the triangle is equilateral. 5. (Korea, 1998) For positive real numbers a; b; c with a + b + c = abc, show that 1 p1 + a2 + 1 p1 + b2 + 1 p1 + c2 \u0014 3 2 ; and determine when equality occurs. (Try proving this by dehomogenizing and you'll appreciate the value of the trig substitution!) 17 Chapter 3 The toolbox In principle, just about any inequality can be reduced to the basic principles we have outlined so far. This proves to be fairly ine\u000ecient in practice, since once spends a lot of time repeating the same reduction arguments. More convenient is to invoke as necessary some of the famous classical inequalities described in this chapter. We only barely scratch the surface here of the known body of inequalities; already [?] provides further examples, and [?] more still. 3.1 Power means The power means constitute a simultaneous generalization of the arithmetic and geometric means; the basic inequality governing them leads to a raft of new statements, and exposes a symmetry in the AM-GM inequality that would not otherwise be evident. For any real number r 6= 0, and any positive reals x1; : : : ; xn, we de\fne the r-th power mean of the xi as M r(x1; : : : ; xn) = \u0012 xr + \u0001 \u0001 \u0001 + xr n \u00131=r : More generally, if w1; : : : ; wn are positive real numbers adding up to 1, we may de\fne the weighted r-th power mean M r w(x1; : : : ; xn) = (w1xr + \u0001 \u0001 \u0001 + wnxr )1=r : Clearly this quantity is continuous as a function of r (keeping the xi \fxed), so it makes sense to de\fne M 0 as lim r!0 M r w = lim r!0 \u00121 r exp log(w1xr + \u0001 \u0001 \u0001 + wnxr \u0013 = exp d dr \fr=0 log(w1xr + \u0001 \u0001 \u0001 + wnxr ) = exp w1 log x1 + \u0001 \u0001 \u0001 + wn log xn w1 + \u0001 \u0001 \u0001 + wn = xw1 1 \u0001 \u0001 \u0001 xwn n 18 or none other than the weighted geometric mean of the xi. Theorem 16 (Power mean inequality). If r > s, then M r w(x1; : : : ; xn) \u0015 M s w(x1; : : : ; xn) with equality if and only if x1 = \u0001 \u0001 \u0001 = xn. Proof. Everything will follow from the convexity of the function f (x) = xr for r \u0015 1 (its second derivative is r(r \u0000 1)xr\u00002), but we have to be a bit careful with signs. Also, we'll assume neither r nor s is nonzero, as these cases can be deduced by taking limits. First suppose r > s > 0. Then Jensen's inequality for the convex function f (x) = xr=s applied to xs; : : : ; xs gives w1xr + \u0001 \u0001 \u0001 + wnxr \u0015 (w1xs + \u0001 \u0001 \u0001 + wnxs ) r=s and taking the 1=r-th power of both sides yields the desired inequality. Now suppose 0 > r > s. Then f (x) = xr=s is concave, so Jensen's inequality is reversed; however, taking 1=r-th powers reverses the inequality again. Finally, in the case r > 0 > s, f (x) = xr=s is again convex, and taking 1=r-th powers preserves the inequality. (Or this case can be deduced from the previous ones by comparing both power means to the geometric mean.) Several of the power means have speci\fc names. Of course r = 1 yields the arithmetic mean, and we de\fned r = 0 as the geometric mean. The case r = \u00001 is known as the harmonic mean, and the case r = 2 as the quadratic mean or root mean square. Problems for Section 3.1 1. (Russia, 1995) Prove that for x; y > 0, x x4 + y2 + y y4 + x2 \u0014 1 xy : 2. (Romania, 1996) Let x1; : : : ; xn+1 be positive real numbers such that x1 + \u0001 \u0001 \u0001 + xn = xn+1. Prove that nX i=1 p xi(xn+1 \u0000 xi) \u0014 v nX i=1 xn+1(xn+1 \u0000 xi): 3. (Poland, 1995) For a \fxed integer n \u0015 1 compute the minimum value of the sum x1 + x2 2 + \u0001 \u0001 \u0001 + xn n given that x1; : : : ; xn are positive numbers subject to the condition 1 x1 + \u0001 \u0001 \u0001 + 1 xn = n: 19 4. Let a; b; c; d \u0015 0. Prove that 1 a + 1 b + 4 c + 16 d \u0015 64 a + b + c + d: 5. Extend the Rado/Popoviciu inequalities to power means by proving that M r w(x1; : : : ; xn)k \u0000 M s w(x1; : : : ; xn)k wn \u0015 M r w0(x1; : : : ; xn\u00001)k \u0000 M s w0(x1; : : : ; xn\u00001)k w0 n\u00001 whenever r \u0015 k \u0015 s. Here the weight vector w0 = (w0 1; : : : ; w0 n\u00001) is given by w0 i = wi=(w1 + \u0001 \u0001 \u0001 + wn\u00001). (Hint: the general result can be easily deduced from the cases k = r; s.) 6. Prove the following \\converse\" to the power mean inequality: if r > s > 0, then X i xr !1=r \u0014 X i xs !1=s : 3.2 Cauchy-Schwarz, H\u0000older and Minkowski inequali- ties This section consists of three progressively stronger inequalities, beginning with the simple but versatile Cauchy-Schwarz inequality. Theorem 17 (Cauchy-Schwarz). For any real numbers a1; : : : ; an, b1; : : : ; bn, (a2 + \u0001 \u0001 \u0001 + a2 )(b2 + \u0001 \u0001 \u0001 + b2 ) \u0015 (a1b1 + \u0001 \u0001 \u0001 + anbn)2; with equality if the two sequences are proportional. Proof. The di\u000berence between the two sides is X i<j (aibj \u0000 ajbi)2 and so is nonnegative, with equality i\u000b aibj = ajbi for all i; j. The \\sum of squares\" trick used here is an important one; look for other opportunities to use it. A clever example of the use of Cauchy-Schwarz is the proposer's solution of Example ??, in which the xyz-form of the desired equation is deduced as follows: start with the inequality \u0012 x2 y + z + y2 z + x + z2 x + y \u0013 ((y + z) + (z + x) + (x + y)) \u0015 (x + y + z)2 which follows from Cauchy-Schwarz, cancel x + y + z from both sides, then apply AM-GM to replace x + y + z on the right side with 3(xyz)1=3. A more \rexible variant of Cauchy-Schwarz is the following. 20 Theorem 18 (H\u0000older). Let w1; : : : ; wn be positive real numbers whose sum is 1. For any positive real numbers aij, nY i=1 mX j=1 aij !wi \u0015 mX j=1 nY i=1 awi ij : Proof. By induction, it su\u000eces to do the case n = 2, in which case we'll write w1 = 1=p and w2 = 1=q. Also without loss of generality, we may rescale the aij so that Pm=1 aij = 1 for i = 1; 2. In this case, we need to prove 1 \u0015 mX j=1 a1=p 1j a1=q 2j : On the other hand, by weighted AM-GM, a1j p + a2j q \u0015 a1=p 1j a1=q 2j and the sum of the left side over j is 1=p + 1=q = 1, so the proof is complete. (The special case of weighted AM-GM we just used is sometimes called Young's inequality.) Cauchy-Schwarz admits a geometric interpretation, as the triangle inequality for vectors in n-dimensional Euclidean space: q x2 + \u0001 \u0001 \u0001 + x2 + q y2 1 + \u0001 \u0001 \u0001 + y2 n \u0015 p (x1 + y1)2 + \u0001 \u0001 \u0001 + (xn + yn)2: One can use this interpretation to prove Cauchy-Schwarz by reducing it to the case in two variables, since two nonparallel vectors span a plane. Instead, we take this as a starting point for our next generalization of Cauchy-Schwarz (which will be the case r = 2; s = 1 of Minkowski's inequality). Theorem 19 (Minkowski). Let r > s be nonzero real numbers. Then for any positive real numbers aij, 0 mX j=1 nX i=1 ar !s=r1 1=s \u0015 0 nX i=1 mX j=1 as !r=s1 1=r : Minkowski's theorem may be interpreted as a comparison between taking the r-th power mean of each row of a matrix, then taking the s-th power mean of the results, versus taking the s-th power mean of each column, then taking the r-th power mean of the result. If r > s, the former result is larger, which should not be surprising since there we only use the smaller s-th power mean operation once. This interpretation also tells us what Minkowski's theorem should say in case one of r; s is zero. The result is none other than H\u0000older's inequality! 21 Proof. First suppose r > s > 0. Write L and R for the left and right sides, respectively, and for convenience, write ti = Pm=1 as . Then Rr = nX i=1 tr=s i = nX i=1 titr=s\u00001 i = mX j=1 nX i=1 as tr=s\u00001 i ! \u0014 mX j=1 nX i=1 ar !s=r nX i=1 tr=s i !1\u0000s=r = LsRr\u0000s; where the one inequality is a (term-by-term) invocation of H\u0000older. Next suppose r > 0 > s. Then the above proof carries through provided we can prove a certain variation of H\u0000older's inequality with the direction reversed. We have left this to you (Problem ??). Finally suppose 0 > r > s. Then replacing aij by 1=aij for all i; j turns the desired inequality into another instance of Minkowski, with r and s replaced by \u0000s and \u0000r. This instance follows from what we proved above. Problems for Section 3.2 1. (Iran, 1998) Let x; y; z be real numbers not less than 1, such that 1=x + 1=y + 1=z = 2. Prove that px + y + z \u0015 px \u0000 1 + py \u0000 1 + p z \u0000 1: 2. Complete the proof of Minkowski's inequality by proving that if k < 0 and a1; : : : ; an, b1; : : : ; bn > 0, then akb1\u0000k 1 + \u0001 \u0001 \u0001 + ak b1\u0000k n \u0015 (a1 + \u0001 \u0001 \u0001 + an)k(b1 + \u0001 \u0001 \u0001 + bn)1\u0000k: (Hint: reduce to H\u0000older.) 3. Formulate a weighted version of Minkowski's inequality, with one weight corresponding to each aij. Then show that this apparently stronger result follows from Theorem ??! 4. (Titu Andreescu) Let P be a polynomial with positive coe\u000ecients. Prove that if P \u0012 1 x \u0013 \u0015 1 P (x) holds for x = 1, then it holds for every x > 0. 22 5. Prove that for w; x; y; z \u0015 0, w6x3 + x6y3 + y6z3 + z6x3 \u0015 w2x5y2 + x2y5z2 + y2z5w2 + z2x5y2: (This can be done either with H\u0000older or with weighted AM-GM; try it both ways.) 6. (China) Let ri; si; ti; ui; vi be real numbers not less than 1, for i = 1; 2; : : : ; n, and let R; S; T; U; V be the respective arithmetic means of the ri; si; ti; ui; vi. Prove that nY i=1 risitiuivi + 1 risitiuivi \u0000 1 \u0014 \u0012 RST U V + 1 RST U V \u0000 1 \u0013n : (Hint: use H\u0000older and the concavity of f (x) = (x5 \u0000 1)=(x5 + 1). 7. (IMO 1992/5) Let S be a \fnite set of points in three-dimensional space. Let Sx; Sy; Sz be the orthogonal projections of S onto the yz, zx, xy planes, respectively. Show that jSj2 \u0014 jSxjjSyjjSzj: (Hint: Use Cauchy-Schwarz to prove the inequality X i;j;k aijbjkcki !2 \u0014 X i;j a2 X j;k b2k X k;i c2i: Then apply this with each variable set to 0 or 1.) 3.3 Rearrangement, Chebyshev and \\arrange in or- der\" Our next theorem is in fact a triviality that every businessman knows: you make more money by selling most of your goods at a high price and a few at a low price than vice versa. Nonetheless it can be useful! (In fact, an equivalent form of this theorem was on the 1975 IMO.) 20 (Rearrangement). Given two increasing sequences x1; : : : ; xn and y1; : : : ; yn of real numbers, the sum nX i=1 xiy˙(i); for ˙ a permutation of f1; : : : ; ng, is maximized when ˙ is the identity permutation and minimized when ˙ is the reversing permutation. 23 Proof. For n = 2, the inequality ac + bd \u0015 ad + bc is equivalent, after collecting terms and factoring, to (a \u0000 b)(c \u0000 d) \u0015 0, and so holds when a \u0015 b and c \u0015 d. We leave it to the reader to deduce the general case from this by successively swapping pairs of the yi. Another way to state this argument involves \\partial summation\" (which the calculus- equipped reader will recognize as analogous to doing integration by parts). Let si = y˙(i) + \u0001 \u0001 \u0001 + y˙(n) for i = 1; : : : ; n, and write nX i=1 xiy˙(i) = x1s1 + nX j=2 (xj \u0000 xj\u00001)sj: The desired result follows from this expression and the inequalities x1 + \u0001 \u0001 \u0001 + xn\u0000j+1 \u0014 sj \u0014 xj + \u0001 \u0001 \u0001 + xn; in which the left equality holds for ˙ equal to the reversing permutation and the right equality holds for ˙ equal to the identity permutation. One important consequence of rearrangement is Chebyshev's inequality. Theorem 21 (Chebyshev). Let x1; : : : ; xn and y1; : : : ; yn be two sequences of real numbers, at least one of which consists entirely of positive numbers. Assume that x1 < \u0001 \u0001 \u0001 < xn and y1 < \u0001 \u0001 \u0001 < yn. Then x1 + \u0001 \u0001 \u0001 + xn n \u0001 y1 + \u0001 \u0001 \u0001 + yn n \u0014 x1y1 + \u0001 \u0001 \u0001 + xnyn n : If instead we assume x1 > \u0001 \u0001 \u0001 > xn and y1 < \u0001 \u0001 \u0001 < yn, the inequality is reversed. Proof. Apply rearrangement to x1; x2; : : : ; xn and y1; : : : ; yn, but repeating each number n times. The great power of Chebyshev's inequality is its ability to split a sum of complicated terms into two simpler sums. We illustrate this ability with yet another solution to IMO 1995/5. Recall that we need to prove x2 y + z + y2 z + x + z2 x + y \u0015 x + y + z 2 : Without loss of generality, we may assume x \u0014 y \u0014 z, in which case 1=(y + z) \u0014 1=(z + x) \u0014 1=(x + y). Thus we may apply Chebyshev to deduce x2 y + z + y2 z + x + z2 x + y \u0015 x2 + y2 + z2 3 \u0001 \u0012 1 y + z + 1 z + x + 1 x + y \u0013 : By the Power Mean inequality, x2 + y2 + z2 3 \u0015 \u0012 x + y + z 3 \u00132 1 3 \u0012 1 y + z + 1 z + x + 1 x + y \u0013 \u0015 3 x + y + z 24 and these three inequalities constitute the proof. The rearrangement inequality, Chebyshev's inequality, and Schur's inequality are all general examples of the \\arrange in order\" principle: when the roles of several variables are symmetric, it may be pro\ftable to desymmetrize by assuming (without loss of generality) that they are sorted in a particular order by size. Problems for Section 3.3 1. Deduce Schur's inequality from the rearrangement inequality. 2. For a; b; c > 0, prove that aabbcc \u0015 abbcca. 3. (IMO 1971/1) Prove that the following assertion is true for n = 3 and n = 5 and false for every other natural number n > 2: if a1; : : : ; an are arbitrary real numbers, then nX i=1 Y j6=i(ai \u0000 aj) \u0015 0: 4. (Proposed for 1999 USAMO) Let x; y; z be real numbers greater than 1. Prove that xx2+2yzyy2+2zxzz2+2xy \u0015 (xyz)xy+yz+zx: 3.4 Bernoulli's inequality The following quaint-looking inequality occasionally comes in handy. Theorem 22 (Bernoulli). For r > 1 and x > \u00001 (or r an even integer and x any real), (1 + x)r \u0015 1 + xr. Proof. The function f (x) = (1+x)r has second derivative r(r \u00001)(1+x)r\u00002 and so is convex. The claim is simply the fact that this function lies above its tangent line at x = 0. Problems for Section 3.4 1. (USA, 1992) Let a = (mm+1 + nn+1)=(mm + nn) for m; n positive integers. Prove that am + an \u0015 mm + nn. (The problem came with the hint to consider the ratio (aN \u0000 N N )=(a \u0000 N ) for real a \u0015 0 and integer N \u0015 1. On the other hand, you can prove the claim for real m; n \u0015 1 using Bernoulli.) 25 Chapter 4 Calculuso some extent, I regard this section, as should you, as analogous to sex education in school{ it doesn't condone or encourage you to use calculus on Olympiad inequalities, but rather seeks to ensure that if you insist on using calculus, that you do so properly. Having made that disclaimer, let me turn around and praise calculus as a wonderful discovery that immeasurably improves our understanding of real-valued functions. If you haven't encountered calculus yet in school, this section will be a taste of what awaits you|but no more than that; our treatment is far too compressed to give a comprehensive exposition. After all, isn't that what textbooks are for? Finally, let me advise the reader who thinks he/she knows calculus already not to breeze too this section too quickly. Read the text and make sure you can write down the proofs requested in the exercises. 4.1 Limits, continuity, and derivatives The de\fnition of limit attempts to formalize the idea of evaluating a function at a point where it might not be de\fned, by taking values at points \\in\fnitely close\" to the given point. Capturing this idea in precise mathematical language turned out to be a tricky business which remained unresolved until long after calculus had proven its utility in the hands of Newton and Leibniz; the de\fnition we use today was given by Weierstrass. Let f be a function de\fned on an open interval, except possibly at a single point t. We say the limit of f at t exists and equals L if for each positive real \u000f, there exists a positive real \u000e (depending on \u000f, of course) such that 0 < jx \u0000 tj < \u000e =) jf (x) \u0000 Lj < \u000f: We notate this limx!t f (x) = L. If f is also de\fned at t and f (t) = L, we say f is continuous at t. Most common functions (polynomials, sine, cosine, exponential, logarithm) are contin- uous for all real numbers (the logarithm only for positive reals), and those which are not (rational functions, tangent) are continuous because they go to in\fnity at certain points, so 26 the limits in question do not exist. The \frst example of an \\interesting\" limit occurs in the de\fnition of the derivative. Again, let f be a function de\fned on an open interval and t a point in that interval; this time we require that f (t) is de\fned as well. If the limit lim!t f (x) \u0000 f (t) x \u0000 t exists, we call that the derivative of f at t and notate it f 0(t). We also say that f is di\u000berentiable at t (the process of taking the derivative is called di\u000berentiation). For example, if f (x) = x2, then for x 6= t, (f (x) \u0000 f (t))=(x \u0000 t) = x + t, and so the limit exists and f 0(t) = 2t. More generally, you will show in the exercises that for f (x) = xn, f 0(t) = ntn\u00001. On the other hand, for other functions, e.g. f (x) = sin x, the di\u000berence quotient cannot be expressed in closed form, so certain inequalities must be established in order to calculate the derivative (as will also take place in the exercises). The derivative can be geometrically interpreted as the slope of the tangent line to the graph of f at the point (t; f (t)). This is because the tangent line to a curve can be regarded as the limit of secant lines, and the quantity (f (x + t) \u0000 f (x))=t is precisely the slope of the secant line through (x; f (x)) and (x + t; f (x + t)). (The \frst half of this sentence is entirely unrigorous, but it can be rehabilitated.) It is easy to see that a di\u000berentiable function must be continuous, but the converse is not true; the function f (x) = jxj (absolute value) is continuous but not di\u000berentiable at x = 0. An important property of continuous functions, though one we will not have direct need for here, is the intermediate value theorem. This theorem says the values of a continuous function cannot \\jump\", as the tangent function does near ˇ=2. Theorem 23. Suppose the function f is continuous on the closed interval [a; b]. Then for any c between f (a) and f (b), there exists x 2 [a; b] such that f (x) = c. Proof. Any proof of this theorem tends to be a bit unsatisfying, because it ultimately boils down to one's de\fnition of the real numbers. For example, one consequence of the standard de\fnition is that every set of real numbers which is bounded above has a least upper bound. In particular, if f (a) \u0014 c and f (b) \u0015 c, then the set of x 2 [a; b] such that f (x) < c has a least upper bound y. By continuity, f (x) \u0014 c; on the other hand, if f (x) > c, then f (x \u0000 t) > 0 for all t less than some positive \u000f, again by continuity, in which case x \u0000 \u000f is an upper bound smaller than x, a contradiction. Hence f (x) = c. Another important property of continuous functions is the \\extreme value theorem\". Theorem 24. A continuous function on a closed interval has a global maximum and mini- mum.oof. 27 This statement is of course false for an open or in\fnite interval; the function may go to in\fnity at one end, or may approach an extreme value without achieving it. (In technical terms, a closed interval is compact while an open interval is not.) Problems for Section 4.1 1. Prove that f (x) = x is continuous. 2. Prove that the product of two continuous functions is continuous, and that the recip- rocal of a continuous function is continuous at all points where the original function is nonzero. Deduce that all polynomials and rational functions are continuous. 3. (Product rule) Prove that (f g)0 = f g0 + f 0g and \fnd a similar formula for (f =g)0. (No, it's not f 0=g \u0000 f =g0. Try again.) 4. (Chain rule) If h(x) = f (g(x)), prove that h0(x) = f 0(g(x))g0(x). 5. Show that the derivative of xn is nxn\u00001 for n 2 Z. 6. Prove that sin x < x < tan x for 0 < x < ˇ=2. (Hint: use a geometric interpretation, and remember that x represents an angle in radians!) Conclude that limx!0 sin x=x = 1. 7. Show that the derivative of sin x is cos x and that of cos x is (\u0000 sin x). While you're at it, compute the derivatives of the other trigonometric functions. 8. Show that an increasing function on a closed interval satisfying the intermediate value theorem must be continuous. (Of course, this can fail if the function is not increasing!) In particular, the functions cx (where c > 0 is a constant) and log x are continuous. 9. For c > 0 a constant, the function f (x) = (cx \u0000 1)=x is continuous for x 6= 0 by the previous exercise. Prove that its limit at 0 exists. (Hint: if c > 1, show that f is increasing for x 6= 0; it su\u000eces to prove this for rational x by continuity.) 10. Prove that the derivative of cx equals kcx for some constant k. (Use the previous exercise.) Also show (using the chain rule) that k is proportional to the logarithm of c. In fact, the base e of the natural logarithms is de\fned by the property that k = 1 when c = e. 11. Use the chain rule and the previous exercise to prove that the derivative of log x is 1=x. (Namely, take the derivative of elog x = x.) 12. (Generalized product rule) Let f (y; z) be a function of two variables, and let g(x) = f (x; x). Prove that g0(x) can be written as the sum of the derivative of f as a function of y alone (treating z as a constant function) and the derivative of f as a function of z alone, both evaluated at y = z = x. For example, the derivative of xx is xx log x + xx. 28 4.2 Extrema and the \frst derivative The derivative can be used to detect local extrema. A point t is a local maximum (resp. minimum) for a function f if f (t) \u0015 f (x) (resp. f (t) \u0014 f (x)) for all x in some open interval containing t. Theorem 25. If t is a local extremum for f and f is di\u000berentiable at t, then f 0(t) = 0. Corollary 26 (Rolle). If f is di\u000berentiable on the interval [a; b] and f (a) = f (b) = 0, then there exists x 2 [a; b] such that f 0(x) = 0. So for example, to \fnd the extrema of a continuous function on a closed interval, it su\u000eces to evaluate it at \u000f all points where the derivative vanishes, \u000f all points where the derivative is not de\fned, and \u000f the endpoints of the interval, since we know the function has global minima and maxima, and each of these must occur at one of the aforementioned points. If the interval is open or in\fnite at either end, one must also check the limiting behavior of the function there. A special case of what we have just said is independently useful: if a function is positive at the left end of an interval and has nonnegative derivative on the interval, it is positive on the entire interval. 4.3 Convexity and the second derivative As noted earlier, a twice di\u000berentiable function is convex if and only if its second derivative is nonnegative. 4.4 More than one variable Warning! The remainder of this chapter is somewhat rougher going than what came before, in part because we need to start using the language and ideas of linear algebra. Rest assured that none of the following material is referenced anywhere else in the notes. We have a pretty good understanding now of the relationship between extremization and the derivative, for functions in a single variable. However, most inequalities in nature deal with functions in more than one variable, so it behooves us to extend the formalism of calculus to this setting. Note that whatever we allow the domain of a function to be, the range of our \\functions\" will always be the real numbers. (We have no need to develop the whole of multivariable calculus when all of our functions arise from extremization questions!) 29 The formalism becomes easier to set up in the language of linear algebra. If f is a function from a vector space to R de\fned in a neighborhood of (i.e. a ball around) a point ~x, then we say the limit of f at ~x exists and equals L if for every \u000f > 0, there exists \u000e > 0 such that 0 < jj~tjj < \u000e =) jf (~x + ~t) \u0000 Lj < \u000f: (The double bars mean length in the Euclidean sense, but any reasonable measure of the length of a vector will give an equivalent criterion; for example, measuring a vector by the maximum absolute value of its components, or by the sum of the absolute values of its components.) We say f is continuous at ~x if lim~t!0 f (~x + ~t) = f (~x). If ~y is any vector and ~x is in the domain of f , we say the directional derivative of f along ~x in the direction ~y exists and equals f~y(~x) if f~y(~x) = lim t!0 f (~x + t~y) \u0000 f (~x) t : If f is written as a function of variables x1; : : : ; xn, we call the directional derivative along the i-th standard basis vector the partial derivative of f with respect to i and denote it by @f @xi . In other words, the partial derivative is the derivative of f as a function of xi along, regarding the other variables as constants. TOTAL DERIVATIVE Caveat! Since the derivative is not a \\function\" in our restricted sense (it has takes values in a vector space, not R) we cannot take a \\second derivative\"|yet. ASSUMING the derivative exists, it can be computed by taking partial derivatives along a basis. 4.5 Convexity in several variables A function f de\fned on a convex subset of a vector space is said to be convex if for all ~x; ~y in the domain and t 2 [0; 1], tf (~x) + (1 \u0000 t)f (~y) \u0015 f (t~x + (1 \u0000 t)~y): Equivalently, f is convex if its restriction to any line is convex. Of course, we say f is concave if \u0000f is convex. The analogue of the second derivative test for convexity is the Hessian criterion. A symmetric matrix M (that is, one with Mij = Mji for all i; j) is said to be positive de\fnite if M~x\u0001~x > 0 for all nonzero vectors ~x, or equivalently, if its eigenvalues are all real and positive. (The \frst condition implies the second because all eigenvalues of a symmetric matrix are real. The second implies the \frst because if M has positive eigenvalues, then it has a square root N which is also symmetric, and M~x \u0001 ~x = (N~x) \u0001 (N~x) > 0.) Theorem 27 (Hessian test). A twice di\u000berentiable function f (x1; : : : ; xn) is convex in a region if and only if the Hessian matrix Hij = @2 @xi@xj 30 is positive de\fnite everywhere in the region. Note that the Hessian is symmetric because of the symmetry of mixed partials, so this statement makes sense. Proof. The function f is convex if and only if its restriction to each line is convex, and the second derivative along a line through ~x in the direction of ~y is (up to a scale factor) just H~y \u0001 ~y evaluated at ~x. So f is convex if and only if H~y \u0001 ~y > 0 for all nonzero ~y, that is, if H is positive de\fnite. The bad news about this criterion is that determining whether a matrix is positive de\fnite is not a priori an easy task: one cannot check M~x \u0001 ~x \u0015 0 for every vector, so it seems one must compute all of the eigenvalues of M , which can be quite a headache. The good news is that there is a very nice criterion for positive de\fniteness of a symmetric matrix, due to Sylvester, that saves a lot of work. Theorem 28 (Sylvester's criterion). An n \u0002 n symmetric matrix of real numbers is positive de\fnite if and only if the determinant of the upper left k \u0002 k submatrix is positive for k = 1; : : : ; n. Proof. By the M~x \u0001 ~x de\fnition, the upper left k \u0002 k submatrix of a positive de\fnite matrix is positive de\fnite, and by the eigenvalue de\fnition, a positive de\fnite matrix has positive determinant. Hence Sylvester's criterion is indeed necessary for positive de\fniteness. We show the criterion is also su\u000ecient by induction on n. BLAH. Problems for Section 4.5 1. (IMO 1968/2) Prove that for all real numbers x1; x2; y1; y2; z1; z2 with x1; x2 > 0 and x1y1 > z2 1, x2y2 > z2, the inequality 8 (x1 + x2)(y1 + y2) \u0000 (z1 + z2)2 \u0014 1 x1y1 \u0000 z2 1 + 1 x2y2 \u0000 z2 2 is satis\fed, and determine when equality holds. (Yes, you really can apply the material of this section to the IMO! Verify convexity of the appropriate function using the Hessian and Sylvester's criterion.) 4.6 Constrained extrema and Lagrange multipliers In the multivariable realm, a new phenomenon emerges that we did not have to consider in the one-dimensional case: sometimes we are asked to prove an inequality in the case where the variables satisfy some constraint. 31 The Lagrange multiplier criterion for an interior local extremum of the function f (x1; : : : ; xn) under the constraint g(x1; : : : ; xn) = c is the existence of \u0015 such that @f @xi (x1; : : : ; xn) = \u0015 @g @xi (x1; : : : ; xn): Putting these conditions together with the constraint on g, one may be able to solve and thus put restrictions on the locations of the extrema. (Notice that the duality of constrained optimization shows up in the symmetry between f and g in the criterion.) It is even more critical here than in the one-variable case that the Lagrange multiplier condition is a necessary one only for an interior extremum. Unless one can prove that the given function is convex, and thus that an interior extremum must be a global one, one must also check all boundary situations, which is far from easy to do when (as often happens) these extend to in\fnity in some direction. For a simple example, let f (x; y; z) = ax + by + cz with a; b; c constants, not all zero, and consider the constraint g(x; y; z) = 1, where g(x; y; z) = x2 + y2 + z2. Then the Lagrange multiplier condition is that a = 2\u0015x; b = 2\u0015y; c = 2\u0015z: The only points satisfying this condition plus the original constraint are \u0006 1 pa2 + b2 + c2 (a; b; c); and these are indeed the minimum and maximum for f under the constraint, as you may verify geometrically. 32 Chapter 5 Coda 5.1 Quick reference Here's a handy reference guide to the techniques we've introduced. \u000f Arithmetic-geometric-harmonic means \u000f Arrange in order \u000f Bernoulli \u000f Bunching \u000f Cauchy-Schwarz \u000f Chebyshev \u000f Convexity \u000f Derivative test \u000f Duality for constrained optimization \u000f Equality conditions \u000f Factoring \u000f Geometric interpretations \u000f Hessian test \u000f H\u0000older \u000f Jensen 33 \u000f Lagrange multipliers \u000f Maclaurin \u000f Minkowski \u000f Newton \u000f Power means \u000f Rearrangement \u000f Reduction of the number of variables (Theorem ??) \u000f Schur \u000f Smoothing \u000f Substitution (algebraic or trigonometric) \u000f Sum of squares \u000f Symmetric sums \u000f Unsmoothing (boundary extrema) 5.2 Additional problems Here is an additional collection of problems covering the entire range of techniques we have introduced, and one or two that you'll have to discover for yourselves! Problems for Section 5.2 1. Let x; y; z > 0 with xyz = 1. Prove that x + y + z \u0014 x2 + y2 + z2. 2. The real numbers x1; x2; : : : ; xn belong to the interval [\u00001; 1] and the sum of their cubes is zero. Prove that their sum does not exceed n=3. 3. (IMO 1972/2) Let x1; : : : ; x5 be positive reals such that (x2+1 \u0000 xi+3xi+5)(x2+2 \u0000 xi+3xi+5) \u0014 0 for i = 1; : : : ; 5, where xn+5 = xn for all n. Prove that x1 = \u0001 \u0001 \u0001 = x5. 4. (USAMO 1979/3) Let x; y; z \u0015 0 with x + y + z = 1. Prove that x3 + y3 + z3 + 6xyz \u0015 1 4 : 34 5. (Taiwan, 1995) Let P (x) = 1 + a1x + \u0001 \u0001 \u0001 + an\u00001xn\u00001 + xn be a polynomial with complex coe\u000ecients. Suppose the roots of P (x) are \u000b1; \u000b2; : : : ; \u000bn with j\u000b1j > 1; j\u000b2j > 1; : : : ; j\u000bjj > 1 and j\u000bj+1j \u0014 1; j\u000bj+2j \u0014 1; : : : ; j\u000bnj \u0014 1: Prove that jY i=1 j\u000bij \u0014 p ja0j2 + ja1j2 + \u0001 \u0001 \u0001 + janj2: (Hint: look at the coe\u000ecients of P (x)P (x), the latter being 1+a1x+\u0001 \u0001 \u0001+an\u00001xn\u00001+xn.) 6. Prove that, for any real numbers x; y; z, 3(x2 \u0000 x + 1)(y2 \u0000 y + 1)(z2 \u0000 z + 1) \u0015 (xyz)2 + xyz + 1: 7. (a) Prove that any polynomial P (x) such that P (x) \u0015 0 for all real x can be written as the sum of the squares of two polynomials. (b) Prove that the polynomial x2(x2 \u0000 y2)(x2 \u0000 1) + y2(y2 \u0000 1)(y2 \u0000 x2) + (1 \u0000 x2)(1 \u0000 y2) is everywhere nonnegative, but cannot be written as the sum of squares of any number of polynomials. (One of Hilbert's problems, solved by Artin and Schreier, was to prove that such a polynomial can always be written as the sum of squares of rational functions.) 35 Bibliography [1] P.S. Bullen, D. Mitrinovi\u0013c, and P.M.Vasi\u0013c, Means and their Inequalities, Reidel, Dor- drecht, 1988. [2] G. Hardy, J.E. Littlewood, and G. P\u0013olya, Inequalities (second edition), Cambridge University Press, Cambridge, 1951. [3] T. Popoviciu, Asupra mediilor aritmetice si medie geometrice (Romanian), Gaz. Mat. Bucharest 40 (1934), 155-160. [4] S. Rabinowitz, Index to Mathematical Problems 1980-1984, Mathpro Press, Westford (Massachusetts), 1992. 36","libVersion":"0.3.1","langs":""}